{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","title: \"Assignment 4-1\"\n","author: \"Sangwon Ju, SNU GSPA\"\n","date: 'DEC/18/2022'\n","format: \n","    html:\n","        code-overflow: wrap\n","        code-fold: true\n","categories:\n","   \"Machine Learning & Deep Learning for Data Science (2022 Fall)\" \n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Recurrent Neural Networks (Score: 84/100)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kO4EhYqCTdbt"},"outputs":[{"name":"stdout","output_type":"stream","text":["e:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제4\\HW4\n","E:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제4\\HW4\n"]}],"source":["import os\n","# get current path\n","print(os.getcwd())\n","# change path\n","os.chdir(\"E:/OneDrive - SNU\\(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/과제4/HW4\")\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDk49qpsU_BK"},"outputs":[],"source":["!pip install torchdata"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HCI6OuVr7a1j"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NMAuwnOwVBOu"},"outputs":[],"source":["import time\n","import math\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","from typing import List, Tuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torchtext\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"I6fOONHU7aMR"},"outputs":[],"source":["\"\"\"\n","import modules you need\n","\"\"\"\n","from plotnine import *\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yn5FYiiQVC2k"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using PyTorch version: 1.13.1+cpu, Device: cpu\n","Using torchtext version: 0.14.1\n"]}],"source":["DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\n","print(\"Using torchtext version: {}\".format(torchtext.__version__))"]},{"cell_type":"markdown","metadata":{"id":"E4E1p3w-VLTq"},"source":["# Load Data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NXSORPxqn7lU"},"outputs":[],"source":["\"\"\"\n","Load AG_NEWS dataset and set up the tokenizer and encoder pipeline.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","train_data, test_data = torchtext.datasets.AG_NEWS(root='./data')\n","\n","tokenizer = get_tokenizer('basic_english')\n","\n","def tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","encoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\n","encoder.set_default_index(encoder[\"<unk>\"])\n","\n","text_pipeline = lambda x: encoder(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jpb0L6hvmh2M"},"outputs":[],"source":["def collate_batch(\n","    batch: List[Tuple[int, str]]\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","    Creates a batch of encoded text, label and token length tensors.\n","\n","    Question (a)\n","    - The input texts in the batch have different lengths.\n","    - Complete your code to make them have same length using their average.\n","    - This means that the length of token sequence in each batch is determined by \n","      the average of token length of all sequences in each batch.\n","    - Text tensors are stacked with dimension of (TOKEN_LENGTH, BATCH),\n","      for easier process in RNN model.\n","    - Token length tensors are used to index the last valid hidden token for classification.\n","\n","    Args:\n","      batch: list of tuples, each containing an integer label and a text input.\n","      - ex) [(3, \"Wall St. Bears...\"), (4, \"Comtes, Asteroids and ...\"), ...]\n","      - number of tuples in the list is same as BATCH SIZE.\n","\n","    Returns:\n","      text_list: batch of encoded long type text tensors with size (TOKEN_LENGTH, BATCH)\n","      label_list: batch of label tensors with size (BATCH)\n","      len_list: batch of token length tensors with size (BATCH)\n","    \"\"\"\n","\n","    ##### YOUR CODE #####\n","    \n","    text_list, label_list, len_list = [], [], []\n","    for (_label, _text) in batch:\n","        length = torch.tensor(text_pipeline(_text), dtype=torch.int64).size(0)\n","        len_list.append(length)\n","    AVG_LEN = int(sum(len_list) / len(len_list)) # bankers round 때문에 np.round 대신 사용\n","    \n","    for (_label, _text) in batch:    \n","        label_list.append(label_pipeline(_label))\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        if processed_text.size(0)>= AVG_LEN:\n","          processed_text = processed_text[:AVG_LEN]\n","        else:\n","          processed_text = torch.cat([processed_text,torch.zeros(AVG_LEN - processed_text.size(0))])\n","        text_list.append(processed_text)\n","        \n","    \n","    text_list = torch.stack(text_list, dim = 1).long()\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    len_list = torch.tensor(len_list)\n","    \n","    assert text_list.size(1) == len(batch)\n","    return (text_list, label_list, len_list)\n","    #####################"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nJXoUuq0NgiV"},"outputs":[],"source":["\"\"\"\n","Load the data loader.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","BATCH_SIZE = 512\n","\n","train_dataset = to_map_style_dataset(train_data)\n","test_dataset = to_map_style_dataset(test_data)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=False, collate_fn=collate_batch)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4X2_aQKhxWV3"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[  846,   472,   340,  ..., 20308,   452,  2209],\n","        [    3,  2188,   108,  ..., 13928,    58,    12],\n","        [   77,     7,   946,  ...,    20,    92,  8976],\n","        ...,\n","        [  846,    16,    58,  ...,  3468,    23,  2820],\n","        [ 1299,  1434,    43,  ..., 49584,    73,    10],\n","        [   77,   875,   294,  ...,  3212,   452,   964]])\n","tensor([3, 0, 2, 2, 2, 1, 0, 2, 1, 3])\n","tensor([40, 27, 46, 46, 37, 28, 19, 34, 35, 21])\n"]}],"source":["\"\"\"\n","Print out the first batch in the train loader.\n","Check if the collate function is implemented correctly.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","batch_x, batch_y, len_x = next(iter(train_dataloader))\n","print(batch_x[:10])\n","print(batch_y[:10])\n","print(len_x[:10])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"95s-BUHc37O_"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"\n","Plot the sequence length distribution of the batches in the train dataloader.\n","Make sure that all batches have difference sequence lengths.\n","\n","Do NOT modify.\n","\"\"\"\n","\n","batch_len = []\n","for batch_x, _, _ in train_dataloader:\n","    seq_len = batch_x.size(0)\n","    batch_len.append(seq_len)\n","plt.hist(batch_len)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kR_cAYRUVbor"},"source":["# Model"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9CMsOucsVbVE"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        input_size: int,\n","        hidden_size: int,\n","        num_class: int,\n","        dropout_ratio: float,\n","    ):\n","        \"\"\"\n","        Define the model weight parameters and initialize the weights.\n","\n","        Question (b)\n","        - Complete the dimension and shape of the weights and biases.\n","        - Use the model parameters (vocab_size, input_size, hidden_size, num_class).\n","\n","        Args:\n","          vocab_size: size of dictionary of vocabularies.\n","          input_size: size of each embedding vector.\n","          hidden_size: size of hidden dimension. \n","          num_class: size of output classes.\n","          dropout_ratio: probability of an element to be zeroed.\n","        \"\"\"\n","        super(RNN, self).__init__()\n","\n","        ##### YOUR CODE #####\n","        whh_size = (hidden_size, hidden_size)\n","        wxh_size = (hidden_size, input_size)\n","        why_size = (num_class, hidden_size)\n","        bhh_size = (hidden_size ,1)\n","        bxh_size = (hidden_size, 1)\n","        bhy_size = (num_class, 1)\n","        #####################\n","\n","        kwargs = {'device': DEVICE, 'dtype': torch.float}\n","        self.dropout = dropout_ratio\n","        self.hidden = hidden_size\n","        self.num_class = num_class\n","        self.embedding = nn.Embedding(vocab_size, input_size)\n","        self.W_hh = nn.parameter.Parameter(torch.empty(whh_size, **kwargs))\n","        self.W_xh = nn.parameter.Parameter(torch.empty(wxh_size, **kwargs))\n","        self.W_hy = nn.parameter.Parameter(torch.empty(why_size, **kwargs))\n","        self.b_hh = nn.parameter.Parameter(torch.empty(bhh_size, **kwargs))\n","        self.b_xh = nn.parameter.Parameter(torch.empty(bxh_size, **kwargs))\n","        self.b_hy = nn.parameter.Parameter(torch.empty(bhy_size, **kwargs))\n","\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        \"\"\"\n","        Initialize the parameters with Kaiming uniform initialization.\n","\n","        Do NOT modify this method.\n","        \"\"\"\n","        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hh)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.b_hh, -bound, bound)\n","        nn.init.kaiming_uniform_(self.W_xh, a=math.sqrt(5))\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_xh)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.b_xh, -bound, bound)\n","        nn.init.kaiming_uniform_(self.W_hy, a=math.sqrt(5))\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hy)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.b_hy, -bound, bound)\n","\n","    def forward(self, inputs: torch.Tensor, length: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Question (c)\n","        - Randomly initialize h_0 with appropriate shape.\n","        - Pass a sequence of tokens into the recurrent network.\n","        - Implement dropout to embedded tokens with the given probability (self.dropout).\n","          For example, if self.dropout is 0.3, 30% of the embedded tokens will be dropped out.\n","        - We do not want to use a hidden cell of a zero-padded token for classification!\n","        - Index the hidden cell of the last valid token (excluding the zero-padding)\n","          based on the token length of each example in the batch.\n","        - Do NOT use pre-defined PyTorch layers for this question. (e.g. nn.RNN, nn.Dropout)\n","\n","        Args:\n","          inputs: a batch of encoded token sequences with shape (SEQ_LEN, BATCH_SIZE)\n","          length: a batch of token lengths with shape (BATCH_SIZE)\n","          \n","        Returns:\n","          Softmax probabilites for each class with shape (BATCH_SIZE, NUM_CLASS)\n","        \"\"\"\n","\n","        ##### YOUR CODE #####\n","        \n","        seq_len = inputs.size(0)\n","        batch_size = length.size(0)\n","        \n","        x_embed = self.embedding(inputs) # (seq_len, batch_size, input_size)\n","        \n","        # dropout     \n","        mask = (torch.rand(list(x_embed.size()))<self.dropout) / self.dropout \n","        x_embed = mask * x_embed     \n","                    \n","        hiddens = [] \n","        h_0 =torch.randn(self.hidden, batch_size).to(DEVICE)   \n","        hidden = h_0\n","        hiddens.append(hidden)\n","        \n","        # forward 1        \n","        for t in np.arange(seq_len): \n","          hidden = torch.tanh((torch.matmul(self.W_hh, hidden) + self.b_xh) + \n","                              (torch.matmul(self.W_xh, x_embed[t,:,:].T) + self.b_hh)).to(DEVICE)\n","          hiddens.append(hidden)         \n","\n","        hiddens_temp = torch.zeros((self.hidden, batch_size)).to(DEVICE)      \n","        for i in np.arange(batch_size):\n","          if length[i] < seq_len:\n","            hidden_length = hiddens[int(length[i])]\n","            hiddens_temp[:,i] = hidden_length[:,i]\n","          else:\n","            hidden_length = hiddens[seq_len]\n","            hiddens_temp[:,i] = hidden_length[:,i]   \n","            \n","        # Y_pred           \n","        softmax = nn.Softmax(dim=0)\n","        softmax_probs = softmax(torch.matmul(self.W_hy, hiddens_temp) + self.b_hy).permute(1,0)\n","        \n","        return softmax_probs \n","        #####################\n","\n","    def compute_loss(\n","        self,\n","        prediction: torch.Tensor, \n","        label: torch.Tensor) -> Tuple[torch.Tensor, int]:\n","        \"\"\"\n","        Question (d)\n","        - Compute the cross entropy loss and the number of correct predictions\n","        - Do NOT use loss function in torch.nn library (e.g. nn.CrossEntropyLoss())\n","\n","        Args:\n","          prediction: output(softmax probabilities) from self.forward function with shape (BATCH_SIZE, NUM_CLASS)\n","          label: integer labels of the batch inputs with shape (BATCH_SIZE)\n","        \n","        Returns:\n","          cross entropy loss of the batch (float tensor) and the number of correct predictions (integer)\n","        \"\"\"\n","\n","        ##### YOUR CODE #####\n","        loss = 0\n","        correct = 0\n","        \n","        batch_size = label.size(0)\n","                \n","        label_enc = F.one_hot(label, self.num_class)\n","        label_enc = label_enc.to(DEVICE)\n","\n","        loss = - torch.sum(label_enc * torch.log(prediction))\n","        correct = int((label == prediction.argmax(1)).sum())\n","        loss /= batch_size \n","        \n","        return (loss, correct)\n","        #####################"]},{"cell_type":"markdown","metadata":{"id":"zIdypCAFscfV"},"source":["# Training Modules"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"o9lUSQJjsfrQ"},"outputs":[],"source":["class ScheduledOptim():\n","    \"\"\"\n","    Learning rate scheduler.\n","\n","    Do NOT modify.\n","    \"\"\"\n","\n","    def __init__(self, optimizer, n_warmup_steps, decay_rate):\n","        self._optimizer = optimizer\n","        self.n_warmup_steps = n_warmup_steps\n","        self.decay = decay_rate\n","        self.n_steps = 0\n","        self.initial_lr = optimizer.param_groups[0]['lr']\n","        self.current_lr = optimizer.param_groups[0]['lr']\n","\n","    def zero_grad(self):\n","        self._optimizer.zero_grad()\n","    \n","    def step(self):\n","        self._optimizer.step()\n","    \n","    def get_lr(self):\n","        return self.current_lr\n","    \n","    def update(self):\n","        if self.n_steps < self.n_warmup_steps:\n","            lr = self.n_steps / self.n_warmup_steps * self.initial_lr\n","        elif self.n_steps == self.n_warmup_steps:\n","            lr = self.initial_lr\n","        else:\n","            lr = self.current_lr * self.decay\n","        \n","        self.current_lr = lr\n","        for param_group in self._optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","        self.n_steps += 1"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pgisHLbUsLkM"},"outputs":[],"source":["\"\"\"\n","Functions for training and evaluating the model.\n","\n","Question (e)\n","- There has been minor changes with the model forward operation and loss computation.\n","  Check what has been changed from the train and evaluate functions that we have used previously,\n","  and complete the train and evaluate function that works for the current model architecture.\n","- Use the methods of the ScheduledOptim class above to perform necessary operations on the optimizer.\n","- Do NOT change the arguments given to the train, evaluate functions.\n","\"\"\"\n","\n","def train(model, train_loader, scheduler):\n","    ##### YOUR CODE #####\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","\n","    tqdm_bar = tqdm(train_loader)\n","\n","    for text, label, length in tqdm_bar:\n","        text = text.to(DEVICE)\n","        label = label.to(DEVICE)\n","        length = length.to(DEVICE)\n","                \n","        scheduler.zero_grad()        \n","        class_pred = model(text, length)\n","        loss_temp, correct_temp = model.compute_loss(class_pred, label)\n","        loss_temp.backward()\n","        scheduler.step()\n","        train_loss += loss_temp\n","        correct += correct_temp \n","    \n","    scheduler.update()\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = 100. * correct / len(train_loader.dataset)\n","\n","    return train_loss, train_acc\n","    #####################\n","\n","\n","def evaluate(model, test_loader):\n","    ##### YOUR CODE #####\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    tqdm_bar = tqdm(test_loader)\n","\n","    with torch.no_grad():\n","        for text, label, length in tqdm_bar:\n","            text = text.to(DEVICE)\n","            label = label.to(DEVICE)\n","            length = length.to(DEVICE)\n","    \n","            class_pred = model(text, length)\n","            loss_temp, correct_temp = model.compute_loss(class_pred, label) \n","            test_loss += loss_temp\n","            correct += correct_temp         \n","\n","    test_loss /= len(test_loader.dataset)\n","    test_acc = 100. * correct / len(test_loader.dataset)\n","\n","    return test_loss, test_acc\n","    #####################"]},{"cell_type":"markdown","metadata":{"id":"a1qXY-CDNIia"},"source":["# Model Training"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"EA061dLNNKg2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ffa08ec7e2a43b385e98aab5daccc39","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fef76e81e0b14c6c8572cfd38e0a9fd1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  1 | lr: 0.0000 | train loss:    0.003 | train accuracy:   34.801 | valid accuracy   43.211 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ccbff931e3e4a82a8f5c2fa0b8129e4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c1fe04fe806405391e9d8940ccceea4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  2 | lr: 0.0002 | train loss:    0.002 | train accuracy:   42.688 | valid accuracy   43.171 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"828b6a9fb89743a6a3e0bdded970bcf7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4053cf99a9a44bfaa846be5e6c8587c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  3 | lr: 0.0004 | train loss:    0.002 | train accuracy:   44.939 | valid accuracy   46.316 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e2362ddd10c4e33ba18ea28ae03b7d1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cc819925b84415b96a2d127439ba6b8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  4 | lr: 0.0006 | train loss:    0.002 | train accuracy:   49.015 | valid accuracy   50.842 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95ba324832ee40358dff7a44b0536ada","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"095996972db2456fa13cd54a58775840","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  5 | lr: 0.0008 | train loss:    0.002 | train accuracy:   54.191 | valid accuracy   56.053 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6131a2ae6fd4e49bb2256a1d9e9ec0b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c38b65739eb5496eb1d4670597a16892","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  6 | lr: 0.0010 | train loss:    0.002 | train accuracy:   59.338 | valid accuracy   61.447 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d59c5c72d383475b9d78c6c4f75b010f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ca7c943b77d4ac1939c1425a59f0cce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  7 | lr: 0.0008 | train loss:    0.002 | train accuracy:   64.619 | valid accuracy   67.066 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2c8d81afb634bfe9ff4d762d71e7c70","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09cb45d4db4a43cc9521c4ea36bb0015","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  8 | lr: 0.0006 | train loss:    0.001 | train accuracy:   69.592 | valid accuracy   72.197 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"367f103a85404fd8a5160bc79d61f999","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81f12e1d7aa54398af45559284db07e5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch  9 | lr: 0.0005 | train loss:    0.001 | train accuracy:   73.544 | valid accuracy   72.592 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd90311a70914ccebcdadc07976b3542","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dd3104e15d447ff8ea9b817b721270c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch 10 | lr: 0.0004 | train loss:    0.001 | train accuracy:   75.653 | valid accuracy   74.908 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53cfcd409a0846ddb343baad532582e0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85095da61cde4d62a20dfdc283868508","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch 11 | lr: 0.0003 | train loss:    0.001 | train accuracy:   77.224 | valid accuracy   76.961 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14d0ad73d8c54e3faa9e746d9f15b0a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2bc2dfd0c1e4b7d8d6d838e3f645f87","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch 12 | lr: 0.0003 | train loss:    0.001 | train accuracy:   78.388 | valid accuracy   77.250 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"323bfb961e694f7caaf7221cb4fa489e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f341a4cf5674dbf90a78af30dcaefb5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch 13 | lr: 0.0002 | train loss:    0.001 | train accuracy:   79.117 | valid accuracy   78.066 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28948379cbc1474f879e595cf32af306","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31dff96242414c8ba8f9096b4a9a60fc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch 14 | lr: 0.0002 | train loss:    0.001 | train accuracy:   79.552 | valid accuracy   78.882 \n","-----------------------------------------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5e3fb21e65d472c8226957dcf5cd827","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/235 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"499d3140d21c428689b3a0857995c2bf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------------------\n","| end of epoch 15 | lr: 0.0001 | train loss:    0.001 | train accuracy:   79.949 | valid accuracy   79.342 \n","-----------------------------------------------------------------------------------\n"]}],"source":["\"\"\"\n","Question (f)\n","- Train your RNN model and obtain the test accuracy of 70%.\n","- Select the input size, hidden size of your choice\n","- Try various optimizer type, learning rate and scheduler options for the best performance.\n","- Visualize your experiments with Tensorboard.\n","- Your TensorBoard results should include Train/Validation Loss and Accuracy.\n","\"\"\"\n","\n","##### YOUR CODE #####\n","writer = SummaryWriter(log_dir=\"./logs\")\n","EPOCHS = 15\n","BATCH_SIZE = 512\n","vocab_size = len(encoder) # 95811\n","input_size = 45\n","hidden_size = 128\n","num_class = 4\n","dropout_ratio = 0.3\n","learning_rate = 0.001\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                              shuffle=False, collate_fn=collate_batch)\n","\n","model = RNN(vocab_size, input_size, hidden_size, num_class,dropout_ratio)\n","model = model.to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = ScheduledOptim(optimizer, 5, 0.8)\n","\n","for epoch in range(1, EPOCHS + 1):\n","    loss_train, accu_train = train(model, train_dataloader, scheduler)\n","    writer.add_scalar(\"Loss/train\", loss_train,epoch)\n","    writer.add_scalar(\"Accuracy/train\", accu_train,epoch)\n","    loss_val, accu_val = evaluate(model, valid_dataloader)\n","    writer.add_scalar(\"Loss/test\", loss_val,epoch)\n","    writer.add_scalar(\"Accuracy/test\", accu_val,epoch)\n","\n","    lr = scheduler.get_lr()\n","    print('-' * 83)\n","    print('| end of epoch {:2d} | lr: {:5.4f} | train loss: {:8.3f} | train accuracy: {:8.3f} | '\n","          'valid accuracy {:8.3f} '.format(epoch, lr, loss_train, accu_train, accu_val))\n","    print('-' * 83)\n","\n","writer.flush()\n","writer.close()\n","#####################"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 20996), started 1 day, 0:41:02 ago. (Use '!kill 20996' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-77256717cd312901\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-77256717cd312901\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%tensorboard --logdir ./logs"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"9GT7GbY67idN"},"outputs":[],"source":["# Run this cell to upload the result to TensorBoard.dev\n","# Then you will get the shared link.\n","\n","# tensorboard dev upload --logdir 'e:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제4\\HW4' --name \"MLDL1 HW4\" --description \"Training results from HW4 RNN Problem\" --one_shot"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"o4Xg5bak-RlP"},"outputs":[],"source":["# Write down the shared link in the below cell."]},{"cell_type":"markdown","metadata":{"id":"-SPJ3htk7mzH"},"source":["Write down the TensorBoard link in this cell.\n","\n","https://tensorboard.dev/experiment/1816CBEmQJm0J3ZCKYEE6A/"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('koreait')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"f47a3967e128bc4105a3feb1b049d3674a6a7e31f136575bb78963f94711a914"}}},"nbformat":4,"nbformat_minor":0}
