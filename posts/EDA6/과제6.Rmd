---
title: "EDA Assignment 6: Chapter 8"
author: "Sangwon Ju, Yonsei Public Administration"
date: 'may/26/2021'
page-layout: full
format: 
    html
categories: 
    "Exploratory Data Analysis (2021 Spring) [in Korean]"
image: "image.png"
---

8장. 시계열 자료의 탐색

## 1번

FRIDAY.DAT 자료는 영국에서 1986년 금요일에 발생한 자동차 사고 사망자 수이다. 첫 열은 하루를 24시간으로 표시한 것이고, 두 번째 열은 사망자 수이다. 시간에 따른 사망자 수를 평활하고 시간과 사망자 수의 관계를 서술하여라.

데이터를 불러오고,첫번째 줄에 있는 데이터를 0시부터 23시로 변경하여 Time-series Plot을 그릴 때 보다 가독성을 좋게 만들고자 하였다. Time은 시작시간을 기준으로 한시간의 데이터를 포함하며 시작시간은 당일 0시부터 끝나는 시간은 익일 0시까지이다.

```{r}
pacman::p_load("haven","sleekts")
friday=data.frame(
  stringsAsFactors = FALSE,
                V1 = c("0-1","1-2","2-3","3-4",
                       "4-5","5-6","6-7","7-8","8-9","9-10","10-11","11-12",
                       "12-13","13-14","14-15","15-16","16-17","17-18",
                       "18-19","19-20","20-21","21-22","22-23","23-0"),
                V2 = c(938L,621L,455L,207L,138L,
                       215L,526L,1933L,3377L,2045L,2078L,2351L,3015L,
                       2966L,2912L,4305L,4923L,4427L,3164L,2950L,2601L,2420L,
                       2557L,4319L)
)
names(friday)=c("Time", "Deaths")
friday$Time=0:23
friday
```

결측치를 확인하고 Box plot을 활용해서 Outlier가 존재하는지를 확인한다.

```{r}
sum(is.na(friday)) # 0 결측치는 존재하지 않음.
boxplot(friday$Deaths,main= "Checking Outlier of Deaths Data") 
#outlier는 존재하지 않는 것이 확인되어진다. 
```

Boxplot을 통해 확인 할 경우 outlier는 확인되어지지는 않으나 Whisker의 길이가 길게 뻗어 있는 것이 확인되어 진다. Whisker의 길이와 median의 위치를 고려해볼 때, 자료가 완벽하게 대칭적이지는 않은 것으로 확인되어 진다.

```{r}
stem(friday$Deaths)
stem(friday$Deaths,2)
```

Stem and Leaf Plot을 통해 데이터를 분석할 경우 데이터의 특성을 분석하려고 하였으나 어떠한 특징을 보이고 있는 것으로 보이지는 않는다. 다만, Stem Plot의 Scale을 약간 조정할 경우 Gap이 있는 것으로 보이는데 세개의 그룹으로 나누어지는 것 처럼 보인다.

```{r}
skewness = function(x) {
  hl=fivenum(x)[2]
  median=fivenum(x)[3]
  hu=fivenum(x)[4]
  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))
  return(skew)
}
skewness(friday$Deaths)
```

데이터의 Skewness를 고려해 볼 떄 Box plot에서 확인되였던 것 처럼, 해당 데이터는 Skewed to the left되어 있다는 사실을 확인할 수 있다.

```{r}
skewness(log(friday$Deaths))
skewness(sqrt(friday$Deaths))
skewness(-1/(friday$Deaths))
skewness(-1/sqrt(friday$Deaths))
# 변환을 하면 오히려 skewness가 더 커지고 있다.
```

```{r}
# letter value display
source("http://mgimond.github.io/ES218/es218.R")
(lvd=lsum(friday$Deaths,6)) # mid 값이 점차 커지는 중
# Kurtosis (E-spread / H-spread - 1.705)
(lvd[3,5]-lvd[3,3])/(lvd[2,5]-lvd[2,3]) -1.705 # more peaked than normal
```

-   데이터의 Five Numbers와 요약 지표

```{r}
fivenum(friday$Deaths)
summary(friday$Deaths)# Median, Mean이 큰 차이가 없다.
as.numeric(summary(friday$Deaths)[6]-summary(friday$Deaths)[1]) # 최고점과 최저점 사이의 차이는 4785
```

최고값의 경우 4923회의 사고 / 최소값의 경우 138회의 사고 / 평균적으로 2370회의 사고가 발생하였다.

정규성을 따르는가?

```{r}
qqnorm(friday$Deaths, ylab="Death quantiles");qqline(friday$Deaths, col='red',lty=2)
fiv=fivenum(friday$Deaths)
(pseudosigma = (fiv[4]-fiv[2])/1.34)
sd(friday$Deaths)
```

```{r}
qqnorm(friday$Deaths, ylab="Death quantiles");qqline(friday$Deaths, col='red',lty=2)
abline(fiv[3],pseudosigma,col="blue",lty=2)
title(sub="intercept=2488.5 (median); slope=1723.9 (pseudo-sigma) blue line")
```

pseudo-sigma와 sigma가 큰 차이가 없고 자료들이 어느정도 직선을 따르고 있는 것을 보인다. (완벽히 직선을 이루는 것은 아니나 - 중간에 곡선형태를 보임 - 직선위에 있는 것처럼 ) 다만 위에서 Stem-Leaf Plot을 그리면서 언급하였던 것 처럼 세개의 cluster가 나타나고 있는 것이 확인되어 진다.

```{r}
library(ggplot2)
friday2=friday
((friday2$Deaths)/sum(friday2$Deaths))*100

ggplot(friday,aes(x=Deaths))+
    geom_histogram(aes(y = stat(count) / sum(count)),binwidth=400, fill = "pink", colour = "black")+
    labs(title="Histogram of deaths in friday", x ="# of Deaths", y = "Density")+ 
    theme(plot.title = element_text(hjust = 0.5))+ 
    scale_y_continuous(labels = scales::percent)+
    theme_minimal()+
    ggeasy::easy_center_title()
```

Cluster를 재확인 할 수 있음. \# K-means Clustering 사용하여 세개의 군집분류

```{r}
kmeans(friday$Deaths, 3) # 13개, 4개, 7개
```

군집분류를 할 경우 세개의 군집이 나타나는데 0시부터 \~ 6까지의 첫번째 Cluster는 심야시간대의 사고로 인한 사망이므로 수가 많지 않은 편이다. 반면, 두번째 Cluster는 일과시간대인 7시부터 15시, 퇴근시간 이후인 18시부터 23시의 경우 사망건수가 중간수준으로 나타나고, 15시부터 18시 사이와 23시부터 24시 사이의 3번째 Cluster에서 사망건수가 가장 많이 나타난다. 퇴근 시간대보다 일과시간대에서 사고가 더 많이 확인되는 것이 확인된다. 특이시간대는 23\~24시 사이 구간인데, 음주를 하거나 모임을 가질 때 보통 23시 이후 집에 귀가를 많이 하는데 귀가 과정에서 음주운전 사고가 발생한 것으로 추측되어지지만 확실하지 않다.

```{r}
ts_cluster=cbind(friday,as.factor(kmeans(friday$Deaths, 3)$cluster))
names(ts_cluster)=c("Time","Deaths","Cluster")
ts_cluster
```

```{r}
ggplot(data=ts_cluster,aes(x=Time, y=Deaths, colour=Cluster)) + 
   geom_point(shape=19, size=4) + 
   geom_text(label=ts_cluster$Time,hjust=0, vjust=-1)+
   ggtitle("Scatter Plot of Deaths' K-means clusters")+
   theme_bw()
ts.plot(friday,main="Timeseries plot of deaths")

```

교통사고 사망자 수를 평활하기 위해서 수업시간에 교수님께서 Recommend 하신 3RSSH Twice방법과 4253H Twice방법을 활용하고자 한다.

Recommend 해주신 방법들을 사용하기에 앞서 3RSS방법과 3RS3R방법으로 평활을 진행해 보았다.

```{r}
ts.plot(friday,main="Default, 3RSS twice, 3RS3R twice",ylab="Deaths",lty=2)
lines(ts(smooth(friday$Deaths, kind="3RSS",twiceit=T)),col="red")
lines(ts(smooth(friday$Deaths, kind="3RS3R",twiceit=T)),col="blue")
legend(x = 1.5, y = 5000, c("Default", "3RSS Twice","3RS3R Twice"), 
      lty=c(2,1,1),lwd=2,col = c("black","red","blue"))
```

Hanning을 적용하기 전 데이터이다. 23시부터 24시 구간의 이상치가 어느정도 완화된 것을 확인할 수 있다. (다만 그 정도가 커보이기 때문에 조정이 필요할 것으로 보인다.) 또한, 8시 부근의 뾰족한 지점이 어느정도 완화되었고, 다른 Cluster 3에 해당하던 값들의 크기도 다소 작아진 것이 확인되어진다. 다만, 평평한 부분이 연속적으로 존재하는 것이 확인되어지기에 Hanning의 적용이 필요할 것으로 보인다. (전반적으로 3RSS(split 두번)나 3RS3R(Repeated 3 두번)에서는 큰 차이가 나타나고 있지는 않은 것으로 보인다.)

다음으로는, 3RSSH, twice방식을 적용한 평활을 적용하였다.

다만, smoothing 함수에 3RSSH twice 방식의 평활을 지원하지 않기 때문에 함수를 만들어 smooting을 진행해야 할 것으로 보인다. https://blog.daum.net/wonil2480/13 의 방식을 참고하여 함수를 형성하였음.

```{r}
smooth_3RSSH=function(data){
    smooth3RSS=smooth(data, kind="3RSS")
    
    n=length(data)
    smooth3RSSH=smooth3RSS
    
    
    for (i in 2:(n-1)) {smooth3RSSH[i] <- smooth3RSS[i-1]/4 + smooth3RSS[i]/2 + smooth3RSS[i+1]/4}
    smooth3RSSH[1] <- smooth3RSS[1]; smooth3RSSH[n] <- smooth3RSS[n]
    rough=data-smooth3RSSH
    roughH=rough
    
    smooth3RSS2=smooth(rough,kind="3RSS")
    
    for (i in 2:(n-1)) roughH[i] <- smooth3RSS2[i-1]/4 + smooth3RSS2[i]/2 + smooth3RSS2[i+1]/4
    roughH[1] <- smooth3RSS2[1]; roughH[n] <- smooth3RSS2[n]
    out=smooth3RSSH+roughH
    out=as.vector(out)
    return(out)}

#install.packages("LearnEDA")
#library(LearnEDA)

#smooth.3RSSH.twice=function(data)
#{
#SMOOTH=han(smooth(attend,kind=”3RSS”)) # 3RSSH smooth
#ROUGH=data-SMOOTH                      # computes the rough
#SMOOTH+han(smooth(ROUGH,kind=”3RSS”)) 

#원래 있는 패키지를 활용한 방식과도 비교하였으나 차이가 없었다. 
#(CRAN에서 현재 삭제되어 이전 패키지를 다운받아 사용하였음)
```

```{r}
ts.plot(friday,main="Default, 3RSSH Twice",ylab="Deaths",lty=2)
lines(ts(smooth_3RSSH(friday$Deaths)),col="goldenrod4")
legend(x = 1.5, y = 5000, c("Default", "3RSSH Twice"), 
      lty=c(2,1),lwd=2,col = c("black","goldenrod4"))
```

3RSS나 3RS3R와 전반적인 추세선은 크게 다르지 않은 것으로 보인다. 다만 Cluster 3 (15시부터 18시 사이와 23시부터 24시 사이)에 해당하는 크기가 큰 자료들이 3RSSH자료에서 조금 더 잘 드러나고 있다. 동일하게 8시부터 9시 부근에 형성된 뾰족한 Outlier로 보이는 자료는 Smoothing 된것으로 확인된다.

마지막으로 4253H, Twice 방식으로 평활한 데이터를 만들고자 한다. Sleekts 패키지의 sleek 활용하여 4253H, Twice 평활을 진행할 것이다.

```{r}
#install.packages("sleekts")
library(sleekts)
sleek(friday$Death)
ts.plot(friday,main="Default, 4253H Twice",ylab="Deaths",lty=2)
lines(ts(sleek(friday$Death)),col="cyan3")

legend(x = 1.5, y = 5000, c("Default", "4253H Twice"), 
      lty=c(2,1),lwd=2,col = c("black","cyan3"))
```

4253H 방식의 평활은 다른 방식들에 비해서 곡선의 형태가 부드럽게 나타나는 것이 확인된다. 또한 각 Cluster들 별로 특성이 명확하게 드러나고 있다. Cluster 3 (15시부터 18시 사이와 23시부터 24시 사이)에 해당하는 값들이 부드럽게 시계열 그래프 안에서 표현되고 있는 것을 확인할 수 있다. Raw Data에 존재하지 않는 값을 활용해서 양끝값을 극단치에 영향을 최소화한 형태로 표현했고, Hanning을 통해서 평평한 지점들을 보다 부드러운 곡선 형태로 만들었다는 점에서 다른 평활법에 비해서 더 잘 표현되어 있다고 볼 수 있다.

```{r}
ts.plot(friday,main="ALL",ylab="Deaths",lty=2)
lines(ts(smooth(friday$Deaths, kind="3RSS")),col="red")
lines(ts(smooth(friday$Deaths, kind="3RS3R")),col="blue")
lines(ts(smooth_3RSSH(friday$Deaths)),col="goldenrod4")
lines(ts(sleek(friday$Death)),col="cyan3")
legend(x = 1.5, y = 5000, c("Default","3RSS Twice","3RS3R Twice","3RSSH Twice","4253H Twice"), 
      lty=c(2,1,1,1,1),lwd=2,col = c("black","red","blue","goldenrod4","cyan3"))
```

모든 평활법을 비교할경우 다음과 같은 그래프를 그릴 수 있다. 앞에서 언급하였듯이 4253H, Twice 평활법이 가장 적절한 방법이라고 생각된다. 8시부터 9시 사이의 값, 16시 부터 17시 사이의 값 그리고 23시부터 24시 사이의 값이 변환전 자료에 비해 많이 감소되었다. 앞의 두 시간대는 출퇴근 시간이기 때문에 교통량이 많으므로 교통사고가 날 확률이 높아지고 사망사고가 발생할 확률도 다른 시간대에 비해서 높다고 볼 수 있다. 또한 23시부터 24시 시간대의 경우 앞에서 언급한 것과 같이 모든 평활법에서 실제 데이터보다 낮게 평활이 나타났는데, 전체적인 곡선을 고려할 때 이질적인 데이터이므로 그 원인에 대해서 심도있게 고민해볼 필요가 존재한다. Raw Data 기준으로 새벽시간대에는 교통량이 적어 사망사고건수도 적은 편에 비해서 일과시간대와 퇴근시간대에는 사망사고 건수가 많이 나타나고 있고 일과시간대에 퇴근시간대에 비해서 더 많은 사망사고가 발생한다는 것을 확인할 수 있다. 반면 평활된 데이터의 경우 퇴근시간대에 더 많은 사망사고가 발생하는 것으로 보여주고 있는데 평활된 데이터를 해석할 떄는 Raw Data를 유의해서 해석하는 것이 필요하다는 것을 확인할 수 있다.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2564438/ 한 보고서에 따르면 사고가 발생하는 원인을 세가지를 제시하였는데 A credible physiological explanation for the importance of good lighting for road safety is provided; low luminance, low contrast images are processed slowly by the visual apparatus, due mainly to the limited temporal characteristics of the rod photoreceptors.

the rod photoreceptors의 제한된 시간적 특성은 야간의 낮은 불빛, 낮은 이미지의 대비로 인해서 시신경의 피로를 불러 일으키고 그 결과 높은 사망사고율로 이어졌음을 제시하였다.

전날 심야시간대에 사망사고 발생건수가 낮은 것에 비해 토요일 0시에 사망사고 발생건수가 높아 진 것은 주말에 앞서 사람들이 밤 늦게 까지 돌아다녀서 그런 것으로 추측되어지기도 하다. 시계열데이터의 왼쪽과 오른쪽이 대칭적이지 않은 이유는 금요일 0시와 토요일 0시(금요일 24시)가 사람들의 시각에서 질적으로 다르기 때문이라고 생각한다. 띠라서 데이터를 더 잘 이해하기 위해서는 목요일과 토요일의 시계열 자료를 확인하는 것이 필요할 것으로 보인다. (요일적 요인의 고려)

#2번 R의 datasets에 있는 sunspot.year 자료를 평활하고 그 패턴을 기술하라. 주기가 있는지? 많아지는 "속도"와 줄어드는 "속도"도 주목하여라.

```{r}
sunspot.year=tibble::tribble(
    ~time, ~value,
    1700L,      5,
    1701L,     11,
    1702L,     16,
    1703L,     23,
    1704L,     36,
    1705L,     58,
    1706L,     29,
    1707L,     20,
    1708L,     10,
    1709L,      8,
    1710L,      3,
    1711L,      0,
    1712L,      0,
    1713L,      2,
    1714L,     11,
    1715L,     27,
    1716L,     47,
    1717L,     63,
    1718L,     60,
    1719L,     39,
    1720L,     28,
    1721L,     26,
    1722L,     22,
    1723L,     11,
    1724L,     21,
    1725L,     40,
    1726L,     78,
    1727L,    122,
    1728L,    103,
    1729L,     73,
    1730L,     47,
    1731L,     35,
    1732L,     11,
    1733L,      5,
    1734L,     16,
    1735L,     34,
    1736L,     70,
    1737L,     81,
    1738L,    111,
    1739L,    101,
    1740L,     73,
    1741L,     40,
    1742L,     20,
    1743L,     16,
    1744L,      5,
    1745L,     11,
    1746L,     22,
    1747L,     40,
    1748L,     60,
    1749L,   80.9,
    1750L,   83.4,
    1751L,   47.7,
    1752L,   47.8,
    1753L,   30.7,
    1754L,   12.2,
    1755L,    9.6,
    1756L,   10.2,
    1757L,   32.4,
    1758L,   47.6,
    1759L,     54,
    1760L,   62.9,
    1761L,   85.9,
    1762L,   61.2,
    1763L,   45.1,
    1764L,   36.4,
    1765L,   20.9,
    1766L,   11.4,
    1767L,   37.8,
    1768L,   69.8,
    1769L,  106.1,
    1770L,  100.8,
    1771L,   81.6,
    1772L,   66.5,
    1773L,   34.8,
    1774L,   30.6,
    1775L,      7,
    1776L,   19.8,
    1777L,   92.5,
    1778L,  154.4,
    1779L,  125.9,
    1780L,   84.8,
    1781L,   68.1,
    1782L,   38.5,
    1783L,   22.8,
    1784L,   10.2,
    1785L,   24.1,
    1786L,   82.9,
    1787L,    132,
    1788L,  130.9,
    1789L,  118.1,
    1790L,   89.9,
    1791L,   66.6,
    1792L,     60,
    1793L,   46.9,
    1794L,     41,
    1795L,   21.3,
    1796L,     16,
    1797L,    6.4,
    1798L,    4.1,
    1799L,    6.8,
    1800L,   14.5,
    1801L,     34,
    1802L,     45,
    1803L,   43.1,
    1804L,   47.5,
    1805L,   42.2,
    1806L,   28.1,
    1807L,   10.1,
    1808L,    8.1,
    1809L,    2.5,
    1810L,      0,
    1811L,    1.4,
    1812L,      5,
    1813L,   12.2,
    1814L,   13.9,
    1815L,   35.4,
    1816L,   45.8,
    1817L,   41.1,
    1818L,   30.1,
    1819L,   23.9,
    1820L,   15.6,
    1821L,    6.6,
    1822L,      4,
    1823L,    1.8,
    1824L,    8.5,
    1825L,   16.6,
    1826L,   36.3,
    1827L,   49.6,
    1828L,   64.2,
    1829L,     67,
    1830L,   70.9,
    1831L,   47.8,
    1832L,   27.5,
    1833L,    8.5,
    1834L,   13.2,
    1835L,   56.9,
    1836L,  121.5,
    1837L,  138.3,
    1838L,  103.2,
    1839L,   85.7,
    1840L,   64.6,
    1841L,   36.7,
    1842L,   24.2,
    1843L,   10.7,
    1844L,     15,
    1845L,   40.1,
    1846L,   61.5,
    1847L,   98.5,
    1848L,  124.7,
    1849L,   96.3,
    1850L,   66.6,
    1851L,   64.5,
    1852L,   54.1,
    1853L,     39,
    1854L,   20.6,
    1855L,    6.7,
    1856L,    4.3,
    1857L,   22.7,
    1858L,   54.8,
    1859L,   93.8,
    1860L,   95.8,
    1861L,   77.2,
    1862L,   59.1,
    1863L,     44,
    1864L,     47,
    1865L,   30.5,
    1866L,   16.3,
    1867L,    7.3,
    1868L,   37.6,
    1869L,     74,
    1870L,    139,
    1871L,  111.2,
    1872L,  101.6,
    1873L,   66.2,
    1874L,   44.7,
    1875L,     17,
    1876L,   11.3,
    1877L,   12.4,
    1878L,    3.4,
    1879L,      6,
    1880L,   32.3,
    1881L,   54.3,
    1882L,   59.7,
    1883L,   63.7,
    1884L,   63.5,
    1885L,   52.2,
    1886L,   25.4,
    1887L,   13.1,
    1888L,    6.8,
    1889L,    6.3,
    1890L,    7.1,
    1891L,   35.6,
    1892L,     73,
    1893L,   85.1,
    1894L,     78,
    1895L,     64,
    1896L,   41.8,
    1897L,   26.2,
    1898L,   26.7,
    1899L,   12.1,
    1900L,    9.5,
    1901L,    2.7,
    1902L,      5,
    1903L,   24.4,
    1904L,     42,
    1905L,   63.5,
    1906L,   53.8,
    1907L,     62,
    1908L,   48.5,
    1909L,   43.9,
    1910L,   18.6,
    1911L,    5.7,
    1912L,    3.6,
    1913L,    1.4,
    1914L,    9.6,
    1915L,   47.4,
    1916L,   57.1,
    1917L,  103.9,
    1918L,   80.6,
    1919L,   63.6,
    1920L,   37.6,
    1921L,   26.1,
    1922L,   14.2,
    1923L,    5.8,
    1924L,   16.7,
    1925L,   44.3,
    1926L,   63.9,
    1927L,     69,
    1928L,   77.8,
    1929L,   64.9,
    1930L,   35.7,
    1931L,   21.2,
    1932L,   11.1,
    1933L,    5.7,
    1934L,    8.7,
    1935L,   36.1,
    1936L,   79.7,
    1937L,  114.4,
    1938L,  109.6,
    1939L,   88.8,
    1940L,   67.8,
    1941L,   47.5,
    1942L,   30.6,
    1943L,   16.3,
    1944L,    9.6,
    1945L,   33.2,
    1946L,   92.6,
    1947L,  151.6,
    1948L,  136.3,
    1949L,  134.7,
    1950L,   83.9,
    1951L,   69.4,
    1952L,   31.5,
    1953L,   13.9,
    1954L,    4.4,
    1955L,     38,
    1956L,  141.7,
    1957L,  190.2,
    1958L,  184.8,
    1959L,    159,
    1960L,  112.3,
    1961L,   53.9,
    1962L,   37.5,
    1963L,   27.9,
    1964L,   10.2,
    1965L,   15.1,
    1966L,     47,
    1967L,   93.8,
    1968L,  105.9,
    1969L,  105.5,
    1970L,  104.5,
    1971L,   66.6,
    1972L,   68.9,
    1973L,     38,
    1974L,   34.5,
    1975L,   15.5,
    1976L,   12.6,
    1977L,   27.5,
    1978L,   92.5,
    1979L,  155.4,
    1980L,  154.7,
    1981L,  140.5,
    1982L,  115.9,
    1983L,   66.6,
    1984L,   45.9,
    1985L,   17.9,
    1986L,   13.4,
    1987L,   29.2,
    1988L,  100.2
    )

```

```{r}
names(sunspot.year)=c("Time", "Value")
head(sunspot.year,5)


```

Boxplot을 그려서 Outlier 존재 여부를 확인하고, 결측치의 존재여부를 확인한다.

```{r}
boxplot(sunspot.year$Value) # Outer Fence 밖의 값들이 다수 존재하는 것으로 보인다.
sum(is.na(sunspot.year$Value)) # 0이라 결측치가 존재하지 않는다.
```

```{r}
(boxplot(sunspot.year$Value))
length((boxplot(sunspot.year$Value))$out) # 총 7개의 Outlier가 존재한다. 
# upper outer fence의 값은 141.7이다.
subset(sunspot.year,sunspot.year$Value>141.7) 
```

1778년, 1947년, 1957년, 1958년, 1959년, 1979년, 1980년 데이터가 outer fence 밖에서 발견되며 1950년대 후반에 3년 연속으로, 1979\~1980년 2년 연속 outlier를 보였다는 점을 염두에 두고 데이터 분석을 진행해야 할 것으로 보인다.

```{r}
stem(sunspot.year$Value,1) 
```

앞에서 언급하엿었던 것 처럼 총 7개의 outlier를 확인할 수 있고, 전반적으로 skewed되어있는 데이터인 것으로 확인된다.

```{r}
skewness = function(x) {
  hl=fivenum(x)[2]
  median=fivenum(x)[3]
  hu=fivenum(x)[4]
  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))
  return(skew)
}
skewness(sunspot.year$Value)
```

```{r}
skewness(log(sunspot.year$Value))
skewness(sqrt(sunspot.year$Value))
skewness(-1/(sunspot.year$Value))
skewness(-1/sqrt(sunspot.year$Value))
# sqrt 변환을 skewness가 최소화 되는 것으로 보인다.
```

skewness를 계산할 경우 자료가 정규분포에 비해 skewed to the right 라는 사실을 확인할 수 있다.

```{r}
stem(sqrt(sunspot.year$Value)) # 자료의 비대칭성이 다소 개선 된 것으로 보인다.
```

```{r}
# letter value display
source("http://mgimond.github.io/ES218/es218.R")
(lvd2=lsum(sunspot.year$Value,9)) # mid 값이 점차 커지는 중
# Kurtosis (E-spread / H-spread )- 1.705
(lvd2[3,5]-lvd2[3,3])/(lvd2[2,5]-lvd2[2,3])-1.705 # more peaked than normal
```

데이터의 Five Numbers와 요약 지표

```{r}
fivenum(sunspot.year$Value)
summary(sunspot.year$Value)# Median, Mean이 큰 차이가 없다.
as.numeric(summary(sunspot.year$Value)[6]-summary(sunspot.year$Value)[1]) # 최고점과 최저점 사이의 차이는 190.20
```

데이터의 최고점과 최저점 사이의 차이는 190.2정도인 것으로 확인된다. Median과 Mean값 사이의 차이가 꽤 크다는 점에서 skewed to the right 되었다는 사실을 어느정도 확인할 수 있다.

```{r}
qqnorm(sunspot.year$Value, ylab="Sunspots quantiles");qqline(sunspot.year$Value, col='red',lty=2)
fiv=fivenum(sunspot.year$Value)
(pseudosigma = (fiv[4]-fiv[2])/1.34)
sd(sunspot.year$Value) 
```

psudosigma와 실제 표준편차 사이에 큰 차이가 없는 것이 확인된다. 다만 qqplot에서 데이터가 직선을 따르기 보다는 Convex한 곡선을 따르고 있는 것처럼 보인다. (정규분포는 아니다)

```{r}
qqnorm(sunspot.year$Value, ylab="Sunspots quantiles");qqline(sunspot.year$Value, col='red',lty=2)
abline(fiv[3],pseudosigma,col="blue",lty=2)
title(sub="intercept=39.00(median); slope=39.776 (pseudo-sigma) blue line")
```

```{r}
qqnorm(sqrt(sunspot.year$Value), ylab="Sqrt Sunspots quantiles");qqline(sqrt(sunspot.year$Value), col='red',lty=2)
# 변환 이전의 데이터에 비해서 정규분포를 상대적으로 잘 따르고 있는 것으로 보인다. 
```

데이터가 감마분포를 따르는가?

```{r}
Sunspot.sort <- sort(sunspot.year$Value)
(n.Sunspot<- length(sunspot.year$Value))
i <- 1:n.Sunspot

```

```{r}
(mean.Sunspot <- mean(sunspot.year$Value))
(var.Sunspot <- var(sunspot.year$Value))
(shape.Sunspot <- mean.Sunspot^2/var.Sunspot)
(scale.Sunspot <- mean.Sunspot/var.Sunspot)
q.gamma.Sunspot <- qgamma((i-0.5)/n.Sunspot, shape.Sunspot, scale.Sunspot)
plot(q.gamma.Sunspot, Sunspot.sort, main="Gamma prob plot")
line(q.gamma.Sunspot, Sunspot.sort)[2]
abline(a=-4.14999766208764,b=1.11107022976103,col="red",lwd=2) # 위에서 7개의 Outlier들을 제외하면 대부분의 데이터들이 직선 위에 존재하고 있는 것으로 보인다.
```

데이터들이 Outlier들을 제외하면 감마분포를 잘 따르고 있는 것으로 보인다. 다만 데이터들이 0 근처에 Dense하게 몰려있는 것으로 보아 3승근 변환을 통해서 그러한 경향성을 완화하고자 한다.

```{r}
plot(q.gamma.Sunspot^(1/3), Sunspot.sort^(1/3), main="Gamma prob plot")
line(q.gamma.Sunspot^(1/3), Sunspot.sort^(1/3))[2]
abline(a=-0.547632367221911,b=1.15174089923436,col="red",lwd=2)
```

삼승근 변환 후의 데이터들이 직선위에 잘 모여있는 것으로 보아 시계열 데이터들이 감마분포를 잘 다르고 있다고 볼 수 있다.

## 정리

데이터가 현재 정규분포가 아닌 감마분포를 따르고 있는 것으로 보인다. 데이터의 정규성을 확보하기 위해서, sqrt 변환이나 Box Cox Transformation을 활용해 분석을 추가로 진행하고자 한다.

평활을 진행하기 전 데이터가 어떤 경향을 가지고 있는지 Raw data와 차분 데이터에 대한 Plot을 그려 확인한다.

```{r}
# install.packages("forecast")
library(forecast)
sunspot.year=ts(sunspot.year$Value,start=1700)
ts.plot(sunspot.year, main = "Sunspot.year Time-Series Plot",xlim=c(1700,2000))

```

```{r}
lambda <- forecast::BoxCox.lambda(sunspot.year+1e-07)
sunspot.year_new <- forecast::BoxCox(sunspot.year, lambda)
```

```{r}
plot(sunspot.year_new, main = "Box-Cox : Sunspot.year",xlim=c(1700,2000))
plot(diff(sunspot.year_new), main = "Difference & Box-Cox : sunspot.year",xlim=c(1700,2000))
```

Box-Cox Transformation을 통해서 데이터를 변환하였다. 다만, 0인 값이 있어 매우 작은 수를 더해 경향성 만을 확인할 수 있도록 변환하였다. (변환을 적용하기 적합하지 않음)

```{r}
lambda <- forecast::BoxCox.lambda(sunspot.year+1e-07)
sunspot.year_new <- forecast::BoxCox(sunspot.year, lambda)
```

```{r}
ts.plot(sunspot.year_new, main = "Box-Cox : Sunspot.year",xlim=c(1700,2000))

```

변환전 그래프와 변환 후 그래프를 비교할 경우 차분된 데이터를 확인하면 원자료에서는 분산이 증가하는 경향성이 있었는데 두 변환을 사용할 경우 그 경향성이 다소 완화된 것이 확인되어 진다. 순서대로 원자료에 대한 평활 / Box-Cox Transforamtion에 대한 평활 / Sqrt 변환에 대한 평활을 진행하고자 한다. (3RSSH Twice,4253H Twice)

원자료 3RSSH Twice

```{r}
smooth_3RSSH=function(data){
    smooth3RSS=smooth(data, kind="3RSS")
    
    n=length(data)
    smooth3RSSH=smooth3RSS
    
    
    for (i in 2:(n-1)) {smooth3RSSH[i] <- smooth3RSS[i-1]/4 + smooth3RSS[i]/2 + smooth3RSS[i+1]/4}
    smooth3RSSH[1] <- smooth3RSS[1]; smooth3RSSH[n] <- smooth3RSS[n]
    rough=data-smooth3RSSH
    roughH=rough
    
    smooth3RSS2=smooth(rough,kind="3RSS")
    
    for (i in 2:(n-1)) roughH[i] <- smooth3RSS2[i-1]/4 + smooth3RSS2[i]/2 + smooth3RSS2[i+1]/4
    roughH[1] <- smooth3RSS2[1]; roughH[n] <- smooth3RSS2[n]
    out=smooth3RSSH+roughH
    out=as.vector(out)
    return(out)
  }
```

```{r}
ts.plot(sunspot.year,main="Raw Data: Default, 3RSSH Twice",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(smooth_3RSSH(sunspot.year),start=1700,end=1988),col="red")
legend(x = 1700, y = 190, c("Default", "3RSSH Twice"), 
      lty=c(2,1),lwd=2,col = c("black","red"))
```

원자료 4253H Twice

```{r}
ts.plot(sunspot.year,main="Raw Data: Default, 4253H Twice",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(sleek(sunspot.year),start=1700,end=1988),col="blue")
legend(x = 1700, y = 190, c("Default", "4253H Twice"), 
      lty=c(2,1),lwd=2,col = c("black","blue"))
```

Boxcox 3RSSH Twice

```{r}

ts.plot(sunspot.year_new,main="Box-Cox Transformed Data: Default, 3RSSH Twice",ylab="Spots",lty=2,xlim=c(1700,2000))

lines(ts(smooth_3RSSH(sunspot.year_new),start=1700,end=1988),col="red")

legend(x = 1850, y = 0, c("Default", "3RSSH Twice"), 
       lty=c(2,1),lwd=2,col = c("black","red"))
```

Boxcox 4253H Twice

```{r}
ts.plot(sunspot.year_new,main="Box-Cox Transformed Data: Default, 4253H Twice",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(sleek(sunspot.year_new),start=1700,end=1988),col="red")
legend(x = 1850, y = 0, c("Default", "4253H Twice"), 
       lty=c(2,1),lwd=2,col = c("black","blue"))
```

Sqrt 3RSSH Twice

```{r}
ts.plot(sqrt(sunspot.year),main="Sqrt Transformed Data: Default, 3RSSH Twice",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(smooth_3RSSH(sqrt(sunspot.year)),start=1700,end=1988),col="red")
legend(x = 1700, y = 14, c("Default", "3RSSH Twice"), 
       lty=c(2,1),lwd=2,col = c("black","red"))
```

Sqrt 4253H Twice

```{r}
ts.plot(sqrt(sunspot.year),main="Sqrt Transformed Data: Default, 4253H Twice",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(sleek(sqrt(sunspot.year)),start=1700,end=1988),col="blue")
legend(x = 1700, y = 14, c("Default", "4253H Twice"), 
       lty=c(2,1),lwd=2,col = c("black","blue"))
```

평활된 자료를 비교하기

```{r}
ts.plot(sunspot.year,main="Raw Data:All",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(smooth_3RSSH(sunspot.year),start=1700,end=1988),col="red")
lines(ts(sleek(sunspot.year),start=1700,end=1988),col="blue")
lines(ts(smooth(sunspot.year, kind="3RS3R",twiceit=T),start=1700,end=1988),col="darkgreen")
legend(x = 1700, y = 190, c("Default", "3RSSH Twice","4253H Twice","3RS3R Twice"), 
      lty=c(2,1,1,1),lwd=2,col = c("black","red","blue","darkgreen"))
```

-   총 289년간 26번의 Fluctuation이 있는 것으로 보아 대략 11년마다 주기가 반복되어지는 것이 확인된다.
-   각 주기별 Fluctuation에서 늘어나는 속도와 줄어드는 속도는 대체로 대칭적인 것으로 확인되어 진다.
-   3RSSH twice 기법에 비해 4253H twice에서의 시계열 데이터가 조금 더 이 Fluctuation의 크기가 줄어든 것이 확인되어 진다.
-   3RSSH Twice,4253H Twice 두 기법 모두 Hanning을 사용하여 3RS3R Twice 기법에 비해 평평한 지접이 나타나지 않는다.
-   3RS3R Twice --\> 3RSSH Twice --\> 4253H Twice 순으로 극단값에 민감 한 것으로 보인다. 다만, 앞에서 언급하였던 1778년, 1947년, 1957년, 1958년, 1959년, 1979년, 1980년 자료들의 값들을 4253H Twice 기법이 다른 Smoothing 기법에 비해서 많이 깎아냈기 때문에 해석에 유의가 필요할 것으로 보인다.
-   평활법을 사용하면 양 끝값을 실제 데이터를 활용하지 않고 인접 데이터들을 활용해 계산하기 때문에 실제 데이터에 비해서 덜 늘어나는 경향이 있다. 1988년도 실제 데이터는 더 늘어나고 있다는 점을 유의하여 해석해야 할 것이다.
-   1950년대후반\~1960년대초반에 Fluctuaition의 크기가 가장 큰 것으로 확인되고 1800년대 초반의 Fluctuation의 크기가 상대적으로 작은 것으로 보인다.
-   봉우리의 Cycle은 앞에서 언급하였던 것 처럼 11년 정도이고 이 봉우리의 크기 또한 Fluctuate하는데 약 50년\~60년 정도 마다 늘어나고 줄어드는 속도 또한 늘어나고 줄어드는 것으로 보인다. (ex, 1800s년대 초반 3주기 까지는 봉우리의 크기가 작다가 1840년이후의 3\~4주기 정도는 봉우리의 크기가 늘어나고 그 이후 3주기 정도는 다시 그 크기가 줄어들고 있다.) (각 봉우리들의 크기 또한 주기성에 따라 움직이고 있는 것이 확인되어 진다.)
-   이 분석에서는 주기성에 의미를 부여하기 위해서 3RSSH Twice Smoothing 기법이 다른 기법들에 비해서 해석에 도움이 될 것으로 보인다. 4253H Twice 기법을 사용할 경우 봉우리들의 크기 변화를 포착하기 어렵기 때문이다.

```{r}
ts.plot(sunspot.year_new,main="Box-Cox Transformed Data:All",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(smooth_3RSSH(sunspot.year_new),start=1700,end=1988),col="red")
lines(ts(sleek(sunspot.year_new),start=1700,end=1988),col="blue")
legend(x=1880, y = 0, c("Default", "3RSSH Twice","4253H Twice"), 
       lty=c(2,1,1),lwd=2,col = c("black","red","blue"))
```

```{r}
ts.plot(sqrt(sunspot.year),main="Sqrt Transformed Data:All",ylab="Spots",lty=2,xlim=c(1700,2000))
lines(ts(smooth_3RSSH(sqrt(sunspot.year)),start=1700,end=1988),col="red")
lines(ts(sleek(sqrt(sunspot.year)),start=1700,end=1988),col="blue")
legend(x = 1700, y = 14, c("Default", "3RSSH Twice","4253H Twice"), 
       lty=c(2,1,1),lwd=2,col = c("black","red","blue"))
```

-   자료를 변환하고 Smoothing을 적용할 경우 각 봉우리들의 변화 크기가 정규화 되는 장점은 있으나 각 봉우리들의 크기 변화를 포착하기 어려워 지는 단점이 있다.
-   흑점의 수가 0인 지접이 있는데, 0일 경우에 Box-Cox Transformation을 사용하는 것은 부적절한 변환 방법이다. 따라서 주기성에 집중하기 위해서는 다른 변환 방법을 사용해야 할 것이다.

```{r}
aTSA::adf.test(sunspot.year, nlag = NULL, output = TRUE) # p-value<=0.01 귀무가설을 기각하여 정상시계열
```

```{r}
forecast(arima(sunspot.year,order=c(9,0,0)),h=10)
plot(forecast(arima(sunspot.year,order=c(9,0,0)),h=10)) # Arima를 통해 미래 데이터가 어떻게 변할지 예측
```

```{r}
# 실제 2014년까지의 흑점 데이터와 예측된 데이터의 추이를 비교할 경우 
new=ts(read.table("https://raw.githubusercontent.com/SangwonJu/data/main/solar.txt")$V2,start=1988)
plot(forecast(arima(sunspot.year,order=c(9,0,0)),h=26))
lines(new,col="red",lwd=2)
legend(x = 1700, y = 220, c("Forecasted Data", "Actual Data"), 
       lty=c(1,1),lwd=2,col = c("#2297E6","red"),cex=0.7)
```

2000년대 초반까지의 흑점수의 변화는 거의 정확하게 예측된 것으로 보인다. 그 이후의 데이터는 다소 차이를 보이나 전반적인 추세는 유사한 것으로 보인다.

```{r}
plot(forecast(auto.arima(sunspot.year),h=26))
lines(new,col="red",lwd=2)
legend(x = 1700, y = 220, c("Forecasted Data", "Actual Data"), 
       lty=c(1,1),lwd=2,col = c("#2297E6","red"),cex=0.7)
```

# 3번

R의 fdeaths 자료를 평활하고 분석하여라

Monthly Deaths from Lung Diseases in the UK Three time series giving the monthly deaths from bronchitis, emphysema and asthma in the UK, 1974--1979, both sexes (ldeaths), males (mdeaths) and females (fdeaths).

```{r}
yr <- floor(tt <- time(mdeaths))
plot(mdeaths, fdeaths,
     xy.labels = paste(month.abb[12*(tt - yr)], yr-1900, sep = "'"))
```

```{r}
fdeaths_ts=ts(fdeaths,frequency = 12,start = c(1974,1))
fdeaths_ts
```

결측치와 outlier의 존재여부를 체크하고자 한다.

```{r}
boxplot(fdeaths_ts, main="Number of Female Deaths")
(boxplot(fdeaths_ts, main="Number of Female Deaths")) # Outlier는 한개 존재한다.
subset(fdeaths,fdeaths>901) # 1976년 2월 자료가 Fence 밖의 Outlier 인 것으로 확인된다.
sum(is.na(fdeaths_ts)) #결측치는 없음
```

Stem and Leaf Plot을 활용해 자료의 분포를 예측해보자 한다.

```{r}
stem(fdeaths_ts,1)
stem(fdeaths_ts,2) # Skewd to the right 된 자료 인 것으로 보인다. 위에서 Boxplot을 통해 확인했던 Outlier를 재확인 할 수 있었다. Stem 4를 축으로 해서 데이터들이 잘 Cluster 되어 있는 것이 확인되어 진다.
```

Skewness와 Kurtosis의 분석

```{r}
skewness = function(x) {
  hl=fivenum(x)[2]
  median=fivenum(x)[3]
  hu=fivenum(x)[4]
  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))
  return(skew)
}
skewness(fdeaths_ts) #Skewed to the Right 되어 있는 것을 확인할 수 있다. 
```

```{r}
skewness(log(fdeaths_ts))
skewness(sqrt(fdeaths_ts))
skewness(-1/(fdeaths_ts)) 
skewness(-1/sqrt(fdeaths_ts))
# Minus Inverse 변환에서 Skewness가 최소화 되는 것으로 보인다.
```

skewness를 계산할 경우 자료가 정규분포에 비해 skewed to the right 라는 사실을 확인할 수 있다.

```{r}
stem(log(fdeaths_ts))  # 자료의 비대칭성이 다소 개선 된 것으로 보인다.
stem(-1/(fdeaths_ts))  
# 단봉 분포가 아니라 쌍봉분포 인 것으로 보인다. stem (-24,-22)와 stem -12를 중심으로 하는 두개의 Cluster가 있는 것으로 보인다.
```

```{r}
# letter value display
source("http://mgimond.github.io/ES218/es218.R")
(lvd3=lsum(fdeaths_ts,9)) # mid 값이 점차 커지는 중
# Kurtosis (E-spread) / (H-spread) - 1.705
(lvd3[3,5]-lvd3[3,3])/(lvd3[2,5]-lvd3[2,3])-1.705 # flatter than normal
```

데이터의 Five Numbers와 요약 지표

```{r}
fivenum(fdeaths_ts)
summary(fdeaths_ts)# Median, Mean이 큰 차이가 없다.
as.numeric(summary(fdeaths_ts)[6]-summary(fdeaths_ts)[1]) # 최고점과 최저점 사이의 차이는 811명
```

데이터의 최고점과 최저점 사이의 차이는 811 정도인 것으로 확인된다. Median과 Mean값 사이의 차이가 50명 내외로 나타나고 있는 것이 확인되어 진다.

```{r}
qqnorm(fdeaths_ts, ylab="Sunspots quantiles");qqline(fdeaths_ts, col='red',lty=2)
fiv=fivenum(fdeaths_ts)
(pseudosigma = (fiv[4]-fiv[2])/1.34)
sd(fdeaths_ts) 
```

psudosigma와 실제 표준편차 사이에 30정도의 차이가 확인된다. 그러나 qqplot에서 데이터가 직선을 따르기 보다는 역S자의 곡선을 이루고 있는 것으로 보인다. (Sample 분포가 정규분포는 아니다)

```{r}
qqnorm(fdeaths_ts, ylab="Female Deaths quantiles");qqline(fdeaths_ts, col='red',lty=2)
abline(fiv[3],pseudosigma,col="blue",lty=2)
title(sub="intercept=512(median); slope=203.7313 (pseudo-sigma) blue line")
```

```{r}
qqnorm(-1/fdeaths_ts, ylab="Minus Inverse Sunspots quantiles");qqline(-1/fdeaths_ts, col='red',lty=2)
qqnorm(log(fdeaths_ts), ylab="Log Sunspots quantiles");qqline(log(fdeaths_ts), col='red',lty=2)
# 변환을 적용하더라도 qqplot에서 데이터의 역S자적 경향은 완화되지 않는다. 
```

데이터가 감마분포를 따르는가?

```{r}
fdeaths.sort <- sort(fdeaths_ts)
(n.fdeaths<- length(fdeaths_ts))
i <- 1:n.fdeaths

```

```{r}
(mean.fdeaths <- mean(fdeaths_ts))
(var.fdeaths <- var(fdeaths_ts))
(shape.fdeaths <- mean.fdeaths^2/var.fdeaths)
(scale.fdeaths <- mean.fdeaths/var.fdeaths)
q.gamma.fdeaths <- qgamma((i-0.5)/n.fdeaths, shape.fdeaths, scale.fdeaths)
plot(q.gamma.fdeaths, fdeaths.sort, main="Gamma prob plot")
line(q.gamma.fdeaths, fdeaths.sort)[2]
abline(a=-4.14999766208764,b=1.11107022976103,col="red",lwd=2) 


```

여전히 Probability Plot에서 역 S자적 성질이 사라지지 않고 있기 때문에 데이터는 혼합 분포에서 추출 된 것으로 추측된다. Minus Inverse 변환을 통해서 예측하였던 것이 재 확인되고 있다.

## 정리

해당 데이터는 정규분포를 따르고 있지 않다. (정규성x)

```{r}
library(forecast)
ts.plot(fdeaths_ts, main = "Female Deaths from Lung Diseases Time-Series Plot") #시계열 자료의 계절성을 확인할 수 있다.
# 1976년을 제외하면 데이터의 Fluctuation이 일정하게 유지되고 있다.
plot(diff(fdeaths_ts), main = "Difference : Female Deaths from Lung Diseases") # 차분데이터를 확인할 경우 동일하게 1976년 자료를 제외하면 분산의 증가 없이 일정하게 변동성이 유지 되는 것으로 볼 수 있다.
```

Box-Cox Transformation을 통해서 데이터를 변환하였다. (계절변동을 확인하기 위해서 데이터의 정규성을 개선하기 위해)

```{r}
lambda <- forecast::BoxCox.lambda(fdeaths_ts)
fdeaths_ts_new <- forecast::BoxCox(fdeaths_ts, lambda)
```

```{r}
plot(fdeaths_ts_new, main = "Box-Cox : Female Deaths from lung diseases") # 1976년의 데이터의 이질성이 다소 완화되었다.
plot(diff(fdeaths_ts_new), main = "Difference & Box-Cox : Female Deaths from lung diseases")
```

fdeaths 데이터 - 분해 시계열

```{r}
# 계절요인 분해시계열
 
fdeaths.decompose <- decompose(fdeaths)       # 데이터에서 4가지 요인을 분해
fdeaths.decompose$seasonal                     # 계절요인으로 분해된 부분이다.
plot(fdeaths.decompose)
ts.plot(fdeaths.decompose$seasonal,main="The Plot of Seasoal Decomposition")
```

계절 요인을 제외한 나머지 변화 확인

```{r}
# 계절요인 제외시키기
fdeaths.decompose_new <- fdeaths - fdeaths.decompose$seasonal
ts.plot(fdeaths.decompose_new, main="Time-Series without Seasonal Effect")
```

계절성을 제외한 나머지 요인들을 분석할 경우 1976년도의 Random한 요인에 의해서 데이터가 늘어난 것을 확인 할 수 있다. 1977년도 초반의 Random한 요인 또한 시계열에서 1\~2월 데이터를 각 년도 동기에 비해서 다소 낮은 수준으로 유지하게 만들었다.

```{r}
fdeaths_1974=ts(fdeaths_ts[1:12],start=1)
fdeaths_1975=ts(fdeaths_ts[13:24],start=1)
fdeaths_1976=ts(fdeaths_ts[25:36],start=1)
fdeaths_1977=ts(fdeaths_ts[37:48],start=1)  
fdeaths_1978=ts(fdeaths_ts[49:60],start=1)
```

```{r}
fdeaths_1979=ts(fdeaths_ts[61:72],start=1)  
yr=paste("fdeaths","_",1974:1979,sep="")

xat=seq(0,12,by=1)
par(mfrow=c(2,3))
for (i in yr) {ts.plot(as.name(i),main=i,ylab="Female Deaths by lung cancer")
               axis(side=1,at=xat)}
```

각 년도별 계절성을 비교하기 위해서 이런식으로 그래프를 연도별로 쪼개서 그렸다. 연도별로 그러한 경향성이 비슷하게 드러나고 있는 것으로 보인다.

```{r}
# install.packages("ghibli")
library("ghibli")
pal=ghibli_palette("PonyoLight",n=6)
as.character(pal)
par(mfrow=c(1,1))

ts.plot(fdeaths_1974,main="Female Deaths by lung diseases in UK",xlab="Month",ylab="# of Deaths",col="#A6A0A0FF",lwd=2,ylim=c(300,1200))
axis(side=1,at=xat)
lines(fdeaths_1975,lwd=2,col="#ADB7C0FF")
lines(fdeaths_1976,lwd=2,col="#94C5CCFF")
lines(fdeaths_1977,lwd=2,col="#F4ADB3FF")
lines(fdeaths_1978,lwd=2,col="#EEBCB1FF")
lines(fdeaths_1979,lwd=2,col="#ECD89DFF")

legend(x = 10, y = 1200, c(1974:1979), 
          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)
```

전체데이터와 월별데이터에 평활을 진행하고자 한다. 평활법은 교수님께서 추천하셨던 3RSSH Twice, 4253H Twice에 더해 Hanning이 이루어지기 전 평활법인 3RS3R Twice를 추가하고자 한다.

```{r}
library(sleekts)
pal2=as.vector(ghibli_palette("MononokeMedium")[c(1,3,5,7)])
ts.plot(fdeaths_ts,main="Raw Data:All",ylab="# of Female deaths",lty=2,col=pal2[1])
lines(ts(smooth(fdeaths_ts, kind="3RS3R",twiceit=T),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[2],lwd=2)
lines(ts(smooth_3RSSH(fdeaths_ts),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[3],lwd=2)
lines(ts(sleek(fdeaths_ts),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[4],lwd=2)

legend(x =1978.7, y = 1100, c("Default", "3RS3R Twice", "3RSSH Twice","4253H Twice"), 
       lty=c(2,1,1,1),lwd=2,col = pal2,cex=0.5)

```

평활법을 적용하여 확인할 경우 원 자료에 비해 최댓값과 최솟값의 폭이 많이 줄어들었음을 확인할 수 있다. 작은 값에서는 크게 변화가 없지만 값이 큰 자료들의 경우 많이 깎여나갔다. 시계열을 확인하는데 있어서 그 계절성을 확인하기 좋은 형태로 평활이 된 것은 사실이지만, 위에서 언급한 1976년도의 특이값이 사라지게 되었고 그 특이값을 해석하는데 있어서 주의를 기울여야 할 것으로 보인다. 3RS3R Twice 기법의 경우 Hanning이 진행되지 않았기 때문에 다소 각진 부분이 남아있지만, 전반적으로 계절성이 나타나는 형태로 데이터를 변화시켰다. 나머지 Hanning을 사용한 2가지 평활법의 차이를 분석하면 4253H방법에서 큰 값들의 감소폭이 크게 나타나고 있다. 3가지 평활법의 양 끝자료의 경우 실제 존재하는 데이터를 가지고 만든 것이 아니기 때문에 그 추세를 해석하는데 있어서 용이하지만 실제 데이터와 차이가 있으므로 해석에 유의해야 할 것이다.

앞에서 시계열 Decompose를 통해서 그렸던 계절성 그래프의 모양과 4253H Twice의 그래프가 상당히 유사한 것으로 보인다. 1년을 주기로 폐질환 사망자수가 Fluctuate 하고 있는데 겨울철에 전반적으로 증가하고 여름철에 감소하는 경향성을 가지는 것이 확인된다. 각 주기는 거의 대칭적으로 증감을 반복하고 있으며 1976, 1977년도를 제외하면 사망건수는 거의 비슷하게 유지 되는 것을 확인할 수 있다. 따라서 시계열 자료를 해석할 때 1976년도와 1977년도 자료는 유의해서 해석해야 할 것으로 보인다.

https://premium.weatherweb.net/weather-in-history-1975-to-1999-ad/ For England and Wales (using the Met Office EWR/EWP series), it was one of the six DRIEST winters in the previous 100 years, and the third consecutive season with less rain than usual: summer and autumn 1975 were also dry. Winter 1975/76 had around 61% of average rainfall over England and Wales. It was this persistence of low precipitation, particularly throughout the winter 're-charge' season, that led to the severe DROUGHT problems encountered in 1976 (q.v.) 1976년도의 특이 값에 대한 원인을 분석할 때 해당 년도의 겨울이 100년중 6번째로 건조한 겨울이었다는 점을 고려해야 할 것으로 보인다. 가뭄 문제를 걱정할 정도로 겨울이 건조하였다는 사실은 해당 년도에 사람들이 호흡기 질환을 걸릴 가능성이 상당히 높았음을 예측할 수 있다.

##Raw Data

```{r}
ts.plot(fdeaths_1974,main="Female Deaths by lung diseases in UK",xlab="Month",ylab="# of Deaths",col="#A6A0A0FF",lwd=2,ylim=c(300,1200))
axis(side=1,at=xat)
lines(fdeaths_1975,lwd=2,col="#ADB7C0FF")
lines(fdeaths_1976,lwd=2,col="#94C5CCFF")
lines(fdeaths_1977,lwd=2,col="#F4ADB3FF")
lines(fdeaths_1978,lwd=2,col="#EEBCB1FF")
lines(fdeaths_1979,lwd=2,col="#ECD89DFF")

legend(x = 10, y = 1200, c(1974:1979), 
          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)
```

## 3RS3R Twice

```{r}
ts.plot(ts(smooth(fdeaths_1974, kind="3RS3R",twiceit=T),start=1, end=12),main="3RS3R Twice: Female Deaths by lung diseases in UK",xlab="Month",ylab="# of Deaths",col=pal[1],lwd=2,ylim=c(300,1200))
axis(side=1,at=xat)
lines(ts(smooth(fdeaths_1975, kind="3RS3R",twiceit=T),start=1, end=12),lwd=2,col=pal[2])
lines(ts(smooth(fdeaths_1976, kind="3RS3R",twiceit=T),start=1, end=12),lwd=2,col=pal[3])
lines(ts(smooth(fdeaths_1977, kind="3RS3R",twiceit=T),start=1, end=12),lwd=2,col=pal[4])
lines(ts(smooth(fdeaths_1978, kind="3RS3R",twiceit=T),start=1, end=12),lwd=2,col=pal[5])
lines(ts(smooth(fdeaths_1979, kind="3RS3R",twiceit=T),start=1, end=12),lwd=2,col=pal[6])
legend(x = 10, y = 1200, c(1974:1979), 
          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)
```

## 3RSSH Twice

```{r}
ts.plot(ts(smooth_3RSSH(fdeaths_1974),start=1, end=12),main="3RSSH Twice: Female Deaths by lung diseases in UK",xlab="Month",ylab="# of Deaths",col=pal[1],lwd=2,ylim=c(300,1200))
axis(side=1,at=xat)
lines(ts(smooth_3RSSH(fdeaths_1975),start=1, end=12),lwd=2,col=pal[2])
lines(ts(smooth_3RSSH(fdeaths_1976),start=1, end=12),lwd=2,col=pal[3])
lines(ts(smooth_3RSSH(fdeaths_1977),start=1, end=12),lwd=2,col=pal[4])
lines(ts(smooth_3RSSH(fdeaths_1978),start=1, end=12),lwd=2,col=pal[5])
lines(ts(smooth_3RSSH(fdeaths_1979),start=1, end=12),lwd=2,col=pal[6])
legend(x = 10, y = 1200, c(1974:1979), 
          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)

```

## 4253H Twice

```{r}
ts.plot(ts(sleek(fdeaths_1974),start=1, end=12),main="4253H Twice: Female Deaths by lung diseases in UK",xlab="Month",ylab="# of Deaths",col=pal[1],lwd=2,ylim=c(300,1200))
axis(side=1,at=xat)
lines(ts(sleek(fdeaths_1975),start=1, end=12),lwd=2,col=pal[2])
lines(ts(sleek(fdeaths_1976),start=1, end=12),lwd=2,col=pal[3])
lines(ts(sleek(fdeaths_1977),start=1, end=12),lwd=2,col=pal[4])
lines(ts(sleek(fdeaths_1978),start=1, end=12),lwd=2,col=pal[5])
lines(ts(sleek(fdeaths_1979),start=1, end=12),lwd=2,col=pal[6])
legend(x = 10, y = 1200, c(1974:1979), 
       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)
```

원자료 --\> 3RS3R Twice --\> 3RSSH Twice--\> 4253H Twice순으로 시계열의 추세가 유사해지고 있는것을 확인할 수 있다. Hanning을 사용하여 3RSSH Twice, 4253H Twice는 평평한 구간 없이 부드럽게 넘어가고 있으며 1976년도 평활법을 통해서 최대값이 줄어들어 다른 년도와 유사한 계절성을 확인할 수 있게 되었다.

```{r}
library(forecast)
aTSA::adf.test(fdeaths, nlag = NULL, output = TRUE) # p-value<=0.01 귀무가설을 기각하여 정상시계열
fit <- auto.arima(fdeaths)
plot(forecast(fit, level=c(80, 95), h=12)) 
# 다음 1년간의 변화를 예측할 경우 이전의 자료들과 비슷한 추세를 가지고 있는 것이 확인되어 진다.
```

# 4번

R의 lynx 자료를 평활하고 분석하여라.

Annual numbers of lynx trappings for 1821--1934 in Canada. Taken from Brockwell & Davis (1991), this appears to be the series considered by Campbell & Walker (1977).

```{r}
data(lynx)
boxplot(lynx,main="Boxplot of lynx data") 
```

Boxplot을 통해서 확인했더니 총 4개의 Outlier가 확인되어 진다. Median이 Box 안의 밑에 위치하고 있는 것으로 보아 Skewed to the Right 한 자료일 가능성이 높다.

```{r}
boxplot(lynx)$stats[5,] # upper fence의 값은 4950이고
boxplot(lynx)$out #5943, 6721, 6991, 6313 4개의 자료가 Fence의 밖에 존재한다.
```

Stem and leaf plot을 통해서 해당 데이터의 분포를 추측해본다.

```{r}
stem(lynx) 
```

Box plot을 통해서 확인하였던 5개의 Outlier를 확인하였다. 0 stem들에 자료들이 많이 몰려있고 점차 줄어드는 것을 확인할 수 있다. Skewed to the right 되어 있는 분포를 따르고 있는 것으로 추측되어진다.

```{r}
skewness = function(x) {
  hl=fivenum(x)[2]
  median=fivenum(x)[3]
  hu=fivenum(x)[4]
  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))
  return(skew)
}
skewness(lynx)
```

데이터의 Skewness를 고려해 볼 떄 Box plot에서 확인되였던 것 처럼, 해당 데이터는 Skewed to the right되어 있다는 사실을 확인할 수 있다.

```{r}
skewness(log(lynx))
skewness(sqrt(lynx))
skewness(-1/(lynx))
skewness(-1/sqrt(lynx)) # Minus inverse sqrt 변환이 Skewness를 가장 줄여 주는 것이 확인되어진다.
```

```{r}
# letter value display
source("http://mgimond.github.io/ES218/es218.R")
(lvd=lsum(lynx,8)) # mid 값이 점차 커지는 중
# Kurtosis (E-spread / H-spread - 1.705)
(lvd[3,5]-lvd[3,3])/(lvd[2,5]-lvd[2,3]) -1.705 # flatter than normal
```

-   데이터의 Five Numbers와 요약 지표

```{r}
fivenum(lynx)
summary(lynx)# Median, Mean이 차이가 크다 (767): skewed 되어있는 정도가 크다.
as.numeric(summary(lynx)[6]-summary(lynx)[1]) # 최고점과 최저점 사이의 차이는 6952
```

최고값의 경우 6991마리 / 최소값의 경우 39마리 / 평균적으로 1538마리가 잡혔다.

정규성을 따르는가?

```{r}
qqnorm(lynx, ylab="Lynx quantiles"); qqline(lynx, col='red',lty=2)
fiv=fivenum(lynx)
(pseudosigma = (fiv[4]-fiv[2])/1.34)
sd(lynx)
qqnorm(lynx, ylab="Lynx quantiles");qqline(lynx, col='red',lty=2)
abline(fiv[3],pseudosigma,col="blue",lty=2)
title(sub="intercept=771 (median); slope=1665.672 (pseudo-sigma) blue line")
```

pseudo-sigma와 sigma가 차이가 80 정도로 다소 있다. 또한 Sample 자료들이 qqplot 상에서 직선을 따르고 있지 않은 것으로 보인다. 위의 문제에서 처럼 자료들이 역 S자 곡선을 그리고 있는 것으로 보인다.

데이터가 지수분포를 따르는가?

```{r}
n <- length(lynx)
lynx.sort=(sort(lynx))
i <- 1:n
q.exp.lynx <- -log(1-(i-0.5)/n) # exponential quantile
plot(q.exp.lynx , lynx.sort, main="Exponential prob plot") 
line(q.exp.lynx , lynx.sort)[2]
abline(line(q.exp.lynx , lynx.sort)[2],col="red",lty=2) 
# Outlier을 제외하면 직선을 잘 따르고 있는 것으로 보이나 왼쪽에 데이터가 몰려있는 것으로 보여 삼승근 변환을 통해 데이터를 분산시켜보고자 한다.
```

```{r}
plot(q.exp.lynx^(1/3) , lynx.sort^(1/3), main="Exponential prob plot")
line(q.exp.lynx^(1/3) , lynx.sort^(1/3))[2]
abline(line(q.exp.lynx^(1/3) , lynx.sort^(1/3))[2],col="red",lty=2)
# 직선을 잘 따르고 있는 것으로 보이나, 데이터들이 곡선을 따르고 있는 것 처럼 보이기도 하다.
```

```{r}
# 와이블 분포
q.weibull.lynx <- log(q.exp.lynx)
plot(q.weibull.lynx, log(lynx.sort), main="Weibull prob plot")
line(q.weibull.lynx, log(lynx.sort))[2]
abline(line(q.weibull.lynx, log(lynx.sort))[2],col="red",lty=2)
```

두 Plot을 비교할 경우 지수 분포에 더 적합한 것으로 보인다. (lynx data는 정규성이 확보되지는 못한 것으로 볼 수 있다.)

잡힌 Lynx 수를 평활하기 위해서 수업시간에 교수님께서 Recommend 하신 3RSSH Twice방법과 4253H Twice방법에 더해 3RS3R Twice 활용하고자 한다.

```{r}
library(forecast)
ts.plot(lynx,main="Time series plot of lynx caught",ylab= "# of Caught", xlab="Year",xlim=c(1820,1940)) 
# 시계열 자료의 주기성을 확인할 수 있다.
# 1828년, 1866년, 1904년을 중심으로 하는 봉우리 주기들이 다른 주기들에 비해서 그 늘어나는 강도가 더 큰 것으로 확인되어 진다. 반면 1875년을 중심으로 하는 주기의 경우 다른 봉우리들에 비해서 그 늘어나는 강도가 부족 한 것으로 보인다.
# 111년동안 12번의 봉우리 주기가 돌아왔으므로 9~10년 주기로 늘어났다가 줄어들었다를 반복하는 것을 확인할 수 있다. Lynx가 늘어나는 속도와 줄어드는 속도는 일치하는 것으로 확인되어 진다. 
ts.plot(diff(lynx),main="Difference :  lynx caught",ylab= "# of Caught", xlab="Year",xlim=c(1820,1940)) 
# 차분데이터를 확인할 경우 위에서 언급한 특이점들을 제외하면 분산의 증가 없이 일정하게 변동성이 유지 되는 것으로 볼 수 있다.
```

Box-Cox Transformation을 통해서 데이터를 변환하였다. (데이터의 정규성을 개선하기 위해)

```{r}
lambda2 <- BoxCox.lambda(lynx)
lynx_new <- BoxCox(lynx, lambda2)
```

```{r}
plot(lynx_new, main="Box Cox: Time series plot of lynx caught",ylab= "# of Caught", xlab="Year",xlim=c(1820,1940))
plot(diff(lynx_new), main = "Difference & Box-Cox : Time series plot of lynx caught",ylab= "# of Caught", xlab="Year",xlim=c(1820,1940) )
```

```{r}
# install.packages("ggpmisc")
library(ggpmisc)
library(ggplot2)

ggplot(lynx, as.numeric = F) + 
   geom_line()+
   theme_bw()+
   stat_peaks(colour = "red")+
   stat_valleys(colour = "blue")+
   stat_peaks(geom = "text", colour = "red", vjust = -0.5, x.label.fmt = "%Y") +
   stat_valleys(geom = "text", colour = "blue", angle = 45, vjust = 1.5, hjust = 1, x.label.fmt = "%Y")+
   scale_y_continuous(limits = c(-1000, 8000))+
   ggtitle("Lynx Caught Data")+
   xlab("year")+
   ylab("# of Lynx caught")

```

전체데이터와 월별데이터에 평활을 진행하고자 한다. 평활법은 교수님께서 추천하셨던 3RSSH Twice, 4253H Twice에 더해 Hanning이 이루어지기 전 평활법인 3RS3R Twice를 추가하고자 한다.

```{r}
library(sleekts)
pal3=as.vector(ghibli_palette("PonyoMedium")[c(1,2,4,7)])

ts.plot(lynx,main="Raw Data:All",ylab= "# of Caught", xlab="Year",xlim=c(1820,1940),lty=2,col=pal3[1])
lines(ts(smooth(lynx, kind="3RS3R",twiceit=T),start=1821,end=1934),col=pal3[2],lwd=2)
lines(ts(smooth_3RSSH(lynx),start=1821,end=1934),col=pal3[3],lwd=2)
lines(ts(sleek(lynx),start=1821,end=1934),col=pal3[4],lwd=2)
legend(x =1910, y = 6800, c("Default", "3RS3R Twice", "3RSSH Twice","4253H Twice"), 
       lty=c(2,1,1,1),lwd=2,col = pal3,cex=0.5)

```

Lynx의 포획 수는 약 10년마다 증가하고 감소하는 패턴을 확인할 수 있다. 덫에 걸리는 Lynx들의 수는 자연의 Lynx 개체수와 비례한다고 볼 때, 10년 주기로 총 개체수가 줄어들고 늘어난다고 볼 수 있다. (단, 설치된 Trap의 수가 매 시기별로 어느정도 동일하다는 조건이 붙어야 한다.)

평활법 적용으로 인해 원 자료에 비해 최댓값과 최솟값의 폭이 많이 줄어들었다. 작은 값에서는 크게 변화가 없지만 값이 큰 자료들이 많이 줄어들었다.

앞에서 언급하였던, 1828년, 1866년, 1904년의 특이 봉우리 들이 평활 이후에는 상당히 낮아졌다. 1866년 봉우리의 경우 3가지 평활법에서 모두 거의 절반 가량을 깎아냈다. 또한 1913년, 1916년의 경우 봉우리가 연속적으로 존재하였는데 평활 이후에는 이러한 봉우리가 하나의 봉우리로 합쳐지게 되었다. 평활로 인해 깎아진 봉우리들을 적절하게 해석하기 위해서는 원 데이터들을 반드시 고려해야 할 것이다. (원 데이터에 발생한 특이값들이 특정한 요인에 의해서 발생했다면 그 요인을 정확히 파악하는 것이 시계열 데이터의 올바른 이해를 위해서 필요할 것이라고 보인다.)

3가지 평활법 모두 양 끝자료를 실재하는 자료가 아닌 대체된 자료를 활용하였음에도 불구하고 실제 자료랑 큰 차이지 보이지 않고 있다. 4253H Twice 기법이 다른 두 기법에 비해서 큰 값들을 많이 깎아내는데 이를 해석하는데 있어서 상당히 유의해야 할 것으로 보인다.

Lynx 개체수의 10년의 주기성을 확인하기 위해서는 4253H Twice 기법을 사용하는 것이 적절해 보이고, 각 Local 봉우리별 크기의 변화에 집중하기 위해서는 3RSSH Twice를 사용하는 것이 적절해 보인다.

-   1866년, 1904년의 봉우리 다음 봉우리들은 상당히 Peak의 높이가 낮아졌는데 그 원인을 이전 봉우리에서 생성된 많은 개체들이 서식지를 파괴한 것이 아닐까 라고 추측해볼 수 있으나 확실하지 않다.

##미래에 Lynx의 숫자는 어떻게 변할까?

```{r}
aTSA::adf.test(lynx, nlag = NULL, output = TRUE) # p-value<=0.01 귀무가설을 기각하여 정상시계열
fit <- auto.arima(lynx)
plot(forecast(fit, level=c(80, 95), h=10))  #미래 데이터가 어떻게 이동할지 예측
```
