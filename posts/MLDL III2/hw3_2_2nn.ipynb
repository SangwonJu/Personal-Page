{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"nCYO6dmGgefe"},"source":["---\n","title: \"Assignment 3-2\"\n","author: \"Sangwon Ju, SNU GSPA\"\n","date: 'NOV/27/2022'\n","format: \n","    html:\n","        code-overflow: wrap\n","        code-fold: true\n","categories:\n","   \"Machine Learning & Deep Learning for Data Science (2022 Fall)\" \n","image: image.png\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["2NN and Fully Connected Layers (Score: 107/100)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Local Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Jfeql_8sgnKJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["e:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제3\n","E:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제3\n"]}],"source":["import os\n","# get current path\n","print(os.getcwd())\n","# change path\n","os.chdir(\"E:/OneDrive - SNU/(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/과제3\")\n","print(os.getcwd())"]},{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"OyammZP8hI7P"},"outputs":[],"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist.data_utils import load_data"]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["# Utils"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xuQB6W2U5ZE2"},"outputs":[],"source":["def tanh(z):\n","    \"\"\"\n","    Implement the tanh activation function.\n","    The method takes the input z and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    tanh = lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n","    result = tanh(z)\n","    return result   \n","    ##################### \n","\n","def softmax(X):\n","    \"\"\"\n","    Implement the softmax function.\n","    The method takes the input X and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    elements = np.exp(X-np.amax(X,axis=1,keepdims=True))\n","    sums = np.sum(elements, axis=1, keepdims=True)\n","    return elements/sums\n","    #####################\n","\n","\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch"]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["# 2-Layer Neural Network"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mA5udiGmhRb5"},"outputs":[],"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        initializes parameters with Xavier Initialization.\n","\n","        Question (b)\n","        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n","        \n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","        params = {}\n","        ##### YOUR CODE #####\n","        params[\"W1\"] = np.random.uniform(low=-1/(np.sqrt(num_hiddens)), high=1/(np.sqrt(num_hiddens)), size=(input_dim, num_hiddens))\n","        params[\"b1\"] = np.zeros(num_hiddens)\n","        params[\"W2\"] = np.random.uniform(low=-1/(np.sqrt(num_classes)), high=1/(np.sqrt(num_classes)), size=(num_hiddens, num_classes))\n","        params[\"b2\"] = np.zeros(num_classes)\n","        #####################\n","        return params\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Define and perform the feed forward step of a two-layer neural network.\n","        Specifically, the network structue is given by\n","\n","          y = softmax(tanh(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (c)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","        ff_dict = {}\n","        ##### YOUR CODE #####\n","        params = self.params\n","        w1 = params[\"W1\"] \n","        b1 = params[\"b1\"] \n","        w2 = params[\"W2\"] \n","        b2 = params[\"b2\"] \n","\n","        R1 = np.dot(X, w1) + b1 # N * H\n","        Act1 = tanh(R1) # N * H\n","        R2 = np.dot(Act1, w2) + b2 # N * C \n","        Act2 = softmax(R2) # N * C\n","\n","        ff_dict['Affine1'] =  R1\n","        ff_dict['Tanh'] = Act1\n","        ff_dict['Affine2'] = R2\n","        ff_dict['Softmax'] = Act2\n","        y = Act2\n","        #####################\n","        return y, ff_dict\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (d)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        grads = {}\n","        ##### YOUR CODE #####\n","        batch_size = X.shape[0]\n","        Act1 = ff_dict['Affine1'] \n","        z1 = ff_dict['Tanh']\n","        z2 = ff_dict['Softmax']\n","        w2 = self.params[\"W2\"]\n","\n","        dz2 = (z2-Y) / batch_size  # normalization & entropy diff\n","        grads[\"dW2\"] = np.dot(z1.T, dz2)\n","        grads[\"db2\"] = np.sum(dz2, axis = 0)\n","    \n","        dz1 = (1-tanh(Act1)) * (1+tanh(Act1)) * np.dot(dz2, w2.T) # tanh diff\n","        grads[\"dW1\"] = np.dot(X.T, dz1)\n","        grads[\"db1\"] = np.sum(dz1, axis = 0)\n","        #####################\n","        return grads\n","\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","                self.train_step(X_batch, Y_batch, batch_size, lr)\n","            if epoch % log_interval==0:\n","                Y_hat, ff_dict = self.forward(X)\n","                train_loss = self.compute_loss(Y, Y_hat)\n","                train_acc = self.evaluate(Y, Y_hat)\n","                Y_hat, ff_dict = self.forward(X_val)\n","                valid_loss = self.compute_loss(Y_val, Y_hat)\n","                valid_acc = self.evaluate(Y_val, Y_hat)\n","                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        _, ff_dict = self.forward(X_batch)\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n","        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n","        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n","        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n","\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","        \n","        Do NOT modify this function\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"        \n","        classes_pred = np.argmax(Y_hat, axis=1)\n","        classes_gt = np.argmax(Y, axis=1)\n","        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n","        return accuracy"]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["# Load MNIST"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"48ooR6YIxYhC"},"outputs":[{"name":"stdout","output_type":"stream","text":["MNIST data loaded:\n","Training data shape: (60000, 784)\n","Training labels shape: (60000, 10)\n","Test data shape: (10000, 784)\n","Test labels shape: (10000, 10)\n","\n","Set validation data aside\n","Training data shape:  (48000, 784)\n","Training labels shape:  (48000, 10)\n","Validation data shape:  (12000, 784)\n","Validation labels shape:  (12000, 10)\n"]}],"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["# Training & Evaluation"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"IlnC_rerHPaN"},"outputs":[],"source":["### \n","# Question (e)\n","# Tune the hyperparameters with validation data, \n","# and print the results by running the lines below.\n","###"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TTCqVT4S0Tm5"},"outputs":[],"source":["# model instantiation\n","model = TwoLayerNN(input_dim=784, num_hiddens=80, num_classes=10)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"6cWb6xg0NxOs"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 00 - train loss/acc: 0.512 0.867, valid loss/acc: 0.517 0.866\n","epoch 01 - train loss/acc: 0.406 0.888, valid loss/acc: 0.411 0.888\n","epoch 02 - train loss/acc: 0.361 0.899, valid loss/acc: 0.366 0.899\n","epoch 03 - train loss/acc: 0.333 0.906, valid loss/acc: 0.340 0.904\n","epoch 04 - train loss/acc: 0.314 0.911, valid loss/acc: 0.321 0.909\n","epoch 05 - train loss/acc: 0.299 0.915, valid loss/acc: 0.307 0.913\n","epoch 06 - train loss/acc: 0.286 0.918, valid loss/acc: 0.295 0.917\n","epoch 07 - train loss/acc: 0.275 0.921, valid loss/acc: 0.285 0.920\n","epoch 08 - train loss/acc: 0.266 0.925, valid loss/acc: 0.277 0.922\n","epoch 09 - train loss/acc: 0.257 0.927, valid loss/acc: 0.268 0.924\n","epoch 10 - train loss/acc: 0.249 0.930, valid loss/acc: 0.261 0.927\n","epoch 11 - train loss/acc: 0.241 0.932, valid loss/acc: 0.254 0.928\n","epoch 12 - train loss/acc: 0.234 0.935, valid loss/acc: 0.248 0.930\n","epoch 13 - train loss/acc: 0.228 0.937, valid loss/acc: 0.242 0.931\n","epoch 14 - train loss/acc: 0.222 0.938, valid loss/acc: 0.237 0.932\n","epoch 15 - train loss/acc: 0.216 0.940, valid loss/acc: 0.232 0.933\n","epoch 16 - train loss/acc: 0.211 0.942, valid loss/acc: 0.226 0.935\n","epoch 17 - train loss/acc: 0.205 0.943, valid loss/acc: 0.222 0.935\n","epoch 18 - train loss/acc: 0.200 0.944, valid loss/acc: 0.218 0.938\n","epoch 19 - train loss/acc: 0.196 0.945, valid loss/acc: 0.213 0.939\n","epoch 20 - train loss/acc: 0.192 0.947, valid loss/acc: 0.209 0.940\n","epoch 21 - train loss/acc: 0.187 0.948, valid loss/acc: 0.206 0.941\n","epoch 22 - train loss/acc: 0.183 0.949, valid loss/acc: 0.202 0.942\n","epoch 23 - train loss/acc: 0.179 0.950, valid loss/acc: 0.199 0.943\n","epoch 24 - train loss/acc: 0.176 0.951, valid loss/acc: 0.196 0.943\n","epoch 25 - train loss/acc: 0.172 0.953, valid loss/acc: 0.192 0.945\n","epoch 26 - train loss/acc: 0.169 0.953, valid loss/acc: 0.190 0.946\n","epoch 27 - train loss/acc: 0.166 0.954, valid loss/acc: 0.186 0.947\n","epoch 28 - train loss/acc: 0.162 0.955, valid loss/acc: 0.183 0.947\n","epoch 29 - train loss/acc: 0.160 0.955, valid loss/acc: 0.181 0.948\n","epoch 30 - train loss/acc: 0.157 0.957, valid loss/acc: 0.179 0.949\n","epoch 31 - train loss/acc: 0.154 0.958, valid loss/acc: 0.176 0.949\n","epoch 32 - train loss/acc: 0.151 0.959, valid loss/acc: 0.173 0.949\n","epoch 33 - train loss/acc: 0.148 0.959, valid loss/acc: 0.171 0.950\n","epoch 34 - train loss/acc: 0.146 0.960, valid loss/acc: 0.169 0.951\n","epoch 35 - train loss/acc: 0.144 0.960, valid loss/acc: 0.167 0.952\n","epoch 36 - train loss/acc: 0.141 0.962, valid loss/acc: 0.165 0.952\n","epoch 37 - train loss/acc: 0.139 0.962, valid loss/acc: 0.163 0.953\n","epoch 38 - train loss/acc: 0.136 0.962, valid loss/acc: 0.160 0.953\n","epoch 39 - train loss/acc: 0.135 0.963, valid loss/acc: 0.159 0.954\n"]}],"source":["# train the model\n","lr, n_epochs, batch_size = 3, 40, 128\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hpPsAlXU0T_Z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final test loss = 0.149, acc = 0.956\n"]}],"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"]},{"cell_type":"markdown","metadata":{"id":"5hh1PZpk_g0I"},"source":["# Extra Credit (Optional)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"0R0n6y9_AgXc"},"outputs":[],"source":["######### define relu #############\n","class relu:\n","    def fw(x):\n","        z=x.copy()\n","        fwd = lambda k: np.maximum(0, k)\n","        result = np.vectorize(fwd)(z)\n","        return result\n","    def bw(x):\n","        z=x.copy()\n","        bwd = lambda k : 0 if k<=0 else 1\n","        result = np.vectorize(bwd)(z)\n","        return result        \n","###################################\n","\n","def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","    \"\"\"\n","    initializes parameters with He Initialization.\n","\n","    Question (f)\n","    - refer to https://paperswithcode.com/method/he-initialization for He initialization \n","    \n","    Inputs\n","    - input_dim\n","    - num_hiddens\n","    - num_classes\n","    Returns\n","    - params: a dictionary with the initialized parameters.\n","    \"\"\"\n","\n","    params = {}\n","    ##### YOUR CODE #####\n","    params[\"W1\"] = np.random.uniform(low=-1/(np.sqrt(num_hiddens)), high=1/(np.sqrt(num_hiddens)), size=(input_dim, num_hiddens))\n","    params[\"b1\"] = np.zeros(num_hiddens)\n","    params[\"W2\"] = np.random.uniform(low=-1/(np.sqrt(num_classes)), high=1/(np.sqrt(num_classes)), size=(num_hiddens, num_classes))\n","    params[\"b2\"] = np.zeros(num_classes)\n","    #####################\n","    return params\n","\n","def forward_relu(self, X):\n","    \"\"\"\n","    Defines and performs the feed forward step of a two-layer neural network.\n","    Specifically, the network structue is given by\n","\n","        y = softmax(relu(X W1 + b1) W2 + b2)\n","\n","    where X is the input matrix of shape (N, D), y is the class distribution matrix\n","    of shape (N, C), N is the number of examples (either the entire dataset or\n","    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","    Question (f)\n","\n","    Inputs\n","        X: the input matrix of shape (N, D)\n","\n","    Returns\n","        y: the output of the model\n","        ff_dict: a dictionary containing all the fully connected units and activations.\n","    \"\"\"\n","\n","    ff_dict = {}        \n","    ##### YOUR CODE #####\n","    params = self.params\n","    w1 = params[\"W1\"] \n","    b1 = params[\"b1\"] \n","    w2 = params[\"W2\"] \n","    b2 = params[\"b2\"] \n","\n","    R1 = np.dot(X, w1) + b1 # N * H\n","    Act1 = relu.fw(R1) # N * H\n","    R2 = np.dot(Act1, w2) + b2 # N * C \n","    Act2 = softmax(R2) # N * C\n","\n","    ff_dict['Affine1'] =  R1\n","    ff_dict['ReLU'] = Act1\n","    ff_dict['Affine2'] = R2\n","    ff_dict['Softmax'] = Act2\n","    y = Act2\n","    #####################\n","    return y, ff_dict\n","\n","def backward_relu(self, X, Y, ff_dict):\n","    \"\"\"\n","    Performs backpropagation over the two-layer neural network, and returns\n","    a dictionary of gradients of all model parameters.\n","\n","    Question (f)\n","\n","    Inputs:\n","        - X: the input matrix of shape (B, D), where B is the number of examples\n","            in a mini-batch, D is the feature dimensionality.\n","        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","            where B is the number of examples in a mini-batch, C is the number\n","            of classes.\n","        - ff_dict: the dictionary containing all the fully connected units and\n","            activations.\n","\n","    Returns:\n","        - grads: a dictionary containing the gradients of corresponding weights\n","            and biases.\n","    \"\"\"\n","\n","    grads = {}\n","    ##### YOUR CODE #####\n","    batch_size = X.shape[0]\n","    Act1 = ff_dict['Affine1'] \n","    z1 = ff_dict['ReLU']\n","    z2 = ff_dict['Softmax']\n","    w2 = self.params[\"W2\"]\n","\n","    dz2 = (z2-Y) / batch_size  # normalization & entropy diff\n","    grads[\"dW2\"] = np.dot(z1.T, dz2)\n","    grads[\"db2\"] = np.sum(dz2, axis = 0)\n","    \n","    dz1 = relu.bw(Act1) * np.dot(dz2, w2.T) # relu diff\n","    grads[\"dW1\"] = np.dot(X.T, dz1)\n","    grads[\"db1\"] = np.sum(dz1, axis = 0)\n","    #####################\n","    return grads\n","\n","TwoLayerNNRelu = copy.copy(TwoLayerNN)\n","TwoLayerNNRelu.initialize_parameters = initialize_parameters\n","TwoLayerNNRelu.forward = forward_relu\n","TwoLayerNNRelu.backward = backward_relu"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"2qY3T98XvEIP"},"outputs":[],"source":["### \n","# Question (f)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"h-jJRXqsBxzh"},"outputs":[],"source":["# model instantiation\n","model_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=80, num_classes=10)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"EC8f80a0w53m"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 00 - train loss/acc: 0.511 0.868, valid loss/acc: 0.517 0.866\n","epoch 01 - train loss/acc: 0.399 0.890, valid loss/acc: 0.408 0.886\n","epoch 02 - train loss/acc: 0.353 0.901, valid loss/acc: 0.361 0.897\n","epoch 03 - train loss/acc: 0.325 0.909, valid loss/acc: 0.333 0.905\n","epoch 04 - train loss/acc: 0.306 0.913, valid loss/acc: 0.314 0.909\n","epoch 05 - train loss/acc: 0.290 0.919, valid loss/acc: 0.299 0.915\n","epoch 06 - train loss/acc: 0.277 0.922, valid loss/acc: 0.286 0.918\n","epoch 07 - train loss/acc: 0.264 0.926, valid loss/acc: 0.274 0.921\n","epoch 08 - train loss/acc: 0.254 0.929, valid loss/acc: 0.265 0.923\n","epoch 09 - train loss/acc: 0.245 0.932, valid loss/acc: 0.256 0.926\n","epoch 10 - train loss/acc: 0.238 0.933, valid loss/acc: 0.250 0.927\n","epoch 11 - train loss/acc: 0.230 0.937, valid loss/acc: 0.241 0.930\n","epoch 12 - train loss/acc: 0.223 0.938, valid loss/acc: 0.234 0.932\n","epoch 13 - train loss/acc: 0.216 0.940, valid loss/acc: 0.229 0.933\n","epoch 14 - train loss/acc: 0.211 0.942, valid loss/acc: 0.222 0.935\n","epoch 15 - train loss/acc: 0.205 0.943, valid loss/acc: 0.218 0.936\n","epoch 16 - train loss/acc: 0.200 0.944, valid loss/acc: 0.213 0.937\n","epoch 17 - train loss/acc: 0.195 0.945, valid loss/acc: 0.209 0.940\n","epoch 18 - train loss/acc: 0.190 0.947, valid loss/acc: 0.203 0.941\n","epoch 19 - train loss/acc: 0.186 0.949, valid loss/acc: 0.199 0.941\n","epoch 20 - train loss/acc: 0.182 0.950, valid loss/acc: 0.195 0.943\n","epoch 21 - train loss/acc: 0.178 0.950, valid loss/acc: 0.192 0.944\n","epoch 22 - train loss/acc: 0.174 0.952, valid loss/acc: 0.189 0.945\n","epoch 23 - train loss/acc: 0.171 0.952, valid loss/acc: 0.186 0.946\n","epoch 24 - train loss/acc: 0.167 0.953, valid loss/acc: 0.182 0.947\n","epoch 25 - train loss/acc: 0.164 0.954, valid loss/acc: 0.179 0.947\n","epoch 26 - train loss/acc: 0.160 0.955, valid loss/acc: 0.176 0.948\n","epoch 27 - train loss/acc: 0.157 0.957, valid loss/acc: 0.173 0.950\n","epoch 28 - train loss/acc: 0.154 0.957, valid loss/acc: 0.170 0.950\n","epoch 29 - train loss/acc: 0.151 0.958, valid loss/acc: 0.168 0.951\n","epoch 30 - train loss/acc: 0.148 0.959, valid loss/acc: 0.165 0.952\n","epoch 31 - train loss/acc: 0.146 0.959, valid loss/acc: 0.163 0.953\n","epoch 32 - train loss/acc: 0.143 0.960, valid loss/acc: 0.160 0.955\n","epoch 33 - train loss/acc: 0.141 0.961, valid loss/acc: 0.159 0.954\n","epoch 34 - train loss/acc: 0.139 0.961, valid loss/acc: 0.157 0.956\n","epoch 35 - train loss/acc: 0.136 0.962, valid loss/acc: 0.155 0.956\n","epoch 36 - train loss/acc: 0.134 0.963, valid loss/acc: 0.152 0.958\n","epoch 37 - train loss/acc: 0.132 0.963, valid loss/acc: 0.150 0.958\n","epoch 38 - train loss/acc: 0.130 0.964, valid loss/acc: 0.149 0.958\n","epoch 39 - train loss/acc: 0.128 0.965, valid loss/acc: 0.148 0.959\n"]}],"source":["# train the model\n","lr, n_epochs, batch_size = 2.5, 40, 128\n","history = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"4i__6TfpCqOc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final test loss = 0.142, acc = 0.960\n"]}],"source":["Y_hat, _ = model_relu.forward(X_test)\n","test_loss = model_relu.compute_loss(Y_test, Y_hat)\n","test_acc = model_relu.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('sangwon')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"bae682e32284fbf211f0b4aefb7a0e37ec70f968d3818fa236ac5ae695ef2927"}}},"nbformat":4,"nbformat_minor":0}
