{
  "hash": "632206edb3e2c3147cee5479531ba318",
  "result": {
    "markdown": "---\ntitle: \"Textmining on SDG VNRs of ROK, Germany, Finland\"\nformat: \n    html:\n        code-fold: true\nauthor: \"Sangwon Ju, SNU GSPA\" \nknitr:\n  opts_knit:\n    verbose: false\njupyter: python3\ndate: 2022/08/06\n---\n\n\n1.  기술통계 분석\n\n-   TF-IDF (Term Frequency - Inverse Document Frequency): 단어 빈도 - 역 문서 빈도 총 문서 수 대비 적게 등장한 단어가 중요한 단어 특정 문서 안에서 많이 등장한다고 해도 중요도가 올라가지는 않음\n\n-   TF: 특정 문서 D에서 특정 단어 T의 등장 횟수\n\n-   DF: 특정 단어 T가 등장한 문서의 수\n\n-   IDF: DF에 반비례 하는 수. ln(총문서수/(DF))\n\n-   TF-IDF: TF \\* IDF 희귀하면서도 특정 텍스트에서 자주 사용된 단어 (TF)는 그 텍스트에서 중요함\n\n2.  단어간 혹은 기사간 상관관계\n\n-   동시 출현(Co-Occurrence) 단어 분석 문장 혹은 기사에 함께 사용된 단어는 어떤 단어일지 분석하는 것. 단어의 \"맥락\"을 파악하기 위하여 어떤 단어들이 함께 쓰였는지를 알아야 함. 의미를 가진 단어(명사, 동사, 형용사)등을 추출하여 어떤 단어들이 함께 빈번하게 쓰였는지 분석해보는게 필요함.\n\n3.  연관 규칙 분석 (Association Rules)\n\n-   장바구니 분석 Apriori Algorithm(Agrawal et al., 1993): 어떤 단어가 다른 단어들과의 연관규칙을 가지는지를 추출 하는 방식\n\n4.  **토픽 모델링**\n\n-   토픽 모델링은 문서와 단어로 구성된 행렬(DTM)을 기반으로 문서에 잠재되어 (Latent)있다고 가정된 토픽의 등장확률을 추정하는 일련의 통계적 텍스트 처리기법을 일컫는다. (Blei, 2014; Blei and Lafferty, 2007;Blei, Ng and Jordan, 2003) DTM을 활용하여 주제-확률 분포, 단어-확률 분포를 구한뒤 잠재 주제를 찾는 LDA나 Singular Value Decomposition을 통해 차원 축소를 하는 방법이 있다.\n\n-   **LDA(Latent Dirichlet Allocation):**\n\n    **이 문서에서는 어떤 주제들이 오가고 있을까?**\n\n    PLSA를 조건부 확률로 확장시킨 기법으로 잠재 주제의 확률적 분포에 대한 PLSA의 한계점을 보완한 모델이다. LDA모델은 무작위로 섞여있는 대량의 문서에서 단어들의 패턴을 추론하여 각 토픽의 특성을 도출하는데 용이하며, 텍스트 데이터의 의미구조를 파악하기에 적합한 방법 중 하나이다. 한 문서는 여러가지 토픽으로 이루어지고, 토픽은 여러 단어를 혼합하여 구성된다.\n\n-   1개의 토픽은 여러 단어(서로 다른 확률을 가진)로 구성.\n\n-   1개의 단어는 여러 토픽에서 서로 다른 확률을 가짐.\n\n-   delta는 문장이 각 토픽에 등장할 확률, beta는 단어가 각 토픽에 등장할 확률\n\n# Loading Package\n\n\n::: {.cell result='hide'}\n\n```{.r .cell-code}\npacman::p_load(\"reticulate\",\"tidyverse\",\"tidymodels\",\n               \"tidytext\",\"lda\",\"readr\",\"igraph\",\"forcats\",\n               \"sna\",\"ergm\",\"network\",\"stringr\",\"magrittr\",\"showtext\",\n               \"tokenizers\",\"janeaustenr\",\"forcats\",\"RColorBrewer\",\n               \"viridis\",\"stopwords\",\"purrr\",\"widyr\",\"textmineR\",\"stm\",\n               \"ggraph\",\"tidygraph\",\"ggfortify\",\"ggthemes\",\"cowplot\",\"fs\",\n               \"knitr\",\"kableExtra\",\"ggrepel\",\"grid\",\"gridExtra\",\"topicmodels\",\n               \"scales\",\"ggpattern\",\"magick\",\"corrplot\")\n\n# Font Setting\nfont_add_google(name=\"Gowun Dodum\") #fonts.google.com에서 고르시면 됨\nshowtext_auto()\n```\n:::\n\n\n# Setting Working Directory\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_folder = \"e:/OneDrive - SNU/r/vnr\"\nger_path = fs::path(data_folder, \"germany.txt\")\nfin_path = fs::path(data_folder, \"finland.txt\")\nkor_path = fs::path(data_folder, \"korea.txt\")\njp_path = fs::path(data_folder, \"japan.txt\")\nuk_path = fs::path(data_folder, \"uk.txt\")\nastl_path = fs::path(data_folder, \"austrailia.txt\")\n```\n:::\n\n\n표제어 추출을 통해 어근을 추출하고 토크나이징을 통해서 토픽 추출 용이하게 만들기.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentence_word1=function(x){\n  x=as.character(x)\n  a=tibble(words=tokenize_sentences(x) %>%\n             unlist() %>% tibble(words=.),\n    sentence=1:nrow(words))\n  return(a)}\n\nsentence_word2=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_words(stopwords =  as.vector(stop_words$word),\n                                strip_numeric = T) %>%  \n               unlist())\n  return(a)}\n\nsentence_word3=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_ngrams(.,n = 2, n_min = 2,\n               stopwords = as.vector(stop_words$word)) %>%\n               unlist())\n  return(a)}\n```\n:::\n\n\n# Korea VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea <-  readLines(kor_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nBreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_tokenized <- korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_ngram  <-  korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_tokenized <- korea_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n    \n\nkorea_ngram <- korea_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) \n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(korea_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"대한민국\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> a)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(korea_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"대한민국\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> a1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-8-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_tfidf <- korea_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nkorea_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> conflict </td>\n   <td style=\"text-align:right;\"> 52 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1379310 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.4480599 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> governmental </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1538462 </td>\n   <td style=\"text-align:right;\"> 2.842970 </td>\n   <td style=\"text-align:right;\"> 0.4373799 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> institutional </td>\n   <td style=\"text-align:right;\"> 78 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 2.437504 </td>\n   <td style=\"text-align:right;\"> 0.4062507 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> saemaul </td>\n   <td style=\"text-align:right;\"> 48 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1219512 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.3961506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> undong </td>\n   <td style=\"text-align:right;\"> 48 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1219512 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.3961506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> disability </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 0.1304348 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.3946032 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> humanitarian </td>\n   <td style=\"text-align:right;\"> 49 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1555556 </td>\n   <td style=\"text-align:right;\"> 2.437504 </td>\n   <td style=\"text-align:right;\"> 0.3791674 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:right;\"> 89 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.0961538 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3789983 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mechanism </td>\n   <td style=\"text-align:right;\"> 78 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 2.236834 </td>\n   <td style=\"text-align:right;\"> 0.3728056 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:right;\"> 87 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.0823529 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3246009 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> indicator </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 0.1500000 </td>\n   <td style=\"text-align:right;\"> 2.069780 </td>\n   <td style=\"text-align:right;\"> 0.3104669 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> person </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1014493 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.3069136 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> south </td>\n   <td style=\"text-align:right;\"> 91 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0645161 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2990148 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> indicator </td>\n   <td style=\"text-align:right;\"> 94 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 2.069780 </td>\n   <td style=\"text-align:right;\"> 0.2956828 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> oda </td>\n   <td style=\"text-align:right;\"> 36 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 1.690290 </td>\n   <td style=\"text-align:right;\"> 0.2817150 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> principle </td>\n   <td style=\"text-align:right;\"> 64 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1304348 </td>\n   <td style=\"text-align:right;\"> 2.149822 </td>\n   <td style=\"text-align:right;\"> 0.2804116 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> correspond </td>\n   <td style=\"text-align:right;\"> 33 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0689655 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.2718332 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> discrimination </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.0579710 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2686799 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> administration </td>\n   <td style=\"text-align:right;\"> 89 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.0576923 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2673882 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:right;\"> 31 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.0937500 </td>\n   <td style=\"text-align:right;\"> 2.842970 </td>\n   <td style=\"text-align:right;\"> 0.2665284 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\nkorea_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> korea_dtm.old\n\nrowTotals <- apply(korea_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_dtm <- korea_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_split <- initial_split(korea_tfidf, prop = 0.8)\n\n# Train\nkorea_split_train <- training(korea_split)\nkorea_split_train  %>%\n    cast_dtm(paragraph, words, n) -> korea_train_dtm.old\nrowTotals <- apply(korea_train_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_train_dtm <- korea_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nkorea_split_test <- testing(korea_split)\nkorea_split_test  %>%\n    cast_dtm(paragraph, words, n) -> korea_valid_dtm.old\nrowTotals <- apply(korea_valid_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_valid_dtm <- korea_valid_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(korea_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = korea_valid_dtm)\n}\n\nkorea_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nkorea_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_4 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_5 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_6 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_7 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_coocur <- korea_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- korea_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=korea_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nkorea_g <- as_tbl_graph(korea_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Germany VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany<-readLines(ger_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_tokenized <- germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_ngram  <-  germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_tokenized <- germany_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\n\ngermany_ngram <- germany_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(germany_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#FF0000',\n    pattern_fill = '#FFCC00',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"독일\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> b)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(germany_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#FF0000',\n    pattern_fill = '#FFCC00',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"독일\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> b1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-25-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_tfidf <- germany_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \ngermany_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> fracking </td>\n   <td style=\"text-align:right;\"> 141 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 3.961866 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> civil </td>\n   <td style=\"text-align:right;\"> 352 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 3.109586 </td>\n   <td style=\"text-align:right;\"> 2.073057 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hospital </td>\n   <td style=\"text-align:right;\"> 81 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.980933 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wood </td>\n   <td style=\"text-align:right;\"> 318 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 5.249652 </td>\n   <td style=\"text-align:right;\"> 1.749884 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> administration </td>\n   <td style=\"text-align:right;\"> 288 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 4.333361 </td>\n   <td style=\"text-align:right;\"> 1.733345 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> import </td>\n   <td style=\"text-align:right;\"> 363 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 4.844187 </td>\n   <td style=\"text-align:right;\"> 1.614729 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> urban </td>\n   <td style=\"text-align:right;\"> 252 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 3.052428 </td>\n   <td style=\"text-align:right;\"> 1.526214 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> norway </td>\n   <td style=\"text-align:right;\"> 327 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2857143 </td>\n   <td style=\"text-align:right;\"> 5.249652 </td>\n   <td style=\"text-align:right;\"> 1.499901 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> job </td>\n   <td style=\"text-align:right;\"> 196 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.745575 </td>\n   <td style=\"text-align:right;\"> 1.498230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> adaptation </td>\n   <td style=\"text-align:right;\"> 307 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.640214 </td>\n   <td style=\"text-align:right;\"> 1.456086 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> farm </td>\n   <td style=\"text-align:right;\"> 58 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.544904 </td>\n   <td style=\"text-align:right;\"> 1.417962 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> criminal </td>\n   <td style=\"text-align:right;\"> 339 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 4.151040 </td>\n   <td style=\"text-align:right;\"> 1.383680 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> family </td>\n   <td style=\"text-align:right;\"> 53 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 3.996889 </td>\n   <td style=\"text-align:right;\"> 1.332296 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> migration </td>\n   <td style=\"text-align:right;\"> 235 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2727273 </td>\n   <td style=\"text-align:right;\"> 4.844187 </td>\n   <td style=\"text-align:right;\"> 1.321142 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mint </td>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.320622 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> arctic </td>\n   <td style=\"text-align:right;\"> 305 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.320622 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> profit </td>\n   <td style=\"text-align:right;\"> 365 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.320622 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> disability </td>\n   <td style=\"text-align:right;\"> 233 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3000000 </td>\n   <td style=\"text-align:right;\"> 4.333361 </td>\n   <td style=\"text-align:right;\"> 1.300008 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> labour </td>\n   <td style=\"text-align:right;\"> 194 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.170211 </td>\n   <td style=\"text-align:right;\"> 1.268084 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> south </td>\n   <td style=\"text-align:right;\"> 369 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.170211 </td>\n   <td style=\"text-align:right;\"> 1.268084 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\ngermany_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> germany_dtm.old\n\nrowTotals <- apply(germany_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_dtm <- germany_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_split <- initial_split(germany_tfidf, prop = 0.8)\n\n# Train\ngermany_split_train <- training(germany_split)\ngermany_split_train  %>%\n    cast_dtm(paragraph, words, n) -> germany_train_dtm.old\n\nrowTotals <- apply(germany_train_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_train_dtm <- germany_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\ngermany_split_test <- testing(germany_split)\ngermany_split_test  %>%\n    cast_dtm(paragraph, words, n) -> germany_test_dtm.old\n\nrowTotals <- apply(germany_test_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_test_dtm <- germany_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(germany_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = germany_test_dtm)\n}\n\ngermany_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\ngermany_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_4 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_5 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_6 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_7 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_coocur <- germany_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- germany_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=germany_tp_7,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\ngermany_g <- as_tbl_graph(germany_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Finland VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland <-  readLines(fin_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_tokenized <- finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_ngram  <-  finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_tokenized <-finland_tokenized  %>%\n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"billion\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\"))  %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\nfinland_ngram <- finland_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"billion\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(finland_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#003580',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"핀란드\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> c)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-42-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(germany_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#003580',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"핀란드\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> c1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-42-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_tfidf <- finland_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nfinland_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> species </td>\n   <td style=\"text-align:right;\"> 549 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.2941176 </td>\n   <td style=\"text-align:right;\"> 4.330733 </td>\n   <td style=\"text-align:right;\"> 1.2737451 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> preparation </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3750000 </td>\n   <td style=\"text-align:right;\"> 3.392464 </td>\n   <td style=\"text-align:right;\"> 1.2721739 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> city </td>\n   <td style=\"text-align:right;\"> 97 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3000000 </td>\n   <td style=\"text-align:right;\"> 3.963009 </td>\n   <td style=\"text-align:right;\"> 1.1889026 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> loss </td>\n   <td style=\"text-align:right;\"> 564 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 4.448516 </td>\n   <td style=\"text-align:right;\"> 0.9885592 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> version </td>\n   <td style=\"text-align:right;\"> 39 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1818182 </td>\n   <td style=\"text-align:right;\"> 5.429346 </td>\n   <td style=\"text-align:right;\"> 0.9871538 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> audit </td>\n   <td style=\"text-align:right;\"> 330 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.2352941 </td>\n   <td style=\"text-align:right;\"> 4.043051 </td>\n   <td style=\"text-align:right;\"> 0.9513062 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ministry </td>\n   <td style=\"text-align:right;\"> 123 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 2.814386 </td>\n   <td style=\"text-align:right;\"> 0.9381286 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attitude </td>\n   <td style=\"text-align:right;\"> 245 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1818182 </td>\n   <td style=\"text-align:right;\"> 5.141664 </td>\n   <td style=\"text-align:right;\"> 0.9348479 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> film </td>\n   <td style=\"text-align:right;\"> 114 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 6.527958 </td>\n   <td style=\"text-align:right;\"> 0.9325654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> fishery </td>\n   <td style=\"text-align:right;\"> 204 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 6.527958 </td>\n   <td style=\"text-align:right;\"> 0.9325654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> annually </td>\n   <td style=\"text-align:right;\"> 657 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 0.2127660 </td>\n   <td style=\"text-align:right;\"> 4.330733 </td>\n   <td style=\"text-align:right;\"> 0.9214326 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> minority </td>\n   <td style=\"text-align:right;\"> 244 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2142857 </td>\n   <td style=\"text-align:right;\"> 4.225373 </td>\n   <td style=\"text-align:right;\"> 0.9054370 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> plastic </td>\n   <td style=\"text-align:right;\"> 514 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 5.141664 </td>\n   <td style=\"text-align:right;\"> 0.8569439 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> waste </td>\n   <td style=\"text-align:right;\"> 517 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 3.819908 </td>\n   <td style=\"text-align:right;\"> 0.8488684 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:right;\"> 646 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.2272727 </td>\n   <td style=\"text-align:right;\"> 3.694745 </td>\n   <td style=\"text-align:right;\"> 0.8397147 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> alien </td>\n   <td style=\"text-align:right;\"> 568 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 5.834811 </td>\n   <td style=\"text-align:right;\"> 0.8335444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> invasive </td>\n   <td style=\"text-align:right;\"> 568 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 5.834811 </td>\n   <td style=\"text-align:right;\"> 0.8335444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> fish </td>\n   <td style=\"text-align:right;\"> 553 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2000000 </td>\n   <td style=\"text-align:right;\"> 4.130063 </td>\n   <td style=\"text-align:right;\"> 0.8260125 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> backlog </td>\n   <td style=\"text-align:right;\"> 479 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1250000 </td>\n   <td style=\"text-align:right;\"> 6.527958 </td>\n   <td style=\"text-align:right;\"> 0.8159947 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> nordic </td>\n   <td style=\"text-align:right;\"> 213 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.2058824 </td>\n   <td style=\"text-align:right;\"> 3.963009 </td>\n   <td style=\"text-align:right;\"> 0.8159135 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\nfinland_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> finland_dtm.old\n\nrowTotals <- apply(finland_dtm.old, 1, sum) # Find the sum of words in each Document\nfinland_dtm <- finland_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_4 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-45-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_5 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-46-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_6 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-47-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_7 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-48-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=8)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 8,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_8 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_8 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-49-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=9)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 9,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_9 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_9 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-50-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=10)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 10,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_10 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_10 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-51-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_coocur <- finland_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=finland_tp_7,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nfinland_g <- as_tbl_graph(finland_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-55-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: ggrepel: 4 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: ggrepel: 1 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-56-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Japan VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\njapan<-readLines(jp_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njapan_tokenized <- japan %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njapan_ngram  <-  japan %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\njapan_tokenized <- japan_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"jap*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"asia[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\n\njapan_ngram <- japan_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"jap*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"asia[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(japan_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#BC002D',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"일본\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> d)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-61-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(japan_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#BC002D',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"일본\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> d1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-61-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_tfidf <- japan_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \njapan_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> smoke </td>\n   <td style=\"text-align:right;\"> 298 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.2083333 </td>\n   <td style=\"text-align:right;\"> 6.003887 </td>\n   <td style=\"text-align:right;\"> 1.2508098 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mekong </td>\n   <td style=\"text-align:right;\"> 271 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 6.003887 </td>\n   <td style=\"text-align:right;\"> 0.8576982 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forest </td>\n   <td style=\"text-align:right;\"> 347 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 0.2083333 </td>\n   <td style=\"text-align:right;\"> 3.806662 </td>\n   <td style=\"text-align:right;\"> 0.7930547 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> expectancy </td>\n   <td style=\"text-align:right;\"> 128 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1351351 </td>\n   <td style=\"text-align:right;\"> 5.310740 </td>\n   <td style=\"text-align:right;\"> 0.7176676 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forest </td>\n   <td style=\"text-align:right;\"> 348 </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;\"> 0.1803279 </td>\n   <td style=\"text-align:right;\"> 3.806662 </td>\n   <td style=\"text-align:right;\"> 0.6864473 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> suicide </td>\n   <td style=\"text-align:right;\"> 299 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 0.1494253 </td>\n   <td style=\"text-align:right;\"> 4.394449 </td>\n   <td style=\"text-align:right;\"> 0.6566418 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> household </td>\n   <td style=\"text-align:right;\"> 284 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.1935484 </td>\n   <td style=\"text-align:right;\"> 3.364830 </td>\n   <td style=\"text-align:right;\"> 0.6512574 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> reliance </td>\n   <td style=\"text-align:right;\"> 286 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1176471 </td>\n   <td style=\"text-align:right;\"> 5.310740 </td>\n   <td style=\"text-align:right;\"> 0.6247929 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> litter </td>\n   <td style=\"text-align:right;\"> 232 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1515152 </td>\n   <td style=\"text-align:right;\"> 4.057977 </td>\n   <td style=\"text-align:right;\"> 0.6148450 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> traffic </td>\n   <td style=\"text-align:right;\"> 176 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1129032 </td>\n   <td style=\"text-align:right;\"> 5.310740 </td>\n   <td style=\"text-align:right;\"> 0.5995997 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> city </td>\n   <td style=\"text-align:right;\"> 364 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2307692 </td>\n   <td style=\"text-align:right;\"> 2.477527 </td>\n   <td style=\"text-align:right;\"> 0.5717369 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> food </td>\n   <td style=\"text-align:right;\"> 335 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.2105263 </td>\n   <td style=\"text-align:right;\"> 2.708050 </td>\n   <td style=\"text-align:right;\"> 0.5701158 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> relation </td>\n   <td style=\"text-align:right;\"> 21 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1578947 </td>\n   <td style=\"text-align:right;\"> 3.605992 </td>\n   <td style=\"text-align:right;\"> 0.5693671 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forest </td>\n   <td style=\"text-align:right;\"> 219 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1458333 </td>\n   <td style=\"text-align:right;\"> 3.806662 </td>\n   <td style=\"text-align:right;\"> 0.5551383 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> investment </td>\n   <td style=\"text-align:right;\"> 358 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1904762 </td>\n   <td style=\"text-align:right;\"> 2.912845 </td>\n   <td style=\"text-align:right;\"> 0.5548275 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> railway </td>\n   <td style=\"text-align:right;\"> 177 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.0909091 </td>\n   <td style=\"text-align:right;\"> 6.003887 </td>\n   <td style=\"text-align:right;\"> 0.5458079 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> investor </td>\n   <td style=\"text-align:right;\"> 359 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 3.806662 </td>\n   <td style=\"text-align:right;\"> 0.5438089 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> accident </td>\n   <td style=\"text-align:right;\"> 239 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1086957 </td>\n   <td style=\"text-align:right;\"> 4.905275 </td>\n   <td style=\"text-align:right;\"> 0.5331820 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> approximately </td>\n   <td style=\"text-align:right;\"> 287 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1578947 </td>\n   <td style=\"text-align:right;\"> 3.364830 </td>\n   <td style=\"text-align:right;\"> 0.5312889 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> icon </td>\n   <td style=\"text-align:right;\"> 361 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0869565 </td>\n   <td style=\"text-align:right;\"> 6.003887 </td>\n   <td style=\"text-align:right;\"> 0.5220771 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\njapan_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> japan_dtm.old\n\nrowTotals <- apply(japan_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_dtm <- japan_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_split <- initial_split(japan_tfidf, prop = 0.8)\n\n# Train\njapan_split_train <- training(japan_split)\njapan_split_train  %>%\n    cast_dtm(paragraph, words, n) -> japan_train_dtm.old\n\nrowTotals <- apply(japan_train_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_train_dtm <- japan_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\njapan_split_test <- testing(japan_split)\njapan_split_test  %>%\n    cast_dtm(paragraph, words, n) -> japan_test_dtm.old\n\nrowTotals <- apply(japan_test_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_test_dtm <- japan_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(japan_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = japan_test_dtm)\n}\n\njapan_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\njapan_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-64-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_4 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-65-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_5 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-66-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_6 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-67-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_7 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-68-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=8)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 8,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_8 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_8 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-69-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=9)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 9,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_9 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_9 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-70-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n### Cast LDA (k=10)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_lda <-  LDA(japan_dtm, k = 10,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_10 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_10 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-71-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\njapan_coocur <- japan_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- japan_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=japan_tp_5,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\njapan_g <- as_tbl_graph(japan_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-75-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\njapan_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-76-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# UK VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuk <-readLines(uk_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuk_tokenized <- uk %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuk_ngram  <-  uk %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuk_tokenized <- uk_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"uk[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"wale*\")) %>% \n    filter(!str_detect(words,pattern=\"scot*\")) %>% \n    filter(!str_detect(words,pattern=\"eng*\")) %>% \n    filter(!str_detect(words,pattern=\"irel*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"wt*\")) %>% \n    filter(!str_detect(words,pattern=\"rbmps*\")) %>% \n    filter(!str_detect(words,pattern=\"inn*\")) %>%\n    filter(!str_detect(words,pattern=\"whilst*\")) %>% \n    filter(!str_detect(words,pattern=\"npt*\")) %>%\n    filter(!str_detect(words,pattern=\"britain*\")) \n\nuk_ngram <- uk_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"uk[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"wale*\")) %>% \n    filter(!str_detect(words,pattern=\"scot*\")) %>% \n    filter(!str_detect(words,pattern=\"eng*\")) %>% \n    filter(!str_detect(words,pattern=\"irel*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"wt*\")) %>% \n    filter(!str_detect(words,pattern=\"rbmps*\")) %>% \n    filter(!str_detect(words,pattern=\"inn*\")) %>%\n    filter(!str_detect(words,pattern=\"whilst*\")) %>% \n    filter(!str_detect(words,pattern=\"npt*\")) %>%\n    filter(!str_detect(words,pattern=\"britain*\")) \n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(uk_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00247D',\n    pattern_fill = '#CF142B',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"영국\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> e)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-81-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(uk_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00247D',\n    pattern_fill = '#CF142B',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"독일\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> e1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-81-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_tfidf <- uk_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nuk_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> bus </td>\n   <td style=\"text-align:right;\"> 705 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.7500000 </td>\n   <td style=\"text-align:right;\"> 6.329721 </td>\n   <td style=\"text-align:right;\"> 4.747291 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> nihr </td>\n   <td style=\"text-align:right;\"> 196 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 7.022868 </td>\n   <td style=\"text-align:right;\"> 4.681912 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> daily </td>\n   <td style=\"text-align:right;\"> 709 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 5.636574 </td>\n   <td style=\"text-align:right;\"> 3.757716 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> abortion </td>\n   <td style=\"text-align:right;\"> 201 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 5.231109 </td>\n   <td style=\"text-align:right;\"> 3.487406 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> nhs </td>\n   <td style=\"text-align:right;\"> 210 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 4.943427 </td>\n   <td style=\"text-align:right;\"> 3.295618 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hour </td>\n   <td style=\"text-align:right;\"> 533 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 4.624973 </td>\n   <td style=\"text-align:right;\"> 3.083315 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> contact </td>\n   <td style=\"text-align:right;\"> 199 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 5.636574 </td>\n   <td style=\"text-align:right;\"> 2.818287 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> victim </td>\n   <td style=\"text-align:right;\"> 997 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 4.189655 </td>\n   <td style=\"text-align:right;\"> 2.793103 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> lgbt </td>\n   <td style=\"text-align:right;\"> 652 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.6000000 </td>\n   <td style=\"text-align:right;\"> 4.624973 </td>\n   <td style=\"text-align:right;\"> 2.774984 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> day </td>\n   <td style=\"text-align:right;\"> 429 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 4.132496 </td>\n   <td style=\"text-align:right;\"> 2.754998 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> birth </td>\n   <td style=\"text-align:right;\"> 1002 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 5.413430 </td>\n   <td style=\"text-align:right;\"> 2.706715 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cardiovascular </td>\n   <td style=\"text-align:right;\"> 208 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3750000 </td>\n   <td style=\"text-align:right;\"> 7.022868 </td>\n   <td style=\"text-align:right;\"> 2.633576 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> landfill </td>\n   <td style=\"text-align:right;\"> 769 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 5.231109 </td>\n   <td style=\"text-align:right;\"> 2.615554 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> conviction </td>\n   <td style=\"text-align:right;\"> 334 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 6.329721 </td>\n   <td style=\"text-align:right;\"> 2.531888 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> participatory </td>\n   <td style=\"text-align:right;\"> 1014 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 6.329721 </td>\n   <td style=\"text-align:right;\"> 2.531888 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> paramilitarism </td>\n   <td style=\"text-align:right;\"> 1027 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 6.329721 </td>\n   <td style=\"text-align:right;\"> 2.531888 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> road </td>\n   <td style=\"text-align:right;\"> 207 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.5555556 </td>\n   <td style=\"text-align:right;\"> 4.537961 </td>\n   <td style=\"text-align:right;\"> 2.521090 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pupil </td>\n   <td style=\"text-align:right;\"> 312 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.6000000 </td>\n   <td style=\"text-align:right;\"> 4.189655 </td>\n   <td style=\"text-align:right;\"> 2.513793 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mobility </td>\n   <td style=\"text-align:right;\"> 670 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.4615385 </td>\n   <td style=\"text-align:right;\"> 5.231109 </td>\n   <td style=\"text-align:right;\"> 2.414358 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> product </td>\n   <td style=\"text-align:right;\"> 758 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 3.557132 </td>\n   <td style=\"text-align:right;\"> 2.371421 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\nuk_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> uk_dtm.old\n\nrowTotals <- apply(uk_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_dtm <- uk_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_split <- initial_split(uk_tfidf, prop = 0.8)\n\n# Train\nuk_split_train <- training(uk_split)\nuk_split_train  %>%\n    cast_dtm(paragraph, words, n) -> uk_train_dtm.old\n\nrowTotals <- apply(uk_train_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_train_dtm <- uk_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nuk_split_test <- testing(uk_split)\nuk_split_test  %>%\n    cast_dtm(paragraph, words, n) -> uk_test_dtm.old\n\nrowTotals <- apply(uk_test_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_test_dtm <- uk_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(uk_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = uk_test_dtm)\n}\n\nuk_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nuk_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-84-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_lda <-  LDA(uk_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_4 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-85-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_lda <-  LDA(uk_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_5 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-86-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_lda <-  LDA(uk_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_6 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-87-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_lda <-  LDA(uk_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_7 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-88-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuk_coocur <- uk_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- uk_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=uk_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nuk_g <- as_tbl_graph(uk_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-92-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nuk_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-93-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Austrailia VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\naustrailia<-readLines(astl_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naustrailia_tokenized <- austrailia %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naustrailia_ngram  <-  austrailia %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\naustrailia_tokenized <- austrailia_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"austra*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"torres*\"))%>% \n    filter(!str_detect(words,pattern=\"cent\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\naustrailia_ngram <- germany_ngram %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"austrail[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"torres*\"))%>% \n    filter(!str_detect(words,pattern=\"cent\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(austrailia_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00008B',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"호주\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> f)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-98-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(austrailia_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00008B',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"호주\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> f1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-98-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_tfidf <- austrailia_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \naustrailia_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> broadband </td>\n   <td style=\"text-align:right;\"> 206 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.3076923 </td>\n   <td style=\"text-align:right;\"> 5.971262 </td>\n   <td style=\"text-align:right;\"> 1.837311 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> scarcity </td>\n   <td style=\"text-align:right;\"> 150 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 4.584968 </td>\n   <td style=\"text-align:right;\"> 1.833987 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> park </td>\n   <td style=\"text-align:right;\"> 294 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 4.361824 </td>\n   <td style=\"text-align:right;\"> 1.744730 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> fish </td>\n   <td style=\"text-align:right;\"> 305 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.3888889 </td>\n   <td style=\"text-align:right;\"> 4.361824 </td>\n   <td style=\"text-align:right;\"> 1.696265 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> child </td>\n   <td style=\"text-align:right;\"> 64 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 2.926739 </td>\n   <td style=\"text-align:right;\"> 1.463370 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> child </td>\n   <td style=\"text-align:right;\"> 111 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 2.926739 </td>\n   <td style=\"text-align:right;\"> 1.463370 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> carbon </td>\n   <td style=\"text-align:right;\"> 283 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3750000 </td>\n   <td style=\"text-align:right;\"> 3.668677 </td>\n   <td style=\"text-align:right;\"> 1.375754 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> lgbtiqa </td>\n   <td style=\"text-align:right;\"> 103 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 5.971262 </td>\n   <td style=\"text-align:right;\"> 1.326947 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> discrimination </td>\n   <td style=\"text-align:right;\"> 222 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 3.774037 </td>\n   <td style=\"text-align:right;\"> 1.258012 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> world </td>\n   <td style=\"text-align:right;\"> 293 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.6000000 </td>\n   <td style=\"text-align:right;\"> 1.963929 </td>\n   <td style=\"text-align:right;\"> 1.178357 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> profit </td>\n   <td style=\"text-align:right;\"> 350 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2500000 </td>\n   <td style=\"text-align:right;\"> 4.584968 </td>\n   <td style=\"text-align:right;\"> 1.146242 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> basin </td>\n   <td style=\"text-align:right;\"> 163 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2727273 </td>\n   <td style=\"text-align:right;\"> 4.179502 </td>\n   <td style=\"text-align:right;\"> 1.139864 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> goal </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 3.406312 </td>\n   <td style=\"text-align:right;\"> 1.135438 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> collaboration </td>\n   <td style=\"text-align:right;\"> 219 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 3.406312 </td>\n   <td style=\"text-align:right;\"> 1.135438 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> human </td>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.4545455 </td>\n   <td style=\"text-align:right;\"> 2.474754 </td>\n   <td style=\"text-align:right;\"> 1.124888 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rubbish </td>\n   <td style=\"text-align:right;\"> 301 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1875000 </td>\n   <td style=\"text-align:right;\"> 5.971262 </td>\n   <td style=\"text-align:right;\"> 1.119612 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> park </td>\n   <td style=\"text-align:right;\"> 303 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2500000 </td>\n   <td style=\"text-align:right;\"> 4.361824 </td>\n   <td style=\"text-align:right;\"> 1.090456 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mhfa </td>\n   <td style=\"text-align:right;\"> 102 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1818182 </td>\n   <td style=\"text-align:right;\"> 5.971262 </td>\n   <td style=\"text-align:right;\"> 1.085684 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mikta </td>\n   <td style=\"text-align:right;\"> 361 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1818182 </td>\n   <td style=\"text-align:right;\"> 5.971262 </td>\n   <td style=\"text-align:right;\"> 1.085684 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wto </td>\n   <td style=\"text-align:right;\"> 371 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.2352941 </td>\n   <td style=\"text-align:right;\"> 4.584968 </td>\n   <td style=\"text-align:right;\"> 1.078816 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\naustrailia_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> austrailia_dtm.old\n\nrowTotals <- apply(austrailia_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_dtm <- austrailia_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_split <- initial_split(austrailia_tfidf, prop = 0.8)\n\n# Train\naustrailia_split_train <- training(austrailia_split)\naustrailia_split_train  %>%\n    cast_dtm(paragraph, words, n) -> austrailia_train_dtm.old\n\nrowTotals <- apply(austrailia_train_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_train_dtm <- austrailia_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\naustrailia_split_test <- testing(austrailia_split)\naustrailia_split_test  %>%\n    cast_dtm(paragraph, words, n) -> austrailia_test_dtm.old\n\nrowTotals <- apply(austrailia_test_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_test_dtm <- austrailia_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(austrailia_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = austrailia_test_dtm)}\n\naustrailia_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\naustrailia_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-101-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_lda <-  LDA(austrailia_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_4 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-102-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_lda <-  LDA(austrailia_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_5 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-103-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_lda <-  LDA(austrailia_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_6 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-104-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_lda <-  LDA(austrailia_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_7 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-105-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\naustrailia_coocur <- austrailia_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- austrailia_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=austrailia_tp_7,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\naustrailia_g <- as_tbl_graph(austrailia_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-109-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\naustrailia_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-110-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### simplify?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks=finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") %>%\n    left_join(finland_tp_7 %>% dplyr::select(-beta,term) %>% \n                  transmute(source=term,topic1=topic),by=\"source\") %>% \n    left_join(finland_tp_7 %>% dplyr::select(-beta,term) %>% \n                  transmute(target=term,topic2=topic),by=\"target\") %>% \n    dplyr::select(topic1,topic2,weight) %>% \n    rename(source=\"topic1\",target=\"topic2\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nfinland_network <- graph_from_data_frame(d = links, directed = F)\nfinland_g <- as_tbl_graph(simplify(finland_network))\n\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree()) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight), show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(size = degree, color=\"blue\")) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Gowun Dodum\",size=10, max.overlaps = Inf) +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\",color=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-111-1.png){width=672}\n:::\n:::\n\n\n# Comparing Six countries\n\n## Comparing Word Frequencies of Six Countries\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_tokenized %>% \n    mutate(key=\"대한민국\") %>% \n    full_join(germany_tokenized %>%\n                  mutate(key=\"독일\")) %>% \n    full_join(finland_tokenized %>%\n                  mutate(key=\"핀란드\")) %>% \n    full_join(japan_tokenized %>% \n                mutate(key=\"일본\")) %>%\n    full_join(uk_tokenized %>% \n                mutate(key=\"영국\")) %>%\n    full_join(austrailia_tokenized %>% \n                mutate(key=\"호주\")) %>%\n    mutate(key=as_factor(key)) -> total_tokenized\n\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    group_by(key) %>% \n    top_n(n, n=20) %>%\n    arrange(desc(n)) %>% \n    mutate(words = reorder_within(words, n, key)) %>%\n    ggplot(aes(x=n, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 2, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing Word Frequencies of Six Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))+\n    ggeasy::easy_center_title()\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-112-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Comparing bigrams of Six Countries\n\n\n::: {.cell .column-screen-inset-shaded layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid.arrange(a1,b1,c1,d1,e1,f1, nrow=2,\n             top=textGrob(\"Comparing Bigrams of Six Countries\",\n                          gp = gpar(col = \"black\", \n                                    fontsize = 40,\n                                    fontfamily=\"Gowun Dodum\")))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-113-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Comparing Tf-idf of Six Countries\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    bind_tf_idf(words, key, n) %>% \n    group_by(key) %>% \n    top_n(tf_idf, n=20) %>%\n    arrange(desc(tf_idf)) %>% \n    mutate(words = reorder_within(words, tf_idf, key)) %>%\n    ggplot(aes(x=tf_idf, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 2, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing TF-IDF of Six Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  +\n    ggeasy::easy_center_title()\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-114-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Comparing the word frequencies\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_six <- total_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    dplyr::select(-n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>%\n    pivot_longer(3:7,\n               names_to = \"key\", values_to = \"proportion\")\n```\n:::\n\n\nWords that are far from the line are words that are found more in one set of texts than another.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\n# expect a warning about rows with missing values being removed\nset.seed(2022)\ncompare_six %>%\n    ggplot(aes(x = proportion, y = 대한민국, color = abs(대한민국 - proportion))) +\n    geom_abline(color = \"darkgreen\", lty = 2, size=1.5) +\n    geom_jitter(alpha = 0.1, size = 1.5, width = 0.3, height = 0.3) +\n    geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5, family=\"Gowun Dodum\",size=9) +\n    scale_x_log10(labels = percent_format()) +\n    scale_y_log10(labels = percent_format()) +\n    scale_color_gradient(limits = c(0.001, 0.01), \n                         low = \"darkgreen\", high =  \"black\") +\n    facet_wrap(~key, nrow = 2) +\n    theme(legend.position=\"none\") +\n    labs(y = \"대한민국\", x = NULL, title=\"Comparing the word frequencies of Six Countries\") +\n    theme_bw() +\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Gowun Dodum\",size=25))+\n    guides(color=\"none\") +\n    ggeasy::easy_center_title()\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-116-1.png){fig-align='center' width=1056}\n:::\n:::\n\n\n## Correlation of words\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ntotal_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    dplyr::select(- n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>% \n    dplyr::select(- words) -> correlation_total  \n\ntestRes=corrplot::cor.mtest(correlation_total, conf.level = 0.95)\n\ncorrelation_total %>% \n    cor(use=\"pairwise.complete.obs\") %>% \n    .[order(.[ , 1],decreasing = T), order(.[ 1, ],decreasing = T)] %>% \n    corrplot(tl.col = \"black\",\n             diag=T, \n             type=\"lower\",\n             method=\"color\",\n             cl.pos=\"n\",\n             addCoef.col = 1,\n             number.cex = 4,\n             tl.cex = 3) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-117-1.png){fig-align='center' width=576}\n:::\n:::\n\n::: {.cell result='hide'}\n\n```{.r .cell-code}\ncortrix = function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \n  \"spearman\"), ...) \n{\n  x = as.matrix(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  cormeth <- method\n  panel.cor <- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", \n    method = cormeth, cex.cor, ...) {\n    usr <- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- cor(x, y, use = use, method = method)\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex <- 2\n    test <- cor.test(as.numeric(x), as.numeric(y), method = method)\n    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, \n      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n        \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex)\n    text(0.8, 0.8, Signif, cex = 2, col = 2)\n  }\n  f <- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs <- list(...)\n  dotargs$method <- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light blue\", probability = TRUE, axes = FALSE, \n      main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 2)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n      diag.panel = hist.panel, ...)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n    ...)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorrelation_total %>% \n    cortrix(histogram=F)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-119-1.png){fig-align='center' width=150%}\n:::\n:::\n\n\n\\*\\*\\*: 0 - 0.001\n\n## Document Similarity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_string <- korea %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \ngermany_string <- germany %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \nfinland_string <- finland %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \njapan_string <- japan %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \nuk_string <- uk %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \naustrailia_string <- austrailia %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pylab as plt \nimport matplotlib as mpl\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\nfrom tabulate import tabulate\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport warnings\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport string\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmpl.rc('font', family='NanumGothic') # 폰트 설정\nmpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정\n\n# 차트 스타일 설정\nsns.set(font=\"NanumGothic\", rc={\"axes.unicode_minus\":False}, style='white')\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n### Convert data from R objects\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndocument_df = pd.DataFrame.from_dict(\n    {'filename': [\"Korea\", \"Germany\", \"Finland\", \"Japan\", \"UK\", \"Austrailia\"],\n    'opinion_text':[r.korea_string, r.germany_string, r.finland_string, \n    r.japan_string, r.uk_string, r.austrailia_string]})\n```\n:::\n\n\n### Tokenizer\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 단어 원형 추출 함수\nlemmar = WordNetLemmatizer()\n\ndef LemTokens(tokens):\n    return [lemmar.lemmatize(token) for token in tokens]\n\n# 특수 문자 사전 생성: {33: None ...}\n# ord(): 아스키 코드 생성\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n\n# 특수 문자 제거 및 단어 원형 추출\ndef LemNormalize(text):\n    text_new = text.lower().translate(remove_punct_dict)\n    word_tokens = nltk.word_tokenize(text_new)\n    return LemTokens(word_tokens)\n\n# nltk.download('punkt')\n# nltk.download('wordnet')\n# nltk.download('omw-1.4')\ntfidf_vect = TfidfVectorizer(stop_words='english' , ngram_range=(1,2), \n                             tokenizer = LemNormalize, min_df=0.05, max_df=0.85)\n\nfeature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])    \n```\n:::\n\n\n### Cosine Similarity\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_pair = cosine_similarity(feature_vect[0] , feature_vect)\ndocument_df[\"similarity\"] = similarity_pair.reshape(-1,1)\n```\n:::\n\n\n### 시각화\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nplt.clf()\nvis=document_df.iloc[1:].sort_values(by=\"similarity\",ascending=False).reset_index(drop=True)\nsns.set_style(\"white\")\nplt.figure(figsize = (10,7))\nsns.barplot(x=\"similarity\", y=\"filename\", data=vis)\nplt.title('Korea')\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-126-1.png){fig-align='center' width=960}\n:::\n:::\n",
    "supporting": [
      "Vnr_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}