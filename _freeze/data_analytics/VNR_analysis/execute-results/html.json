{
  "hash": "c56e70f9e978f9a6a56affc9b1670edc",
  "result": {
    "markdown": "---\ntitle: \"Textmining on SDG VNRs of ROK, Germany, Finland\"\nformat: \n    html:\n        code-fold: true\nauthor: \"Sangwon Ju, SNU GSPA\" \nknitr:\n  opts_knit:\n    verbose: true\ndate: 2022/08/06\n---\n\n\n1.  기술통계 분석\n\n-   TF-IDF (Term Frequency - Inverse Document Frequency): 단어 빈도 - 역 문서 빈도 총 문서 수 대비 적게 등장한 단어가 중요한 단어 특정 문서 안에서 많이 등장한다고 해도 중요도가 올라가지는 않음\n\n-   TF: 특정 문서 D에서 특정 단어 T의 등장 횟수\n\n-   DF: 특정 단어 T가 등장한 문서의 수\n\n-   IDF: DF에 반비례 하는 수. ln(총문서수/(DF))\n\n-   TF-IDF: TF \\* IDF 희귀하면서도 특정 텍스트에서 자주 사용된 단어 (TF)는 그 텍스트에서 중요함\n\n2.  단어간 혹은 기사간 상관관계\n\n-   동시 출현(Co-Occurrence) 단어 분석 문장 혹은 기사에 함께 사용된 단어는 어떤 단어일지 분석하는 것. 단어의 \"맥락\"을 파악하기 위하여 어떤 단어들이 함께 쓰였는지를 알아야 함. 의미를 가진 단어(명사, 동사, 형용사)등을 추출하여 어떤 단어들이 함께 빈번하게 쓰였는지 분석해보는게 필요함.\n\n3.  연관 규칙 분석 (Association Rules)\n\n-   장바구니 분석 Apriori Algorithm(Agrawal et al., 1993): 어떤 단어가 다른 단어들과의 연관규칙을 가지는지를 추출 하는 방식\n\n4.  **토픽 모델링**\n\n-   토픽 모델링은 문서와 단어로 구성된 행렬(DTM)을 기반으로 문서에 잠재되어 (Latent)있다고 가정된 토픽의 등장확률을 추정하는 일련의 통계적 텍스트 처리기법을 일컫는다. (Blei, 2014; Blei and Lafferty, 2007;Blei, Ng and Jordan, 2003) DTM을 활용하여 주제-확률 분포, 단어-확률 분포를 구한뒤 잠재 주제를 찾는 LDA나 Singular Value Decomposition을 통해 차원 축소를 하는 방법이 있다.\n\n-   **LDA(Latent Dirichlet Allocation):**\n\n    **이 문서에서는 어떤 주제들이 오가고 있을까?**\n\n    PLSA를 조건부 확률로 확장시킨 기법으로 잠재 주제의 확률적 분포에 대한 PLSA의 한계점을 보완한 모델이다. LDA모델은 무작위로 섞여있는 대량의 문서에서 단어들의 패턴을 추론하여 각 토픽의 특성을 도출하는데 용이하며, 텍스트 데이터의 의미구조를 파악하기에 적합한 방법 중 하나이다. 한 문서는 여러가지 토픽으로 이루어지고, 토픽은 여러 단어를 혼합하여 구성된다.\n\n-   1개의 토픽은 여러 단어(서로 다른 확률을 가진)로 구성.\n\n-   1개의 단어는 여러 토픽에서 서로 다른 확률을 가짐.\n\n-   delta는 문장이 각 토픽에 등장할 확률, beta는 단어가 각 토픽에 등장할 확률\n\n# Loading Package\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(\"reticulate\",\"tidyverse\",\"tidymodels\",\n               \"tidytext\",\"lda\",\"readr\",\"igraph\",\"forcats\",\n               \"sna\",\"ergm\",\"network\",\"stringr\",\"magrittr\",\"showtext\",\n               \"tokenizers\",\"janeaustenr\",\"forcats\",\"RColorBrewer\",\n               \"viridis\",\"stopwords\",\"purrr\",\"widyr\",\"textmineR\",\"stm\",\n               \"ggraph\",\"tidygraph\",\"ggfortify\",\"ggthemes\",\"cowplot\",\"fs\",\n               \"knitr\",\"kableExtra\",\"ggrepel\",\"grid\",\"gridExtra\",\"topicmodels\",\n               \"scales\")\n\n# Font Setting\nfont_add_google(name=\"Uchen\") #fonts.google.com에서 고르시면 됨\nshowtext_auto()\n```\n:::\n\n\n# Setting Working Directory\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_folder = \"c:/Users/jsw06/OneDrive - SNU/r/vnr\"\nger_path = fs::path(data_folder, \"germany.txt\")\nfin_path = fs::path(data_folder, \"finland.txt\")\nkor_path = fs::path(data_folder, \"korea.txt\")\n```\n:::\n\n\n표제어 추출을 통해 어근을 추출하고 토크나이징을 통해서 토픽 추출 용이하게 만들기.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentence_word1=function(x){\n  x=as.character(x)\n  a=tibble(words=tokenize_sentences(x) %>%\n             unlist() %>% tibble(words=.),\n    sentence=1:nrow(words))\n  return(a)}\n\nsentence_word2=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_words(stopwords =  as.vector(stop_words$word),\n                                strip_numeric = T) %>%  \n               unlist())\n  return(a)}\n\nsentence_word3=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_ngrams(.,n = 2, n_min = 2,\n               stopwords = as.vector(stop_words$word)) %>%\n               unlist())\n  return(a)}\n```\n:::\n\n\n# Preprocess: Korea VNR\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea <-  readLines(kor_path) %>%\n  str_flatten(collapse = \" \") %>%\n  str_replace_all(pattern=\"//\", \" // \") %>%\n  str_split(pattern=\"//\") %>%\n  unlist() %>%\n  str_squish() %>%\n  tibble(paragraph=as_factor(1:length(.)),\n         text=.) \n```\n:::\n\n\n## Tokenizing words\n\nBreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_tokenized <- korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_ngram  <-  korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_tokenized <- korea_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"government\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"sustain\")) %>% \n    filter(!str_detect(words,pattern=\"develop\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\"))\n\nkorea_ngram <- korea_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"government\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\n(korea_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(n, word)) +\n    geom_col(fill=\"red\") +\n    geom_text(aes(label=n),hjust=-0.1,size=5,family=\"Uchen\")+\n    labs(y = NULL, title=\"Korea\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=30)) -> a)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n\n```{.r .cell-code}\n(korea_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(n, word)) +\n    geom_col(fill=\"red\") +\n    geom_text(aes(label=n),hjust=-0.1,size=8,family=\"Uchen\")+\n    labs(y = NULL, title=\"Visualizing the frequency of n-grams\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=30)) -> a1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-8-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_tfidf <- korea_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nkorea_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:left;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> conflict </td>\n   <td style=\"text-align:left;\"> 52 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1379310 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.4480599 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> institutional </td>\n   <td style=\"text-align:left;\"> 78 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 2.437504 </td>\n   <td style=\"text-align:right;\"> 0.4062507 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> saemaul </td>\n   <td style=\"text-align:left;\"> 48 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1250000 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.4060543 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> undong </td>\n   <td style=\"text-align:left;\"> 48 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1250000 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.4060543 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:left;\"> 89 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1000000 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3941582 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> disability </td>\n   <td style=\"text-align:left;\"> 73 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 0.1285714 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.3889660 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> humanitarian </td>\n   <td style=\"text-align:left;\"> 49 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1555556 </td>\n   <td style=\"text-align:right;\"> 2.437504 </td>\n   <td style=\"text-align:right;\"> 0.3791674 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mechanism </td>\n   <td style=\"text-align:left;\"> 78 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 2.236834 </td>\n   <td style=\"text-align:right;\"> 0.3728056 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> south </td>\n   <td style=\"text-align:left;\"> 91 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0740741 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.3433133 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:left;\"> 87 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.0853659 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3364765 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> indicator </td>\n   <td style=\"text-align:left;\"> 15 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 0.1538462 </td>\n   <td style=\"text-align:right;\"> 2.069780 </td>\n   <td style=\"text-align:right;\"> 0.3184276 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> principle </td>\n   <td style=\"text-align:left;\"> 64 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 2.149822 </td>\n   <td style=\"text-align:right;\"> 0.3071175 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> correspond </td>\n   <td style=\"text-align:left;\"> 33 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0769231 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3031986 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> person </td>\n   <td style=\"text-align:left;\"> 73 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1000000 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.3025291 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> indicator </td>\n   <td style=\"text-align:left;\"> 94 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 0.1454545 </td>\n   <td style=\"text-align:right;\"> 2.069780 </td>\n   <td style=\"text-align:right;\"> 0.3010589 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> oda </td>\n   <td style=\"text-align:left;\"> 36 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.1764706 </td>\n   <td style=\"text-align:right;\"> 1.690290 </td>\n   <td style=\"text-align:right;\"> 0.2982865 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> administration </td>\n   <td style=\"text-align:left;\"> 89 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.0600000 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2780837 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> financial </td>\n   <td style=\"text-align:left;\"> 77 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 0.1025641 </td>\n   <td style=\"text-align:right;\"> 2.688819 </td>\n   <td style=\"text-align:right;\"> 0.2757763 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rate </td>\n   <td style=\"text-align:left;\"> 62 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0909091 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.2750265 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> investment </td>\n   <td style=\"text-align:left;\"> 90 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.0909091 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.2750265 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\nkorea_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> korea_dtm.old\n\nrowTotals <- apply(korea_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_dtm <- korea_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_split <- initial_split(korea_tfidf, prop = 0.8)\n\n# Train\nkorea_split_train <- training(korea_split)\nkorea_split_train  %>%\n    cast_dtm(paragraph, words, n) -> korea_train_dtm.old\nrowTotals <- apply(korea_train_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_train_dtm <- korea_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nkorea_split_test <- testing(korea_split)\nkorea_split_test  %>%\n    cast_dtm(paragraph, words, n) -> korea_valid_dtm.old\nrowTotals <- apply(korea_valid_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_valid_dtm <- korea_valid_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(korea_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = korea_valid_dtm)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...  2 \n...  3 \n...  4 \n...  5 \n...  6 \n...  7 \n...  8 \n...  9 \n...  10 \n...  11 \n...  12 \n...  13 \n...  14 \n...  15 \n...  16 \n...  17 \n...  18 \n...  19 \n...  20 \n```\n:::\n\n```{.r .cell-code}\nkorea_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nkorea_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_4 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) \n\nkorea_tp_4 %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\",ncol=3) +\n  scale_y_reordered() +\n  theme_bw()+ \n  theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_5 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta)  \n\nkorea_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_6 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n    \n\nkorea_tp_6 %>%\n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%     \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_7 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta)  \n\nkorea_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic)%>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_coocur <- korea_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `distinct_()` was deprecated in dplyr 0.7.0.\nPlease use `distinct()` instead.\nSee vignette('programming') for more help\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n```\n:::\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- korea_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=korea_tp_6,by=\"term\") %>%\n    select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nkorea_g <- as_tbl_graph(korea_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) +\n    scale_size(range = c(1,5)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Preprocess: Germany VNR\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany<-readLines(ger_path) %>%\n  str_flatten(collapse = \" \") %>%\n  str_replace_all(pattern=\"//\", \" // \") %>%\n  str_split(pattern=\"//\") %>%\n  unlist() %>%\n  str_squish() %>%\n  tibble(paragraph=as_factor(1:length(.)),\n         text=.) \n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_tokenized <- germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_ngram  <-  germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_tokenized <- germany_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"government\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"sustain\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"develop\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\"))\n\n\ngermany_ngram <- germany_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"government\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"sustain\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"develop\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\"))%>% \n    filter(!str_detect(words,pattern=\"billi*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\n(germany_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>% \n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(n, word)) +\n    geom_col(fill=\"darkgreen\") +\n    geom_text(aes(label=n),hjust=-0.1,size=5,family=\"Uchen\")+\n    labs(y = NULL, title=\"Germany\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=30)) -> b)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=960}\n:::\n\n```{.r .cell-code}\n(germany_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(n, word)) +\n    geom_col(fill=\"darkgreen\") +\n    geom_text(aes(label=n),hjust=-0.1,size=8,family=\"Uchen\")+\n    labs(y = NULL, title=\"Visualizing the frequency of n-grams\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=30)) -> b1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-25-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_tfidf <- germany_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \ngermany_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:left;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> fracking </td>\n   <td style=\"text-align:left;\"> 142 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.6666667 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 3.961866 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hospital </td>\n   <td style=\"text-align:left;\"> 82 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.980933 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wood </td>\n   <td style=\"text-align:left;\"> 319 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 5.249652 </td>\n   <td style=\"text-align:right;\"> 1.749884 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> administration </td>\n   <td style=\"text-align:left;\"> 289 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 4.333361 </td>\n   <td style=\"text-align:right;\"> 1.733345 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> import </td>\n   <td style=\"text-align:left;\"> 364 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 4.844187 </td>\n   <td style=\"text-align:right;\"> 1.614729 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> civil </td>\n   <td style=\"text-align:left;\"> 353 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 3.109586 </td>\n   <td style=\"text-align:right;\"> 1.554793 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> urban </td>\n   <td style=\"text-align:left;\"> 253 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.5000000 </td>\n   <td style=\"text-align:right;\"> 3.052428 </td>\n   <td style=\"text-align:right;\"> 1.526214 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> norway </td>\n   <td style=\"text-align:left;\"> 328 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2857143 </td>\n   <td style=\"text-align:right;\"> 5.249652 </td>\n   <td style=\"text-align:right;\"> 1.499901 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> job </td>\n   <td style=\"text-align:left;\"> 197 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.745575 </td>\n   <td style=\"text-align:right;\"> 1.498230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> farm </td>\n   <td style=\"text-align:left;\"> 59 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.544904 </td>\n   <td style=\"text-align:right;\"> 1.417962 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> criminal </td>\n   <td style=\"text-align:left;\"> 340 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 4.151040 </td>\n   <td style=\"text-align:right;\"> 1.383680 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> family </td>\n   <td style=\"text-align:left;\"> 54 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 3.996889 </td>\n   <td style=\"text-align:right;\"> 1.332296 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> migration </td>\n   <td style=\"text-align:left;\"> 236 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2727273 </td>\n   <td style=\"text-align:right;\"> 4.844187 </td>\n   <td style=\"text-align:right;\"> 1.321142 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> profit </td>\n   <td style=\"text-align:left;\"> 366 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.320622 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> disability </td>\n   <td style=\"text-align:left;\"> 234 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3000000 </td>\n   <td style=\"text-align:right;\"> 4.333361 </td>\n   <td style=\"text-align:right;\"> 1.300008 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> labour </td>\n   <td style=\"text-align:left;\"> 195 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.4000000 </td>\n   <td style=\"text-align:right;\"> 3.170211 </td>\n   <td style=\"text-align:right;\"> 1.268084 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> gas </td>\n   <td style=\"text-align:left;\"> 285 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3750000 </td>\n   <td style=\"text-align:right;\"> 3.377850 </td>\n   <td style=\"text-align:right;\"> 1.266694 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> monitor </td>\n   <td style=\"text-align:left;\"> 139 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3000000 </td>\n   <td style=\"text-align:right;\"> 4.151040 </td>\n   <td style=\"text-align:right;\"> 1.245312 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mint </td>\n   <td style=\"text-align:left;\"> 105 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2000000 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.188560 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> arctic </td>\n   <td style=\"text-align:left;\"> 306 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2000000 </td>\n   <td style=\"text-align:right;\"> 5.942799 </td>\n   <td style=\"text-align:right;\"> 1.188560 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\ngermany_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> germany_dtm.old\n\nrowTotals <- apply(germany_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_dtm <- germany_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_split <- initial_split(germany_tfidf, prop = 0.8)\n\n# Train\ngermany_split_train <- training(germany_split)\ngermany_split_train  %>%\n    cast_dtm(paragraph, words, n) -> germany_train_dtm.old\n\nrowTotals <- apply(germany_train_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_train_dtm <- germany_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\ngermany_split_test <- testing(germany_split)\ngermany_split_test  %>%\n    cast_dtm(paragraph, words, n) -> germany_test_dtm.old\n\nrowTotals <- apply(germany_test_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_test_dtm <- germany_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(germany_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = germany_test_dtm)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n...  2 \n...  3 \n...  4 \n...  5 \n...  6 \n...  7 \n...  8 \n...  9 \n...  10 \n...  11 \n...  12 \n...  13 \n...  14 \n...  15 \n...  16 \n...  17 \n...  18 \n...  19 \n...  20 \n```\n:::\n\n```{.r .cell-code}\ngermany_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\ngermany_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_4 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) \n\ngermany_tp_4 %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\",ncol=3) +\n  scale_y_reordered() +\n  theme_bw()+ \n  theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_5 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta)  \n\ngermany_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_6 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n    \n\ngermany_tp_6 %>%\n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%     \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_lda <-  LDA(germany_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_7 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta)  \n\ngermany_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic)%>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngermany_coocur <- germany_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- germany_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=germany_tp_7,by=\"term\") %>%\n    select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\ngermany_g <- as_tbl_graph(germany_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) +\n    scale_size(range = c(1,5)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Preprocess: Finland VNR\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland <-  readLines(fin_path) %>%\n  str_flatten(collapse = \" \") %>%\n  str_replace_all(pattern=\"//\", \" // \") %>%\n  str_split(pattern=\"//\") %>%\n  unlist() %>%\n  str_squish() %>%\n  tibble(paragraph=as_factor(1:length(.)),\n         text=.) \n```\n:::\n\n\n## Tokenizing words\n\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_tokenized <- finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_ngram  <-  finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_tokenized <-finland_tokenized  %>%\n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"government\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"sustain\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"develop\\\\w*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\"))\n\nfinland_ngram <- finland_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"government\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"sustain\\\\w*\")) %>% \n    filter(!str_detect(words,pattern=\"develop\\\\w*\"))%>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\"))\n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\n(finland_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(n, word)) +\n    geom_col(fill=\"orange\") +\n    geom_text(aes(label=n),hjust=-0.1,size=5,family=\"Uchen\")+\n    labs(y = NULL, title=\"Finland\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=30)) -> c)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-42-1.png){fig-align='center' width=960}\n:::\n\n```{.r .cell-code}\n(finland_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(n, word)) +\n    geom_col(fill=\"orange\") +\n    geom_text(aes(label=n),hjust=-0.1,size=8,family=\"Uchen\")+\n    labs(y = NULL, title=\"Visualizing the frequency of n-grams\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=30)) -> c1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-42-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_tfidf <- finland_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nfinland_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:left;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> city </td>\n   <td style=\"text-align:left;\"> 97 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3750000 </td>\n   <td style=\"text-align:right;\"> 3.963009 </td>\n   <td style=\"text-align:right;\"> 1.4861282 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> species </td>\n   <td style=\"text-align:left;\"> 549 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.2777778 </td>\n   <td style=\"text-align:right;\"> 4.330733 </td>\n   <td style=\"text-align:right;\"> 1.2029815 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> audit </td>\n   <td style=\"text-align:left;\"> 330 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.2857143 </td>\n   <td style=\"text-align:right;\"> 4.043051 </td>\n   <td style=\"text-align:right;\"> 1.1551575 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> preparation </td>\n   <td style=\"text-align:left;\"> 11 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 3.392464 </td>\n   <td style=\"text-align:right;\"> 1.1308212 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> loss </td>\n   <td style=\"text-align:left;\"> 564 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 4.448516 </td>\n   <td style=\"text-align:right;\"> 0.9885592 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> version </td>\n   <td style=\"text-align:left;\"> 39 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1818182 </td>\n   <td style=\"text-align:right;\"> 5.429346 </td>\n   <td style=\"text-align:right;\"> 0.9871538 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ministry </td>\n   <td style=\"text-align:left;\"> 123 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.3333333 </td>\n   <td style=\"text-align:right;\"> 2.814386 </td>\n   <td style=\"text-align:right;\"> 0.9381286 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attitude </td>\n   <td style=\"text-align:left;\"> 245 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1818182 </td>\n   <td style=\"text-align:right;\"> 5.141664 </td>\n   <td style=\"text-align:right;\"> 0.9348479 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> film </td>\n   <td style=\"text-align:left;\"> 114 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 6.527958 </td>\n   <td style=\"text-align:right;\"> 0.9325654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:left;\"> 646 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.2500000 </td>\n   <td style=\"text-align:right;\"> 3.694745 </td>\n   <td style=\"text-align:right;\"> 0.9236861 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> annually </td>\n   <td style=\"text-align:left;\"> 657 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 0.2127660 </td>\n   <td style=\"text-align:right;\"> 4.330733 </td>\n   <td style=\"text-align:right;\"> 0.9214326 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> minority </td>\n   <td style=\"text-align:left;\"> 244 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2142857 </td>\n   <td style=\"text-align:right;\"> 4.225373 </td>\n   <td style=\"text-align:right;\"> 0.9054370 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> fish </td>\n   <td style=\"text-align:left;\"> 553 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.2142857 </td>\n   <td style=\"text-align:right;\"> 4.130063 </td>\n   <td style=\"text-align:right;\"> 0.8850134 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> fishery </td>\n   <td style=\"text-align:left;\"> 204 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1333333 </td>\n   <td style=\"text-align:right;\"> 6.527958 </td>\n   <td style=\"text-align:right;\"> 0.8703944 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> backlog </td>\n   <td style=\"text-align:left;\"> 479 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1333333 </td>\n   <td style=\"text-align:right;\"> 6.527958 </td>\n   <td style=\"text-align:right;\"> 0.8703944 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> nordic </td>\n   <td style=\"text-align:left;\"> 213 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.2187500 </td>\n   <td style=\"text-align:right;\"> 3.963009 </td>\n   <td style=\"text-align:right;\"> 0.8669081 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> plastic </td>\n   <td style=\"text-align:left;\"> 514 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 5.141664 </td>\n   <td style=\"text-align:right;\"> 0.8569439 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> waste </td>\n   <td style=\"text-align:left;\"> 517 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.2222222 </td>\n   <td style=\"text-align:right;\"> 3.819908 </td>\n   <td style=\"text-align:right;\"> 0.8488684 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> alien </td>\n   <td style=\"text-align:left;\"> 568 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 5.834811 </td>\n   <td style=\"text-align:right;\"> 0.8335444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> invasive </td>\n   <td style=\"text-align:left;\"> 568 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 5.834811 </td>\n   <td style=\"text-align:right;\"> 0.8335444 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\n\nfinland_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> finland_dtm.old\n\nrowTotals <- apply(finland_dtm.old, 1, sum) # Find the sum of words in each Document\nfinland_dtm <- finland_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n### Cast LDA (k=4)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_4 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) \n\nfinland_tp_4 %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\",ncol=3) +\n  scale_y_reordered() +\n  theme_bw()+ \n  theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-45-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=5)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_5 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta)  \n\nfinland_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-46-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_6 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n    \n\nfinland_tp_6 %>%\n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%     \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-47-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=7)\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_lda <-  LDA(finland_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_7 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta)  \n\nfinland_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic)%>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=20)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-48-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinland_coocur <- finland_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=finland_tp_7,by=\"term\") %>%\n    select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nfinland_g <- as_tbl_graph(finland_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk',maxiter=200 * vcount(.)) +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-52-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) +\n    scale_size(range = c(1,5)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-53-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### simplify?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks=finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") %>%\n    left_join(finland_tp_7 %>% select(-beta,term) %>% \n                  transmute(source=term,topic1=topic),by=\"source\") %>% \n    left_join(finland_tp_7 %>% select(-beta,term) %>% \n                  transmute(target=term,topic2=topic),by=\"target\") %>% \n    select(topic1,topic2,weight) %>% \n    rename(source=\"topic1\",target=\"topic2\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nfinland_network <- graph_from_data_frame(d = links, directed = F)\nsimplify(finland_network)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIGRAPH 051a69e UNW- 7 19 -- \n+ attr: name (v/c), weight (e/n)\n+ edges from 051a69e (vertex names):\n [1] 4--3 4--6 4--2 4--7 4--5 4--1 3--6 3--7 3--5 3--1 6--2 6--7 6--5 6--1 2--7\n[16] 2--1 7--5 7--1 5--1\n```\n:::\n\n```{.r .cell-code}\nfinland_g <- as_tbl_graph(simplify(finland_network))\n\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree()) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight), show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(size = degree, color=\"blue\")) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Uchen\") +\n    theme_graph(base_family=\"Uchen\") +\n    theme(text=element_text(family=\"Uchen\",size=20)) +\n    guides(size=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n# Comparing three countries\n\n## Comparing Word Frequencies of Three Countries\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid.arrange(a,b,c, nrow=1,\n             top=textGrob(\"Comparing Word Frequencies of Three Countries\",\n                          gp = gpar(col = \"black\", \n                                    fontsize = 25,\n                                    fontfamily=\"Uchen\")))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-55-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Comparing bigrams of Three Countries\n\n\n::: {.cell .column-screen-inset-shaded layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid.arrange(a1,b1,c1, nrow=1,\n             top=textGrob(\"Comparing Bigrams of Three Countries\",\n                          gp = gpar(col = \"black\", \n                                    fontsize = 25,\n                                    fontfamily=\"Uchen\")))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-56-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Comparing Tf-idf of Three Countries\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_tokenized %>% \n    mutate(key=\"Korea\") %>% \n    full_join(germany_tokenized %>% \n                mutate(key=\"Germany\")) %>% \n    full_join(finland_tokenized %>% \n                mutate(key=\"Finland\")) %>% \n    mutate(key=as_factor(key)) -> total_tokenized\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = c(\"paragraph\", \"sentence\", \"words\", \"key\")\nJoining, by = c(\"paragraph\", \"sentence\", \"words\", \"key\")\n```\n:::\n\n```{.r .cell-code}\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    bind_tf_idf(words, key, n) %>% \n    group_by(key) %>% \n    top_n(tf_idf, n=30) %>%\n    arrange(desc(tf_idf)) %>% \n    mutate(words = reorder_within(words, tf_idf, key)) %>%\n    ggplot(aes(x=tf_idf, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 1, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing Tf-idf of Three Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Uchen\",size=25)) \n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-57-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Comparing the word frequencies\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_three <- total_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    select(-n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>%\n    pivot_longer(3:4,\n               names_to = \"key\", values_to = \"proportion\")\n```\n:::\n\n\nWords that are far from the line are words that are found more in one set of texts than another.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\n# expect a warning about rows with missing values being removed\nset.seed(2022)\ncompare_three %>%\n    ggplot(aes(x = proportion, y = Korea, color = abs(Korea - proportion))) +\n    geom_abline(color = \"darkgreen\", lty = 2, size=1.5) +\n    geom_jitter(alpha = 0.1, size = 1.5, width = 0.3, height = 0.3) +\n    geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5, family=\"Uchen\") +\n    scale_x_log10(labels = percent_format()) +\n    scale_y_log10(labels = percent_format()) +\n    scale_color_gradient(limits = c(0.001, 0.01), \n                         low = \"darkslategray4\", high =  \"black\") +\n    facet_wrap(~key, ncol = 2) +\n    theme(legend.position=\"none\") +\n    labs(y = \"Korea\", x = NULL, title=\"Comparing the word frequencies of Three Countries\") +\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Uchen\",size=25))+\n    guides(color=\"none\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-59-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Correlation of words\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_germany=cor.test(data = compare_three[compare_three$key == \"Germany\",],\n         ~ proportion + Korea) %>% tidy() %>% mutate(country=\"Germany\",.before = estimate)\ncor_finland=cor.test(data = compare_three[compare_three$key == \"Finland\",],\n         ~ proportion + Korea) %>% tidy() %>% mutate(country=\"Finland\",.before = estimate)\ncor_germany %>% \n    full_join(cor_finland) %>% \n    mutate(across(where(is.numeric), round, 3))%>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = c(\"country\", \"estimate\", \"statistic\", \"p.value\", \"parameter\",\n\"conf.low\", \"conf.high\", \"method\", \"alternative\")\n```\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> country </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n   <th style=\"text-align:right;\"> parameter </th>\n   <th style=\"text-align:right;\"> conf.low </th>\n   <th style=\"text-align:right;\"> conf.high </th>\n   <th style=\"text-align:left;\"> method </th>\n   <th style=\"text-align:left;\"> alternative </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Germany </td>\n   <td style=\"text-align:right;\"> 0.552 </td>\n   <td style=\"text-align:right;\"> 11.462 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 299 </td>\n   <td style=\"text-align:right;\"> 0.469 </td>\n   <td style=\"text-align:right;\"> 0.626 </td>\n   <td style=\"text-align:left;\"> Pearson's product-moment correlation </td>\n   <td style=\"text-align:left;\"> two.sided </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Finland </td>\n   <td style=\"text-align:right;\"> 0.688 </td>\n   <td style=\"text-align:right;\"> 28.668 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 912 </td>\n   <td style=\"text-align:right;\"> 0.653 </td>\n   <td style=\"text-align:right;\"> 0.721 </td>\n   <td style=\"text-align:left;\"> Pearson's product-moment correlation </td>\n   <td style=\"text-align:left;\"> two.sided </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "Vnr_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}