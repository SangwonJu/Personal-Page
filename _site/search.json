[
  {
    "objectID": "data_analytics/Vnr_analysis.html",
    "href": "data_analytics/Vnr_analysis.html",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "",
    "text": "기술통계 분석"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#preprocess",
    "href": "data_analytics/Vnr_analysis.html#preprocess",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\nkorea <-  readLines(kor_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tokenizing-words",
    "href": "data_analytics/Vnr_analysis.html#tokenizing-words",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nBreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\nkorea_tokenized <- korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#n-grams",
    "href": "data_analytics/Vnr_analysis.html#n-grams",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\nkorea_ngram  <-  korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis",
    "href": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\nkorea_tokenized <- korea_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n    \n\nkorea_ngram <- korea_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#frequency-table",
    "href": "data_analytics/Vnr_analysis.html#frequency-table",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(korea_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"대한민국\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> a)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(korea_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"대한민국\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> a1) \n\n\nSelecting by n"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph",
    "href": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\nkorea_tfidf <- korea_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nkorea_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    conflict \n    52 \n    4 \n    0.1379310 \n    3.248435 \n    0.4480599 \n  \n  \n    governmental \n    10 \n    2 \n    0.1538462 \n    2.842970 \n    0.4373799 \n  \n  \n    institutional \n    78 \n    2 \n    0.1666667 \n    2.437504 \n    0.4062507 \n  \n  \n    saemaul \n    48 \n    5 \n    0.1219512 \n    3.248435 \n    0.3961506 \n  \n  \n    undong \n    48 \n    5 \n    0.1219512 \n    3.248435 \n    0.3961506 \n  \n  \n    disability \n    73 \n    9 \n    0.1304348 \n    3.025291 \n    0.3946032 \n  \n  \n    humanitarian \n    49 \n    7 \n    0.1555556 \n    2.437504 \n    0.3791674 \n  \n  \n    tax \n    89 \n    5 \n    0.0961538 \n    3.941582 \n    0.3789983 \n  \n  \n    mechanism \n    78 \n    2 \n    0.1666667 \n    2.236834 \n    0.3728056 \n  \n  \n    tax \n    87 \n    7 \n    0.0823529 \n    3.941582 \n    0.3246009 \n  \n  \n    indicator \n    15 \n    12 \n    0.1500000 \n    2.069780 \n    0.3104669 \n  \n  \n    person \n    73 \n    7 \n    0.1014493 \n    3.025291 \n    0.3069136 \n  \n  \n    south \n    91 \n    2 \n    0.0645161 \n    4.634729 \n    0.2990148 \n  \n  \n    indicator \n    94 \n    8 \n    0.1428571 \n    2.069780 \n    0.2956828 \n  \n  \n    oda \n    36 \n    6 \n    0.1666667 \n    1.690290 \n    0.2817150 \n  \n  \n    principle \n    64 \n    3 \n    0.1304348 \n    2.149822 \n    0.2804116 \n  \n  \n    correspond \n    33 \n    2 \n    0.0689655 \n    3.941582 \n    0.2718332 \n  \n  \n    discrimination \n    73 \n    4 \n    0.0579710 \n    4.634729 \n    0.2686799 \n  \n  \n    administration \n    89 \n    3 \n    0.0576923 \n    4.634729 \n    0.2673882 \n  \n  \n    age \n    31 \n    3 \n    0.0937500 \n    2.842970 \n    0.2665284"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#topic-modeling",
    "href": "data_analytics/Vnr_analysis.html#topic-modeling",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\nkorea_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> korea_dtm.old\n\nrowTotals <- apply(korea_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_dtm <- korea_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\nkorea_split <- initial_split(korea_tfidf, prop = 0.8)\n\n# Train\nkorea_split_train <- training(korea_split)\nkorea_split_train  %>%\n    cast_dtm(paragraph, words, n) -> korea_train_dtm.old\nrowTotals <- apply(korea_train_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_train_dtm <- korea_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nkorea_split_test <- testing(korea_split)\nkorea_split_test  %>%\n    cast_dtm(paragraph, words, n) -> korea_valid_dtm.old\nrowTotals <- apply(korea_valid_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_valid_dtm <- korea_valid_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(korea_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = korea_valid_dtm)\n}\n\nkorea_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nkorea_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=4)\n\n\nCode\nkorea_lda <-  LDA(korea_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_4 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=5)\n\n\nCode\nkorea_lda <-  LDA(korea_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_5 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\nkorea_lda <-  LDA(korea_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_6 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=7)\n\n\nCode\nkorea_lda <-  LDA(korea_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_7 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nkorea_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#network-graph",
    "href": "data_analytics/Vnr_analysis.html#network-graph",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\nkorea_coocur <- korea_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- korea_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\")\n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=korea_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\nkorea_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nkorea_g <- as_tbl_graph(korea_network)\n\n\n\n\nNetwork Graph\n\n\nCode\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph per topic\n\n\nCode\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#preprocess-1",
    "href": "data_analytics/Vnr_analysis.html#preprocess-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\ngermany<-readLines(ger_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tokenizing-words-1",
    "href": "data_analytics/Vnr_analysis.html#tokenizing-words-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\ngermany_tokenized <- germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#n-grams-1",
    "href": "data_analytics/Vnr_analysis.html#n-grams-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\ngermany_ngram  <-  germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-1",
    "href": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\ngermany_tokenized <- germany_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\n\ngermany_ngram <- germany_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#frequency-table-1",
    "href": "data_analytics/Vnr_analysis.html#frequency-table-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(germany_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#FF0000',\n    pattern_fill = '#FFCC00',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"독일\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> b)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(germany_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#FF0000',\n    pattern_fill = '#FFCC00',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"독일\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> b1) \n\n\nSelecting by n"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-1",
    "href": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\ngermany_tfidf <- germany_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \ngermany_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    fracking \n    141 \n    2 \n    0.6666667 \n    5.942799 \n    3.961866 \n  \n  \n    civil \n    352 \n    2 \n    0.6666667 \n    3.109586 \n    2.073057 \n  \n  \n    hospital \n    81 \n    2 \n    0.3333333 \n    5.942799 \n    1.980933 \n  \n  \n    wood \n    318 \n    2 \n    0.3333333 \n    5.249652 \n    1.749884 \n  \n  \n    administration \n    288 \n    2 \n    0.4000000 \n    4.333361 \n    1.733345 \n  \n  \n    import \n    363 \n    2 \n    0.3333333 \n    4.844187 \n    1.614729 \n  \n  \n    urban \n    252 \n    2 \n    0.5000000 \n    3.052428 \n    1.526214 \n  \n  \n    norway \n    327 \n    2 \n    0.2857143 \n    5.249652 \n    1.499901 \n  \n  \n    job \n    196 \n    2 \n    0.4000000 \n    3.745575 \n    1.498230 \n  \n  \n    adaptation \n    307 \n    2 \n    0.4000000 \n    3.640214 \n    1.456086 \n  \n  \n    farm \n    58 \n    6 \n    0.4000000 \n    3.544904 \n    1.417962 \n  \n  \n    criminal \n    339 \n    2 \n    0.3333333 \n    4.151040 \n    1.383680 \n  \n  \n    family \n    53 \n    2 \n    0.3333333 \n    3.996889 \n    1.332296 \n  \n  \n    migration \n    235 \n    3 \n    0.2727273 \n    4.844187 \n    1.321142 \n  \n  \n    mint \n    104 \n    2 \n    0.2222222 \n    5.942799 \n    1.320622 \n  \n  \n    arctic \n    305 \n    2 \n    0.2222222 \n    5.942799 \n    1.320622 \n  \n  \n    profit \n    365 \n    2 \n    0.2222222 \n    5.942799 \n    1.320622 \n  \n  \n    disability \n    233 \n    3 \n    0.3000000 \n    4.333361 \n    1.300008 \n  \n  \n    labour \n    194 \n    2 \n    0.4000000 \n    3.170211 \n    1.268084 \n  \n  \n    south \n    369 \n    4 \n    0.4000000 \n    3.170211 \n    1.268084"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#topic-modeling-1",
    "href": "data_analytics/Vnr_analysis.html#topic-modeling-1",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\ngermany_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> germany_dtm.old\n\nrowTotals <- apply(germany_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_dtm <- germany_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\ngermany_split <- initial_split(germany_tfidf, prop = 0.8)\n\n# Train\ngermany_split_train <- training(germany_split)\ngermany_split_train  %>%\n    cast_dtm(paragraph, words, n) -> germany_train_dtm.old\n\nrowTotals <- apply(germany_train_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_train_dtm <- germany_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\ngermany_split_test <- testing(germany_split)\ngermany_split_test  %>%\n    cast_dtm(paragraph, words, n) -> germany_test_dtm.old\n\nrowTotals <- apply(germany_test_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_test_dtm <- germany_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(germany_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = germany_test_dtm)\n}\n\ngermany_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\ngermany_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=4)\n\n\nCode\ngermany_lda <-  LDA(germany_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_4 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=5)\n\n\nCode\ngermany_lda <-  LDA(germany_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_5 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\ngermany_lda <-  LDA(germany_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_6 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=7)\n\n\nCode\ngermany_lda <-  LDA(germany_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_7 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\ngermany_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#network-graph-2",
    "href": "data_analytics/Vnr_analysis.html#network-graph-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\ngermany_coocur <- germany_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- germany_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=germany_tp_7,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\ngermany_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\ngermany_g <- as_tbl_graph(germany_network)\n\n\n\n\nNetwork Graph\n\n\nCode\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph per topic\n\n\nCode\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#preprocess-2",
    "href": "data_analytics/Vnr_analysis.html#preprocess-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\nfinland <-  readLines(fin_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tokenizing-words-2",
    "href": "data_analytics/Vnr_analysis.html#tokenizing-words-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\nfinland_tokenized <- finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#n-grams-2",
    "href": "data_analytics/Vnr_analysis.html#n-grams-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\nfinland_ngram  <-  finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-2",
    "href": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\nfinland_tokenized <-finland_tokenized  %>%\n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"billion\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\"))  %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\nfinland_ngram <- finland_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"billion\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#frequency-table-2",
    "href": "data_analytics/Vnr_analysis.html#frequency-table-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(finland_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#003580',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"핀란드\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> c)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(germany_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#003580',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"핀란드\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> c1) \n\n\nSelecting by n"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-2",
    "href": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\nfinland_tfidf <- finland_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nfinland_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    species \n    549 \n    5 \n    0.2941176 \n    4.330733 \n    1.2737451 \n  \n  \n    preparation \n    11 \n    3 \n    0.3750000 \n    3.392464 \n    1.2721739 \n  \n  \n    city \n    97 \n    3 \n    0.3000000 \n    3.963009 \n    1.1889026 \n  \n  \n    loss \n    564 \n    2 \n    0.2222222 \n    4.448516 \n    0.9885592 \n  \n  \n    version \n    39 \n    2 \n    0.1818182 \n    5.429346 \n    0.9871538 \n  \n  \n    audit \n    330 \n    4 \n    0.2352941 \n    4.043051 \n    0.9513062 \n  \n  \n    ministry \n    123 \n    2 \n    0.3333333 \n    2.814386 \n    0.9381286 \n  \n  \n    attitude \n    245 \n    4 \n    0.1818182 \n    5.141664 \n    0.9348479 \n  \n  \n    film \n    114 \n    2 \n    0.1428571 \n    6.527958 \n    0.9325654 \n  \n  \n    fishery \n    204 \n    2 \n    0.1428571 \n    6.527958 \n    0.9325654 \n  \n  \n    annually \n    657 \n    10 \n    0.2127660 \n    4.330733 \n    0.9214326 \n  \n  \n    minority \n    244 \n    3 \n    0.2142857 \n    4.225373 \n    0.9054370 \n  \n  \n    plastic \n    514 \n    2 \n    0.1666667 \n    5.141664 \n    0.8569439 \n  \n  \n    waste \n    517 \n    2 \n    0.2222222 \n    3.819908 \n    0.8488684 \n  \n  \n    tax \n    646 \n    5 \n    0.2272727 \n    3.694745 \n    0.8397147 \n  \n  \n    alien \n    568 \n    2 \n    0.1428571 \n    5.834811 \n    0.8335444 \n  \n  \n    invasive \n    568 \n    2 \n    0.1428571 \n    5.834811 \n    0.8335444 \n  \n  \n    fish \n    553 \n    3 \n    0.2000000 \n    4.130063 \n    0.8260125 \n  \n  \n    backlog \n    479 \n    2 \n    0.1250000 \n    6.527958 \n    0.8159947 \n  \n  \n    nordic \n    213 \n    7 \n    0.2058824 \n    3.963009 \n    0.8159135"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#topic-modeling-2",
    "href": "data_analytics/Vnr_analysis.html#topic-modeling-2",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\nfinland_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> finland_dtm.old\n\nrowTotals <- apply(finland_dtm.old, 1, sum) # Find the sum of words in each Document\nfinland_dtm <- finland_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCast LDA (k=4)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_4 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=5)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_5 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_6 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=7)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_7 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=8)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 8,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_8 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_8 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=9)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 9,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_9 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_9 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=10)\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 10,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_10 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nfinland_tp_10 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#network-graph-4",
    "href": "data_analytics/Vnr_analysis.html#network-graph-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\nfinland_coocur <- finland_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=finland_tp_7,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\nfinland_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nfinland_g <- as_tbl_graph(finland_network)\n\n\n\n\nNetwork Graph\n\n\nCode\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph per topic\n\n\nCode\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n\n\nWarning: ggrepel: 4 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\nWarning: ggrepel: 1 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#preprocess-3",
    "href": "data_analytics/Vnr_analysis.html#preprocess-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\njapan<-readLines(jp_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tokenizing-words-3",
    "href": "data_analytics/Vnr_analysis.html#tokenizing-words-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\njapan_tokenized <- japan %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#n-grams-3",
    "href": "data_analytics/Vnr_analysis.html#n-grams-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\njapan_ngram  <-  japan %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-3",
    "href": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\njapan_tokenized <- japan_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"jap*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"asia[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\n\njapan_ngram <- japan_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"jap*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"asia[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n     filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#frequency-table-3",
    "href": "data_analytics/Vnr_analysis.html#frequency-table-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(japan_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#BC002D',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"일본\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> d)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(japan_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#BC002D',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"일본\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> d1) \n\n\nSelecting by n"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-3",
    "href": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\njapan_tfidf <- japan_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \njapan_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    smoke \n    298 \n    5 \n    0.2083333 \n    6.003887 \n    1.2508098 \n  \n  \n    mekong \n    271 \n    4 \n    0.1428571 \n    6.003887 \n    0.8576982 \n  \n  \n    forest \n    347 \n    10 \n    0.2083333 \n    3.806662 \n    0.7930547 \n  \n  \n    expectancy \n    128 \n    5 \n    0.1351351 \n    5.310740 \n    0.7176676 \n  \n  \n    forest \n    348 \n    11 \n    0.1803279 \n    3.806662 \n    0.6864473 \n  \n  \n    suicide \n    299 \n    13 \n    0.1494253 \n    4.394449 \n    0.6566418 \n  \n  \n    household \n    284 \n    6 \n    0.1935484 \n    3.364830 \n    0.6512574 \n  \n  \n    reliance \n    286 \n    2 \n    0.1176471 \n    5.310740 \n    0.6247929 \n  \n  \n    litter \n    232 \n    5 \n    0.1515152 \n    4.057977 \n    0.6148450 \n  \n  \n    traffic \n    176 \n    7 \n    0.1129032 \n    5.310740 \n    0.5995997 \n  \n  \n    city \n    364 \n    3 \n    0.2307692 \n    2.477527 \n    0.5717369 \n  \n  \n    food \n    335 \n    4 \n    0.2105263 \n    2.708050 \n    0.5701158 \n  \n  \n    relation \n    21 \n    3 \n    0.1578947 \n    3.605992 \n    0.5693671 \n  \n  \n    forest \n    219 \n    7 \n    0.1458333 \n    3.806662 \n    0.5551383 \n  \n  \n    investment \n    358 \n    4 \n    0.1904762 \n    2.912845 \n    0.5548275 \n  \n  \n    railway \n    177 \n    3 \n    0.0909091 \n    6.003887 \n    0.5458079 \n  \n  \n    investor \n    359 \n    2 \n    0.1428571 \n    3.806662 \n    0.5438089 \n  \n  \n    accident \n    239 \n    5 \n    0.1086957 \n    4.905275 \n    0.5331820 \n  \n  \n    approximately \n    287 \n    3 \n    0.1578947 \n    3.364830 \n    0.5312889 \n  \n  \n    icon \n    361 \n    2 \n    0.0869565 \n    6.003887 \n    0.5220771"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#topic-modeling-3",
    "href": "data_analytics/Vnr_analysis.html#topic-modeling-3",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\njapan_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> japan_dtm.old\n\nrowTotals <- apply(japan_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_dtm <- japan_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\njapan_split <- initial_split(japan_tfidf, prop = 0.8)\n\n# Train\njapan_split_train <- training(japan_split)\njapan_split_train  %>%\n    cast_dtm(paragraph, words, n) -> japan_train_dtm.old\n\nrowTotals <- apply(japan_train_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_train_dtm <- japan_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\njapan_split_test <- testing(japan_split)\njapan_split_test  %>%\n    cast_dtm(paragraph, words, n) -> japan_test_dtm.old\n\nrowTotals <- apply(japan_test_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_test_dtm <- japan_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(japan_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = japan_test_dtm)\n}\n\njapan_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\njapan_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=4)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_4 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=5)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_5 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_6 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=7)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_7 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=8)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 8,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_8 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_8 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=9)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 9,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_9 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_9 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=10)\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 10,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_10 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\njapan_tp_10 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=5) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#network-graph-6",
    "href": "data_analytics/Vnr_analysis.html#network-graph-6",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\njapan_coocur <- japan_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- japan_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=japan_tp_5,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\njapan_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\njapan_g <- as_tbl_graph(japan_network)\n\n\n\n\nNetwork Graph\n\n\nCode\njapan_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph per topic\n\n\nCode\njapan_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#preprocess-4",
    "href": "data_analytics/Vnr_analysis.html#preprocess-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\nuk <-readLines(uk_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tokenizing-words-4",
    "href": "data_analytics/Vnr_analysis.html#tokenizing-words-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\nuk_tokenized <- uk %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#n-grams-4",
    "href": "data_analytics/Vnr_analysis.html#n-grams-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\nuk_ngram  <-  uk %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-4",
    "href": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\nuk_tokenized <- uk_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"uk[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"wale*\")) %>% \n    filter(!str_detect(words,pattern=\"scot*\")) %>% \n    filter(!str_detect(words,pattern=\"eng*\")) %>% \n    filter(!str_detect(words,pattern=\"irel*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"wt*\")) %>% \n    filter(!str_detect(words,pattern=\"rbmps*\")) %>% \n    filter(!str_detect(words,pattern=\"inn*\")) %>%\n    filter(!str_detect(words,pattern=\"whilst*\")) %>% \n    filter(!str_detect(words,pattern=\"npt*\")) %>%\n    filter(!str_detect(words,pattern=\"britain*\")) \n\nuk_ngram <- uk_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"uk[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"wale*\")) %>% \n    filter(!str_detect(words,pattern=\"scot*\")) %>% \n    filter(!str_detect(words,pattern=\"eng*\")) %>% \n    filter(!str_detect(words,pattern=\"irel*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"wt*\")) %>% \n    filter(!str_detect(words,pattern=\"rbmps*\")) %>% \n    filter(!str_detect(words,pattern=\"inn*\")) %>%\n    filter(!str_detect(words,pattern=\"whilst*\")) %>% \n    filter(!str_detect(words,pattern=\"npt*\")) %>%\n    filter(!str_detect(words,pattern=\"britain*\"))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#frequency-table-4",
    "href": "data_analytics/Vnr_analysis.html#frequency-table-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(uk_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00247D',\n    pattern_fill = '#CF142B',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"영국\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> e)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(uk_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00247D',\n    pattern_fill = '#CF142B',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"독일\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> e1) \n\n\nSelecting by n"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-4",
    "href": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\nuk_tfidf <- uk_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nuk_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    bus \n    705 \n    3 \n    0.7500000 \n    6.329721 \n    4.747291 \n  \n  \n    nihr \n    196 \n    2 \n    0.6666667 \n    7.022868 \n    4.681912 \n  \n  \n    daily \n    709 \n    2 \n    0.6666667 \n    5.636574 \n    3.757716 \n  \n  \n    abortion \n    201 \n    2 \n    0.6666667 \n    5.231109 \n    3.487406 \n  \n  \n    nhs \n    210 \n    2 \n    0.6666667 \n    4.943427 \n    3.295618 \n  \n  \n    hour \n    533 \n    2 \n    0.6666667 \n    4.624973 \n    3.083315 \n  \n  \n    contact \n    199 \n    2 \n    0.5000000 \n    5.636574 \n    2.818287 \n  \n  \n    victim \n    997 \n    8 \n    0.6666667 \n    4.189655 \n    2.793103 \n  \n  \n    lgbt \n    652 \n    3 \n    0.6000000 \n    4.624973 \n    2.774984 \n  \n  \n    day \n    429 \n    2 \n    0.6666667 \n    4.132496 \n    2.754998 \n  \n  \n    birth \n    1002 \n    2 \n    0.5000000 \n    5.413430 \n    2.706715 \n  \n  \n    cardiovascular \n    208 \n    3 \n    0.3750000 \n    7.022868 \n    2.633576 \n  \n  \n    landfill \n    769 \n    2 \n    0.5000000 \n    5.231109 \n    2.615554 \n  \n  \n    conviction \n    334 \n    2 \n    0.4000000 \n    6.329721 \n    2.531888 \n  \n  \n    participatory \n    1014 \n    2 \n    0.4000000 \n    6.329721 \n    2.531888 \n  \n  \n    paramilitarism \n    1027 \n    2 \n    0.4000000 \n    6.329721 \n    2.531888 \n  \n  \n    road \n    207 \n    5 \n    0.5555556 \n    4.537961 \n    2.521090 \n  \n  \n    pupil \n    312 \n    3 \n    0.6000000 \n    4.189655 \n    2.513793 \n  \n  \n    mobility \n    670 \n    6 \n    0.4615385 \n    5.231109 \n    2.414358 \n  \n  \n    product \n    758 \n    2 \n    0.6666667 \n    3.557132 \n    2.371421"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#topic-modeling-4",
    "href": "data_analytics/Vnr_analysis.html#topic-modeling-4",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\nuk_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> uk_dtm.old\n\nrowTotals <- apply(uk_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_dtm <- uk_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\nuk_split <- initial_split(uk_tfidf, prop = 0.8)\n\n# Train\nuk_split_train <- training(uk_split)\nuk_split_train  %>%\n    cast_dtm(paragraph, words, n) -> uk_train_dtm.old\n\nrowTotals <- apply(uk_train_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_train_dtm <- uk_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nuk_split_test <- testing(uk_split)\nuk_split_test  %>%\n    cast_dtm(paragraph, words, n) -> uk_test_dtm.old\n\nrowTotals <- apply(uk_test_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_test_dtm <- uk_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(uk_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = uk_test_dtm)\n}\n\nuk_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nuk_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=4)\n\n\nCode\nuk_lda <-  LDA(uk_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_4 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=5)\n\n\nCode\nuk_lda <-  LDA(uk_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_5 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\nuk_lda <-  LDA(uk_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_6 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=7)\n\n\nCode\nuk_lda <-  LDA(uk_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_7 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\nuk_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#network-graph-8",
    "href": "data_analytics/Vnr_analysis.html#network-graph-8",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\nuk_coocur <- uk_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- uk_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=uk_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\nuk_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nuk_g <- as_tbl_graph(uk_network)\n\n\n\n\nNetwork Graph\n\n\nCode\nuk_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph per topic\n\n\nCode\nuk_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#preprocess-5",
    "href": "data_analytics/Vnr_analysis.html#preprocess-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\naustrailia<-readLines(astl_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tokenizing-words-5",
    "href": "data_analytics/Vnr_analysis.html#tokenizing-words-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\naustrailia_tokenized <- austrailia %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#n-grams-5",
    "href": "data_analytics/Vnr_analysis.html#n-grams-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\naustrailia_ngram  <-  austrailia %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-5",
    "href": "data_analytics/Vnr_analysis.html#remove-inadequate-words-for-analysis-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\naustrailia_tokenized <- austrailia_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"austra*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"torres*\"))%>% \n    filter(!str_detect(words,pattern=\"cent\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\naustrailia_ngram <- germany_ngram %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"austrail[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"torres*\"))%>% \n    filter(!str_detect(words,pattern=\"cent\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#frequency-table-5",
    "href": "data_analytics/Vnr_analysis.html#frequency-table-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(austrailia_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00008B',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"호주\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> f)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(austrailia_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00008B',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Gowun Dodum\")+\n    labs(y = NULL, title=\"호주\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Gowun Dodum\",size=25)) -> f1) \n\n\nSelecting by n"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-5",
    "href": "data_analytics/Vnr_analysis.html#tf-idf-per-paragraph-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\naustrailia_tfidf <- austrailia_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \naustrailia_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    broadband \n    206 \n    4 \n    0.3076923 \n    5.971262 \n    1.837311 \n  \n  \n    scarcity \n    150 \n    2 \n    0.4000000 \n    4.584968 \n    1.833987 \n  \n  \n    park \n    294 \n    2 \n    0.4000000 \n    4.361824 \n    1.744730 \n  \n  \n    fish \n    305 \n    7 \n    0.3888889 \n    4.361824 \n    1.696265 \n  \n  \n    child \n    64 \n    2 \n    0.5000000 \n    2.926739 \n    1.463370 \n  \n  \n    child \n    111 \n    3 \n    0.5000000 \n    2.926739 \n    1.463370 \n  \n  \n    carbon \n    283 \n    3 \n    0.3750000 \n    3.668677 \n    1.375754 \n  \n  \n    lgbtiqa \n    103 \n    2 \n    0.2222222 \n    5.971262 \n    1.326947 \n  \n  \n    discrimination \n    222 \n    2 \n    0.3333333 \n    3.774037 \n    1.258012 \n  \n  \n    world \n    293 \n    3 \n    0.6000000 \n    1.963929 \n    1.178357 \n  \n  \n    profit \n    350 \n    2 \n    0.2500000 \n    4.584968 \n    1.146242 \n  \n  \n    basin \n    163 \n    3 \n    0.2727273 \n    4.179502 \n    1.139864 \n  \n  \n    goal \n    7 \n    2 \n    0.3333333 \n    3.406312 \n    1.135438 \n  \n  \n    collaboration \n    219 \n    2 \n    0.3333333 \n    3.406312 \n    1.135438 \n  \n  \n    human \n    20 \n    5 \n    0.4545455 \n    2.474754 \n    1.124888 \n  \n  \n    rubbish \n    301 \n    3 \n    0.1875000 \n    5.971262 \n    1.119612 \n  \n  \n    park \n    303 \n    2 \n    0.2500000 \n    4.361824 \n    1.090456 \n  \n  \n    mhfa \n    102 \n    2 \n    0.1818182 \n    5.971262 \n    1.085684 \n  \n  \n    mikta \n    361 \n    4 \n    0.1818182 \n    5.971262 \n    1.085684 \n  \n  \n    wto \n    371 \n    4 \n    0.2352941 \n    4.584968 \n    1.078816"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#topic-modeling-5",
    "href": "data_analytics/Vnr_analysis.html#topic-modeling-5",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\naustrailia_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> austrailia_dtm.old\n\nrowTotals <- apply(austrailia_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_dtm <- austrailia_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\naustrailia_split <- initial_split(austrailia_tfidf, prop = 0.8)\n\n# Train\naustrailia_split_train <- training(austrailia_split)\naustrailia_split_train  %>%\n    cast_dtm(paragraph, words, n) -> austrailia_train_dtm.old\n\nrowTotals <- apply(austrailia_train_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_train_dtm <- austrailia_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\naustrailia_split_test <- testing(austrailia_split)\naustrailia_split_test  %>%\n    cast_dtm(paragraph, words, n) -> austrailia_test_dtm.old\n\nrowTotals <- apply(austrailia_test_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_test_dtm <- austrailia_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(austrailia_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = austrailia_test_dtm)}\n\naustrailia_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\naustrailia_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Gowun Dodum\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=4)\n\n\nCode\naustrailia_lda <-  LDA(austrailia_dtm, k = 4,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_4 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_4 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=5)\n\n\nCode\naustrailia_lda <-  LDA(austrailia_dtm, k = 5,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_5 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_5 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\naustrailia_lda <-  LDA(austrailia_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_6 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  \n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=7)\n\n\nCode\naustrailia_lda <-  LDA(austrailia_dtm, k = 7,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_7 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) \n\naustrailia_tp_7 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    arrange(topic) %>%\n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#network-graph-10",
    "href": "data_analytics/Vnr_analysis.html#network-graph-10",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\naustrailia_coocur <- austrailia_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- austrailia_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=austrailia_tp_7,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\naustrailia_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\naustrailia_g <- as_tbl_graph(austrailia_network)\n\n\n\n\nNetwork Graph\n\n\nCode\naustrailia_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\", size=40)) +\n    guides(size=\"none\")\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph per topic\n\n\nCode\naustrailia_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Gowun Dodum\") +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\")+\n    facet_nodes(~ community,ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nsimplify?\n\n\nCode\nlinks=finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") %>%\n    left_join(finland_tp_7 %>% dplyr::select(-beta,term) %>% \n                  transmute(source=term,topic1=topic),by=\"source\") %>% \n    left_join(finland_tp_7 %>% dplyr::select(-beta,term) %>% \n                  transmute(target=term,topic2=topic),by=\"target\") %>% \n    dplyr::select(topic1,topic2,weight) %>% \n    rename(source=\"topic1\",target=\"topic2\")\n\n\nSelecting by n\n\n\nCode\nfinland_network <- graph_from_data_frame(d = links, directed = F)\nfinland_g <- as_tbl_graph(simplify(finland_network))\n\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree()) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight), show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(size = degree, color=\"blue\")) +\n    geom_node_text(aes(label = name), repel = TRUE,family=\"Gowun Dodum\",size=10, max.overlaps = Inf) +\n    theme_graph(base_family=\"Gowun Dodum\") +\n    theme(text=element_text(family=\"Gowun Dodum\",size=40)) +\n    guides(size=\"none\",color=\"none\")"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#comparing-word-frequencies-of-six-countries",
    "href": "data_analytics/Vnr_analysis.html#comparing-word-frequencies-of-six-countries",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Comparing Word Frequencies of Six Countries",
    "text": "Comparing Word Frequencies of Six Countries\n\n\nCode\nkorea_tokenized %>% \n    mutate(key=\"대한민국\") %>% \n    full_join(germany_tokenized %>%\n                  mutate(key=\"독일\")) %>% \n    full_join(finland_tokenized %>%\n                  mutate(key=\"핀란드\")) %>% \n    full_join(japan_tokenized %>% \n                mutate(key=\"일본\")) %>%\n    full_join(uk_tokenized %>% \n                mutate(key=\"영국\")) %>%\n    full_join(austrailia_tokenized %>% \n                mutate(key=\"호주\")) %>%\n    mutate(key=as_factor(key)) -> total_tokenized\n\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    group_by(key) %>% \n    top_n(n, n=20) %>%\n    arrange(desc(n)) %>% \n    mutate(words = reorder_within(words, n, key)) %>%\n    ggplot(aes(x=n, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 2, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing Word Frequencies of Six Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))+\n    ggeasy::easy_center_title()"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#comparing-bigrams-of-six-countries",
    "href": "data_analytics/Vnr_analysis.html#comparing-bigrams-of-six-countries",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Comparing bigrams of Six Countries",
    "text": "Comparing bigrams of Six Countries\n\n\nCode\ngrid.arrange(a1,b1,c1,d1,e1,f1, nrow=2,\n             top=textGrob(\"Comparing Bigrams of Six Countries\",\n                          gp = gpar(col = \"black\", \n                                    fontsize = 40,\n                                    fontfamily=\"Gowun Dodum\")))"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#comparing-tf-idf-of-six-countries",
    "href": "data_analytics/Vnr_analysis.html#comparing-tf-idf-of-six-countries",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Comparing Tf-idf of Six Countries",
    "text": "Comparing Tf-idf of Six Countries\n\n\nCode\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    bind_tf_idf(words, key, n) %>% \n    group_by(key) %>% \n    top_n(tf_idf, n=20) %>%\n    arrange(desc(tf_idf)) %>% \n    mutate(words = reorder_within(words, tf_idf, key)) %>%\n    ggplot(aes(x=tf_idf, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 2, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing TF-IDF of Six Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Gowun Dodum\",size=40),\n          axis.text.x = element_text(size = 20))  +\n    ggeasy::easy_center_title()"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#comparing-the-word-frequencies",
    "href": "data_analytics/Vnr_analysis.html#comparing-the-word-frequencies",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Comparing the word frequencies",
    "text": "Comparing the word frequencies\n\n\nCode\ncompare_six <- total_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    dplyr::select(-n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>%\n    pivot_longer(3:7,\n               names_to = \"key\", values_to = \"proportion\")\n\n\nWords that are far from the line are words that are found more in one set of texts than another.\n\n\nCode\n# expect a warning about rows with missing values being removed\nset.seed(2022)\ncompare_six %>%\n    ggplot(aes(x = proportion, y = 대한민국, color = abs(대한민국 - proportion))) +\n    geom_abline(color = \"darkgreen\", lty = 2, size=1.5) +\n    geom_jitter(alpha = 0.1, size = 1.5, width = 0.3, height = 0.3) +\n    geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5, family=\"Gowun Dodum\",size=9) +\n    scale_x_log10(labels = percent_format()) +\n    scale_y_log10(labels = percent_format()) +\n    scale_color_gradient(limits = c(0.001, 0.01), \n                         low = \"darkgreen\", high =  \"black\") +\n    facet_wrap(~key, nrow = 2) +\n    theme(legend.position=\"none\") +\n    labs(y = \"대한민국\", x = NULL, title=\"Comparing the word frequencies of Six Countries\") +\n    theme_bw() +\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Gowun Dodum\",size=25))+\n    guides(color=\"none\") +\n    ggeasy::easy_center_title()"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#correlation-of-words",
    "href": "data_analytics/Vnr_analysis.html#correlation-of-words",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Correlation of words",
    "text": "Correlation of words\n\n\nCode\ntotal_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    dplyr::select(- n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>% \n    dplyr::select(- words) -> correlation_total  \n\ntestRes=corrplot::cor.mtest(correlation_total, conf.level = 0.95)\n\ncorrelation_total %>% \n    cor(use=\"pairwise.complete.obs\") %>% \n    .[order(.[ , 1],decreasing = T), order(.[ 1, ],decreasing = T)] %>% \n    corrplot(tl.col = \"black\",\n             diag=T, \n             type=\"lower\",\n             method=\"color\",\n             cl.pos=\"n\",\n             addCoef.col = 1,\n             number.cex = 4,\n             tl.cex = 3) \n\n\n\n\n\n\n\n\n\n\n\nCode\ncortrix = function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \n  \"spearman\"), ...) \n{\n  x = as.matrix(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  cormeth <- method\n  panel.cor <- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", \n    method = cormeth, cex.cor, ...) {\n    usr <- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- cor(x, y, use = use, method = method)\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex <- 2\n    test <- cor.test(as.numeric(x), as.numeric(y), method = method)\n    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, \n      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n        \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex)\n    text(0.8, 0.8, Signif, cex = 2, col = 2)\n  }\n  f <- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs <- list(...)\n  dotargs$method <- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light blue\", probability = TRUE, axes = FALSE, \n      main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 2)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n      diag.panel = hist.panel, ...)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n    ...)\n}\n\n\n\n\nCode\ncorrelation_total %>% \n    cortrix(histogram=F)\n\n\n\n\n\n\n\n\n\n***: 0 - 0.001"
  },
  {
    "objectID": "data_analytics/Vnr_analysis.html#document-similarity",
    "href": "data_analytics/Vnr_analysis.html#document-similarity",
    "title": "Textmining on SDG VNRs of ROK, Germany, Finland",
    "section": "Document Similarity",
    "text": "Document Similarity\n\n\nCode\nkorea_string <- korea %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \ngermany_string <- germany %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \nfinland_string <- finland %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \njapan_string <- japan %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \nuk_string <- uk %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \naustrailia_string <- austrailia %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \n\n\n\n\nCode\nimport matplotlib.pylab as plt \nimport matplotlib as mpl\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\nfrom tabulate import tabulate\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport warnings\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport string\n\n\n\n\nCode\nmpl.rc('font', family='NanumGothic') # 폰트 설정\nmpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정\n\n# 차트 스타일 설정\nsns.set(font=\"NanumGothic\", rc={\"axes.unicode_minus\":False}, style='white')\nwarnings.filterwarnings(\"ignore\")\n\n\n\nConvert data from R objects\n\n\nCode\ndocument_df = pd.DataFrame.from_dict(\n    {'filename': [\"Korea\", \"Germany\", \"Finland\", \"Japan\", \"UK\", \"Austrailia\"],\n    'opinion_text':[r.korea_string, r.germany_string, r.finland_string, \n    r.japan_string, r.uk_string, r.austrailia_string]})\n\n\n\n\nTokenizer\n\n\nCode\n# 단어 원형 추출 함수\nlemmar = WordNetLemmatizer()\n\ndef LemTokens(tokens):\n    return [lemmar.lemmatize(token) for token in tokens]\n\n# 특수 문자 사전 생성: {33: None ...}\n# ord(): 아스키 코드 생성\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n\n# 특수 문자 제거 및 단어 원형 추출\ndef LemNormalize(text):\n    text_new = text.lower().translate(remove_punct_dict)\n    word_tokens = nltk.word_tokenize(text_new)\n    return LemTokens(word_tokens)\n\n# nltk.download('punkt')\n# nltk.download('wordnet')\n# nltk.download('omw-1.4')\ntfidf_vect = TfidfVectorizer(stop_words='english' , ngram_range=(1,2), \n                             tokenizer = LemNormalize, min_df=0.05, max_df=0.85)\n\nfeature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])    \n\n\n\n\nCosine Similarity\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_pair = cosine_similarity(feature_vect[0] , feature_vect)\ndocument_df[\"similarity\"] = similarity_pair.reshape(-1,1)\n\n\n\n\n시각화\n\n\nCode\nplt.clf()\nvis=document_df.iloc[1:].sort_values(by=\"similarity\",ascending=False).reset_index(drop=True)\nsns.set_style(\"white\")\nplt.figure(figsize = (10,7))\nsns.barplot(x=\"similarity\", y=\"filename\", data=vis)\nplt.title('Korea')"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publications / Working Papers",
    "section": "",
    "text": "To be updated"
  },
  {
    "objectID": "publication.html#working-papers",
    "href": "publication.html#working-papers",
    "title": "Publications / Working Papers",
    "section": "Working Papers",
    "text": "Working Papers\n\nSocietal Impacts of Information Systems\n\nJu, S.W. (2023), How does public information systems affect improving internet accessibility of students? (Literature Review)\n\nEvidence-Based Policy and Management\n\nJu, S.W. & Lee, J., Kim, B.J. (2022), Evidence-Based Policy in Asian Countries: Applying Model of Evidence-Based Practice Implementation for Maturation Stage Analysis [in Korean]. (Analysis / Writing Finished, Target Journal: Korean Journal of Public Administration)\n\nOrganizational Decision-making\n\nJu, S.W. (2023), Blame Avoidance and Research Utilization: Evidence From Korean Public Enterprises (Literature Review)"
  },
  {
    "objectID": "publication.html#academic-presentations",
    "href": "publication.html#academic-presentations",
    "title": "Publications / Working Papers",
    "section": "Academic Presentations",
    "text": "Academic Presentations\n\nJu, S. & Lee, J. (2022), Evidence-Based Policy in Asian Countries: Applying Model of Evidence-Based Practice Implementation for Maturation Stage Analysis [in Korean, Poster Session], 2022 Korean Public Administration Summer Conference and Annual KAPA International Conference, Yeosu, Korea, June 2022."
  },
  {
    "objectID": "research_interests.html",
    "href": "research_interests.html",
    "title": "Research Interests",
    "section": "",
    "text": "1. Management information systems\n\npublic value in IT\nquantifying societal impact of information systems\n\n2. Policy analysis and program evaluation\n\nexperimental and quasi-experimental policy evaluation\n\n3. Organization theory and behavior\n\nstrategic decision-making\nhuman-machine cooperation in organizational decision-making\n\n4. Evidence-based public policy and management (EBPPM)\n\nbeing evidence-based is being data-driven?\nwhat is the difference between evidence-based approach and contemporary policy analysis? (such as discourse policy analysis)\npositivism vs. constructivism in research utilization\n\n\n\n\n\n Methodologically, I'm interested in applied econometrics for quasi-experimental approach, machine-learning as well as other computational methods for both business and public data analytics, and I/O psychology survey analysis. Also I have prior experience in policy analysis techniques (such as Cost-Benefit Analysis, Analytic Hierarchy Process etc.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sangwon Ju (주상원)",
    "section": "",
    "text": "Hi. I'm a  Master's student  in    Graduate School of Public Administration at Seoul National University, Seoul, Republic of Korea. \n\nI'm currently studying public policy and management. Generally, my works seeks to  quantify impact of information systems on acquiring public value, understand interaction between information systems and public policy process, and figure out how researchers can contribute to improving organizational decision-making by adequately utilizing information systems and evidence-based practices.   \n\n\n\n\n\nM.P.A. in Public Policy\nSeoul National University, GSPA, Seoul, Republic of Korea | Sep 2021 ~ Aug 2023\n\nB.A. in Public Policy and Management\nB.A. in Business Administration\nMinor in Applied Statistics\nYonsei University, CSS, Seoul, Republic of Korea | Mar 2015 ~ Aug 2021\n\n\n\n\n\n\nResearch Assistant 　\nKorea Research Institute for Local Administration | May 2022 ~ Present\n\nResearch Assistant 　\nEvidence-Based Research Lab, GSPA, SNU | Sep 2021 ~ Present"
  }
]