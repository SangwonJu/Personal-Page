[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sangwon Ju (주상원)",
    "section": "",
    "text": "About Me  \n\nHi. I'm currently a Ph.D. student in  Graduate School of Public Administration at Seoul National University, \nRepublic of Korea. I'm studying public management, cross-sector collaboration, organization theory and behavior, information technology in public organizations, behavioral public administration, and computational social science. Generally, my work focuses on understanding how public organizations and their employees contribute to creating public value. Furthermore, I aim to investigate how researchers can improve organizational routines by examining the importance of effective learning and knowledge management within and surrounding organizations. Lastly, I study the interaction between humans, artificial intelligence, and information systems within public organizations.\n\n\nSeoul National University, GSPA, Seoul, Republic of Korea | 2021 ~\nM.P.A. in Public Policy\nPh.D Student in Public Policy\n\n\nEvidence-Based Research Lab | Research Assistant\n\n\nYonsei University, College of Social Science, Seoul, Republic of Korea | 2015 ~ 2021\nB.A. in Public Policy and Management\nB.A. in Business Administration\nMinor in Applied Statistics\n\nKorea Research Institute for Local Administration | Research Assistant | 2022"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html",
    "href": "posts/EBPPM LAB/index.html",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "",
    "text": "In conclusion, while maintaining the mediating and organizing role of the centralized government, it is necessary to diverge from the current SDGs structure in which all detailed topics are converges to the one centalized entity. Instead, promoting policy development in the direction of strengthening cooperation outward with other countries and strengthening connectivity inward with local communities is the most effective and efficient way to achieve sustainable development goals."
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#goal",
    "href": "posts/EBPPM LAB/index.html#goal",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Goal",
    "text": "Goal\n\nAccording to the annual ranking of SDGs released by the UN, Republic of Korea ranked 28th as of 2021, lower than other developed countries.\nAccordingly, this section presents the strategic direction of environmental policy to achieve and promote SDGs.\nTo achieve this goal, this section analyzes VNRs of five developed countries to verify the characteristics of 17 SDGs directed policies and to present the direction in which SDGs-related environmental policies should be developed in the future through comparative analysis with the Republic of Korea VNR."
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#voluntary-national-review-vnr",
    "href": "posts/EBPPM LAB/index.html#voluntary-national-review-vnr",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Voluntary National Review (VNR)",
    "text": "Voluntary National Review (VNR)\n\nThe United Nations (UN) member states are required to report their SDGs objectives and implementation plans to the United Nations High-Level Political Forum (HLPF) and the material submitted at the forum is a ‘Voluntary National Review’.\nThe purpose of VNRs is to present a snapshot of where the country stands in the implementation of the Sustainable Development Goals, with a view to help accelerate progress through experience sharing, peer-learning, identifying gaps and good practices, and mobilizing partnerships. (UN, 2021)\nEach member (54 countries) started submitting VNRs to the UN’s ‘Sustainable Development Knowledge Platform’ since 2016 and was recommended to submit them at least once every four years\nThe report contains voluntary assessment of the objectives and performance of each member countries SDGs and follows no uniform form"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#countries-of-analysis",
    "href": "posts/EBPPM LAB/index.html#countries-of-analysis",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Countries of analysis",
    "text": "Countries of analysis\nRepublic of Korea, Germany, Finland, United Kingdom, Japan, Australia\n\nIn this chapter, Korea, Germany, Finland, the United Kingdom, Japan, and Australia were selected as target countries\nSelected based on the rankings of Sustainable Development Goals index announced by the UN, and the rest of the countries tried to select evenly by continent, starting with Finland, which ranked first\nIdentifying global trends on SDGs can be achieved through this analysis"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#loading-package-setting-working-directory",
    "href": "posts/EBPPM LAB/index.html#loading-package-setting-working-directory",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Loading Package & Setting Working Directory",
    "text": "Loading Package & Setting Working Directory\n\n\nCode\npacman::p_load(\"reticulate\",\"tidyverse\",\"tidymodels\",\n               \"tidytext\",\"lda\",\"readr\",\"igraph\",\"forcats\",\n               \"sna\",\"ergm\",\"network\",\"stringr\",\"magrittr\",\"showtext\",\n               \"tokenizers\",\"janeaustenr\",\"forcats\",\"RColorBrewer\",\n               \"viridis\",\"stopwords\",\"purrr\",\"widyr\",\"textmineR\",\"stm\",\n               \"ggraph\",\"tidygraph\",\"ggfortify\",\"ggthemes\",\"cowplot\",\"fs\",\n               \"knitr\",\"kableExtra\",\"ggrepel\",\"grid\",\"gridExtra\",\n               \"topicmodels\",\"scales\",\"ggpattern\",\n               \"magick\",\"corrplot\")\n\n# Font Setting\nfont_add_google(name=\"Nanum Gothic\")\nshowtext_auto()\n\n\nger_path = \"https://raw.githubusercontent.com/SangwonJu/data/main/germany.txt\"\nfin_path = \"https://raw.githubusercontent.com/SangwonJu/data/main/finland.txt\"\nkor_path = \"https://raw.githubusercontent.com/SangwonJu/data/main/korea.txt\"\njp_path = \"https://raw.githubusercontent.com/SangwonJu/data/main/japan.txt\"\nuk_path = \"https://raw.githubusercontent.com/SangwonJu/data/main/uk.txt\"\nastl_path = \"https://raw.githubusercontent.com/SangwonJu/data/main/austrailia.txt\""
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokening-custom-functions",
    "href": "posts/EBPPM LAB/index.html#tokening-custom-functions",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokening custom functions",
    "text": "Tokening custom functions\n\n\nCode\nsentence_word1=function(x){\n  x=as.character(x)\n  a=tibble(words=tokenize_sentences(x) %>%\n             unlist() %>% tibble(words=.),\n    sentence=1:nrow(words))\n  return(a)}\n\nsentence_word2=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_words(stopwords =  as.vector(stop_words$word),\n                                strip_numeric = T) %>%  \n               unlist())\n  return(a)}\n\nsentence_word3=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_ngrams(.,n = 2, n_min = 2,\n               stopwords = as.vector(stop_words$word)) %>%\n               unlist())\n  return(a)}"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#preprocess",
    "href": "posts/EBPPM LAB/index.html#preprocess",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\nkorea <-  readLines(kor_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokenizing-words",
    "href": "posts/EBPPM LAB/index.html#tokenizing-words",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nBreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\nkorea_tokenized <- korea %>% \n    mutate(words=map(text, sentence_word1)) %>%\n    dplyr::select(-text) %>%\n    unnest(words) %>% \n    transmute(paragraph,sentence,words) %>%\n    mutate(words=map(words, sentence_word2)) %>%\n    unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#n-grams",
    "href": "posts/EBPPM LAB/index.html#n-grams",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\nkorea_ngram  <-  korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis",
    "href": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\nkorea_tokenized <- korea_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n    \n\nkorea_ngram <- korea_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#frequency-table",
    "href": "posts/EBPPM LAB/index.html#frequency-table",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(korea_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Republic of Korea\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> a)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(korea_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Republic of Korea\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> a1) \n\n\nSelecting by n"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph",
    "href": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\nkorea_tfidf <- korea_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nkorea_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    conflict \n    52 \n    4 \n    0.1379310 \n    3.248435 \n    0.4480599 \n  \n  \n    governmental \n    10 \n    2 \n    0.1538462 \n    2.842970 \n    0.4373799 \n  \n  \n    institutional \n    78 \n    2 \n    0.1666667 \n    2.437504 \n    0.4062507 \n  \n  \n    saemaul \n    48 \n    5 \n    0.1219512 \n    3.248435 \n    0.3961506 \n  \n  \n    undong \n    48 \n    5 \n    0.1219512 \n    3.248435 \n    0.3961506 \n  \n  \n    disability \n    73 \n    9 \n    0.1304348 \n    3.025291 \n    0.3946032 \n  \n  \n    humanitarian \n    49 \n    7 \n    0.1555556 \n    2.437504 \n    0.3791674 \n  \n  \n    tax \n    89 \n    5 \n    0.0961538 \n    3.941582 \n    0.3789983 \n  \n  \n    mechanism \n    78 \n    2 \n    0.1666667 \n    2.236834 \n    0.3728056 \n  \n  \n    tax \n    87 \n    7 \n    0.0823529 \n    3.941582 \n    0.3246009 \n  \n  \n    indicator \n    15 \n    12 \n    0.1500000 \n    2.069780 \n    0.3104669 \n  \n  \n    person \n    73 \n    7 \n    0.1014493 \n    3.025291 \n    0.3069136 \n  \n  \n    south \n    91 \n    2 \n    0.0645161 \n    4.634729 \n    0.2990148 \n  \n  \n    indicator \n    94 \n    8 \n    0.1428571 \n    2.069780 \n    0.2956828 \n  \n  \n    oda \n    36 \n    6 \n    0.1666667 \n    1.690290 \n    0.2817150 \n  \n  \n    principle \n    64 \n    3 \n    0.1304348 \n    2.149822 \n    0.2804116 \n  \n  \n    correspond \n    33 \n    2 \n    0.0689655 \n    3.941582 \n    0.2718332 \n  \n  \n    discrimination \n    73 \n    4 \n    0.0579710 \n    4.634729 \n    0.2686799 \n  \n  \n    administration \n    89 \n    3 \n    0.0576923 \n    4.634729 \n    0.2673882 \n  \n  \n    age \n    31 \n    3 \n    0.0937500 \n    2.842970 \n    0.2665284"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#topic-modeling",
    "href": "posts/EBPPM LAB/index.html#topic-modeling",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\nkorea_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> korea_dtm.old\n\nrowTotals <- apply(korea_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_dtm <- korea_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\nkorea_split <- initial_split(korea_tfidf, prop = 0.8)\n\n# Train\nkorea_split_train <- training(korea_split)\nkorea_split_train  %>%\n    cast_dtm(paragraph, words, n) -> korea_train_dtm.old\nrowTotals <- apply(korea_train_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_train_dtm <- korea_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nkorea_split_test <- testing(korea_split)\nkorea_split_test  %>%\n    cast_dtm(paragraph, words, n) -> korea_valid_dtm.old\nrowTotals <- apply(korea_valid_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_valid_dtm <- korea_valid_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(korea_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = korea_valid_dtm)\n}\n\nkorea_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nkorea_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Nanum Gothic\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\nkorea_topic=\"\ngovernment, implementation, policy, international, establish, national, framework, ministry, stakeholder, oda, cooperation, humanitarian, assistance, project, strategy, saemual,  undong, partnership, global,\ndisability, sector, service, increase, public, act, person, welfare, strengthen, promote, ensure,\neconomic, social, plan, goal, target, integrate, environmental, society, relate,\nsustainable, development, create, environment, organization, civil, report,\ngirl, country, initiative, life, education, develop, health, quality, school, capacity\"\nkorea_topic_update <- korea_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \nkorea_topic_names=data.frame(\n  stringsAsFactors = FALSE,\n                topic = c(1L, 2L, 3L, 4L, 5L, 6L),\n                topic2= c(\"Government Policy\", \"International Cooperation\", \"Welfare\", \"Integrated Goal\", \"Sustainable Development\", \"Local Community\"))\n\n\n\n\nCode\nkorea_lda <-  LDA(korea_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_6 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(korea_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\nkorea_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(korea_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20),\n          axis.title.y=element_blank(),\n        strip.text = element_text(size = 20)) + \n    ghibli::scale_fill_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\nClustered into 6 topics: ‘government policy’, ‘international cooperation’, ‘welfare’, ‘integrated goal setting’, ‘sustainable development’, and ‘human rights’\nLooking at the first topic, ‘government policy’ and its main keywords, it can be seen that the government is taking the lead in setting and implementing goals of sustainable development\nThe implementation of Sustainable Development in Korea is centered on topics such as “international cooperation”, “welfare”, and “human rights”. For Sustainable Development, the international community is trying to promote cooperation with other countries as well as guaranteeing human rights at home and abroad\nIn order to achieve sustainable development, Korea is striving to set an integrated goal, which is usually intended to achieve social and environmental development as well as economic development\nFrequency of the words is listed in the order of occurrence frequencies, words such as ‘conflict’, ‘governmental’, ‘institutional’, ‘saemaul’, and ‘undong’ are at the top, so it can be deduced that the Korea is pursuing policies led by the government"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#network-graph",
    "href": "posts/EBPPM LAB/index.html#network-graph",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\nkorea_coocur <- korea_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- korea_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\")\n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=korea_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\nkorea_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nkorea_g <- as_tbl_graph(korea_network)\n\n\n\n\nNetwork graph of inter-topic\n\n\nCode\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1)\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\n\n\nNetwork graph within topic\n\n\nCode\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 20),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1) +\n    facet_nodes(~ community, ncol=3, scales = \"free\")\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\nLooking at the Network graph within topic, words of ‘government policy’ and ‘integrated goal setting’ has major connections to other words, which means that various discussions on the subject have been dealt with densely\nSustainable and development in Sustainable Development show the strongest connectivity in the entire network, central to the overall network structure, and ‘international cooperation’. Considering that ‘welfare’ and ‘human rights’ are not seem to be a major factor in the sustainable development of the Republic of Korea\nLooking at the Network graph by topic, ‘sustainable development’, ‘government policy’, and ‘integrated goal setting’ had a majority of connection between words within and outside the topic, wheres in the case of ‘Welfare’, ‘Human Rights’, International Cooperation’, the connection was relatively weak.\nSustainable development topics have high connectivity with other topics, especially government policy topics (‘government’, ‘implementation’ and ‘policy’). Sustainable development in Korea is mainly based on government-led integrated goals"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#preprocess-1",
    "href": "posts/EBPPM LAB/index.html#preprocess-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\ngermany<-readLines(ger_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokenizing-words-1",
    "href": "posts/EBPPM LAB/index.html#tokenizing-words-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\ngermany_tokenized <- germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#n-grams-1",
    "href": "posts/EBPPM LAB/index.html#n-grams-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\ngermany_ngram  <-  germany %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-1",
    "href": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\ngermany_tokenized <- germany_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\n\ngermany_ngram <- germany_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"germa[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#frequency-table-1",
    "href": "posts/EBPPM LAB/index.html#frequency-table-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(germany_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#FF0000',\n    pattern_fill = '#FFCC00',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Germany\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> b)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(germany_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#FF0000',\n    pattern_fill = '#FFCC00',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Germany\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> b1) \n\n\nSelecting by n"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-1",
    "href": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\ngermany_tfidf <- germany_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \ngermany_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    fracking \n    141 \n    2 \n    0.6666667 \n    5.942799 \n    3.961866 \n  \n  \n    civil \n    352 \n    2 \n    0.6666667 \n    3.109586 \n    2.073057 \n  \n  \n    hospital \n    81 \n    2 \n    0.3333333 \n    5.942799 \n    1.980933 \n  \n  \n    wood \n    318 \n    2 \n    0.3333333 \n    5.249652 \n    1.749884 \n  \n  \n    administration \n    288 \n    2 \n    0.4000000 \n    4.333361 \n    1.733345 \n  \n  \n    import \n    363 \n    2 \n    0.3333333 \n    4.844187 \n    1.614729 \n  \n  \n    urban \n    252 \n    2 \n    0.5000000 \n    3.052428 \n    1.526214 \n  \n  \n    norway \n    327 \n    2 \n    0.2857143 \n    5.249652 \n    1.499901 \n  \n  \n    job \n    196 \n    2 \n    0.4000000 \n    3.745575 \n    1.498230 \n  \n  \n    adaptation \n    307 \n    2 \n    0.4000000 \n    3.640214 \n    1.456086 \n  \n  \n    farm \n    58 \n    6 \n    0.4000000 \n    3.544904 \n    1.417962 \n  \n  \n    criminal \n    339 \n    2 \n    0.3333333 \n    4.151040 \n    1.383680 \n  \n  \n    family \n    53 \n    2 \n    0.3333333 \n    3.996889 \n    1.332296 \n  \n  \n    migration \n    235 \n    3 \n    0.2727273 \n    4.844187 \n    1.321142 \n  \n  \n    mint \n    104 \n    2 \n    0.2222222 \n    5.942799 \n    1.320622 \n  \n  \n    arctic \n    305 \n    2 \n    0.2222222 \n    5.942799 \n    1.320622 \n  \n  \n    profit \n    365 \n    2 \n    0.2222222 \n    5.942799 \n    1.320622 \n  \n  \n    disability \n    233 \n    3 \n    0.3000000 \n    4.333361 \n    1.300008 \n  \n  \n    labour \n    194 \n    2 \n    0.4000000 \n    3.170211 \n    1.268084 \n  \n  \n    south \n    369 \n    4 \n    0.4000000 \n    3.170211 \n    1.268084"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#topic-modeling-1",
    "href": "posts/EBPPM LAB/index.html#topic-modeling-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\ngermany_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> germany_dtm.old\n\nrowTotals <- apply(germany_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_dtm <- germany_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\ngermany_split <- initial_split(germany_tfidf, prop = 0.8)\n\n# Train\ngermany_split_train <- training(germany_split)\ngermany_split_train  %>%\n    cast_dtm(paragraph, words, n) -> germany_train_dtm.old\n\nrowTotals <- apply(germany_train_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_train_dtm <- germany_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\ngermany_split_test <- testing(germany_split)\ngermany_split_test  %>%\n    cast_dtm(paragraph, words, n) -> germany_test_dtm.old\n\nrowTotals <- apply(germany_test_dtm.old, 1, sum) # Find the sum of words in each Document\ngermany_test_dtm <- germany_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(germany_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = germany_test_dtm)\n}\n\ngermany_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\ngermany_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Nanum Gothic\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\ngermany_topic=\"\naction, indicator, policy, plan, goal, gas, basis. ministry, assign, provision, conflict, body,\nsustainability, city, urban, supply, chain, national, human, local, adopt community,\nwoman, train, social, act, opportunity, child, labour, vocational, law,\nfood, quality, farm, land, agricultural, natural, population, organic,\nconsumption, transition, public, innovation, focus, transport, build, \ncountry, global, fund, world, financial, condition, covid, contribution\"\ngermany_topic_update <- germany_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \ngermany_topic_names=data.frame(\n  stringsAsFactors = FALSE,\n                topic = c(1L, 2L, 3L, 4L, 5L, 6L),\n                topic2= c(\"Government Policy\", \"Local Community\", \"Human Rights\", \"Food Resources\", \"Social Innovation\", \"International Cooperation\"))\n\n\n\n\nCode\ngermany_lda <-  LDA(germany_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\ngermany_tp_6 <- germany_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(germany_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\ngermany_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(germany_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20),\n          axis.title.y=element_blank(),\n        strip.text = element_text(size = 20)) + \n    ghibli::scale_fill_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\nClustered into 6 topics: ‘Government Policy’, Local Community’, ‘Human Rights’, ‘Food Resources’, ‘Social Innovation’, ‘International Cooperation’\nIn the case of Germany, in that ‘Government Policy’, Local Community’ topic are formed, it can be deduced that goals related to sustainable development are being achieved not only by government-led policies but also by local communities\nIn addition, Germany covers various topics related to sustainable development, such as ‘Human Rights’, ‘Food Resources’, ‘Social Innovation’"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#network-graph-1",
    "href": "posts/EBPPM LAB/index.html#network-graph-1",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\ngermany_coocur <- germany_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- germany_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=germany_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\ngermany_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\ngermany_g <- as_tbl_graph(germany_network)\n\n\n\n\nNetwork graph of inter-topic\n\n\nCode\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\n\nNetwork graph within topic\n\n\nCode\ngermany_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 20),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1) +\n    facet_nodes(~ community, ncol=3, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\nLooking at the Network graph within topic, in the case of ‘International Cooperation’, ‘Local Community’, and ‘Human Rights’ topics, the connection between words is stronger than that of other topics\nIn particular, keywords such as ‘global’ and ‘country’ show the strongest connection in the ‘International Cooperation’ topic, indicating that Germany is promoting sustainable development through cooperation with other countries around the world\nLooking at the Network graph of inter-topic, topics such as ‘International cooperation’, ‘Human rights’, and ‘Local Community’ showed strong connectivity between the keywords\nIn particular, Germany’s sustainable development is being planned and implemented based on international cooperation, as keywords such as “global” and “country” of “International Cooperation” show strong connectivity"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#preprocess-2",
    "href": "posts/EBPPM LAB/index.html#preprocess-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\nfinland <-  readLines(fin_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokenizing-words-2",
    "href": "posts/EBPPM LAB/index.html#tokenizing-words-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\nfinland_tokenized <- finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#n-grams-2",
    "href": "posts/EBPPM LAB/index.html#n-grams-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\nfinland_ngram  <-  finland %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-2",
    "href": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\nfinland_tokenized <-finland_tokenized  %>%\n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"billion\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\"))  %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\nfinland_ngram <- finland_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"finl[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"finn[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"eur*\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"million\")) %>% \n    filter(!str_detect(words,pattern=\"billion\")) %>% \n    filter(!str_detect(words,pattern=\"line\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#frequency-table-2",
    "href": "posts/EBPPM LAB/index.html#frequency-table-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(finland_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#003580',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Finland\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> c)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(finland_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#003580',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Finland\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> c1) \n\n\nSelecting by n"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-2",
    "href": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\nfinland_tfidf <- finland_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nfinland_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    species \n    549 \n    5 \n    0.2941176 \n    4.330733 \n    1.2737451 \n  \n  \n    preparation \n    11 \n    3 \n    0.3750000 \n    3.392464 \n    1.2721739 \n  \n  \n    city \n    97 \n    3 \n    0.3000000 \n    3.963009 \n    1.1889026 \n  \n  \n    loss \n    564 \n    2 \n    0.2222222 \n    4.448516 \n    0.9885592 \n  \n  \n    version \n    39 \n    2 \n    0.1818182 \n    5.429346 \n    0.9871538 \n  \n  \n    audit \n    330 \n    4 \n    0.2352941 \n    4.043051 \n    0.9513062 \n  \n  \n    ministry \n    123 \n    2 \n    0.3333333 \n    2.814386 \n    0.9381286 \n  \n  \n    attitude \n    245 \n    4 \n    0.1818182 \n    5.141664 \n    0.9348479 \n  \n  \n    film \n    114 \n    2 \n    0.1428571 \n    6.527958 \n    0.9325654 \n  \n  \n    fishery \n    204 \n    2 \n    0.1428571 \n    6.527958 \n    0.9325654 \n  \n  \n    annually \n    657 \n    10 \n    0.2127660 \n    4.330733 \n    0.9214326 \n  \n  \n    minority \n    244 \n    3 \n    0.2142857 \n    4.225373 \n    0.9054370 \n  \n  \n    plastic \n    514 \n    2 \n    0.1666667 \n    5.141664 \n    0.8569439 \n  \n  \n    waste \n    517 \n    2 \n    0.2222222 \n    3.819908 \n    0.8488684 \n  \n  \n    tax \n    646 \n    5 \n    0.2272727 \n    3.694745 \n    0.8397147 \n  \n  \n    alien \n    568 \n    2 \n    0.1428571 \n    5.834811 \n    0.8335444 \n  \n  \n    invasive \n    568 \n    2 \n    0.1428571 \n    5.834811 \n    0.8335444 \n  \n  \n    fish \n    553 \n    3 \n    0.2000000 \n    4.130063 \n    0.8260125 \n  \n  \n    backlog \n    479 \n    2 \n    0.1250000 \n    6.527958 \n    0.8159947 \n  \n  \n    nordic \n    213 \n    7 \n    0.2058824 \n    3.963009 \n    0.8159135"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#topic-modeling-2",
    "href": "posts/EBPPM LAB/index.html#topic-modeling-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\nfinland_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> finland_dtm.old\n\nrowTotals <- apply(finland_dtm.old, 1, sum) # Find the sum of words in each Document\nfinland_dtm <- finland_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCast LDA (k=6)\n\n\nCode\nfinland_topic=\"\ngovernment, national, policy, report, implementation, plan, ministry, parliament, committee, commission,\ninnovation, development, sustainable, society, research, commitment, company, business, public, sector,\nindicator, assessment, people, panel, union, organisation, participate, issue, include, act,\neducation, increase, social, health, service, equality, woman, population, child,\nenergy, water, carbon, food, emission, consumption, reduce, economy, forest,\ncountry, global, climate, international, cooperation, develop, promote, change\"\nfinland_topic_update <- finland_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \nfinland_topic_names=data.frame(\n  stringsAsFactors = FALSE,\n                topic = c(1L, 2L, 3L, 4L, 5L, 6L),\n                topic2= c(\"Government Policy\", \"Social Innovation\", \"Governance\", \"Human Rights\", \"Resources Management\", \"International Cooperation\"))\n\n\n\n\nCode\nfinland_lda <-  LDA(finland_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nfinland_tp_6 <- finland_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(finland_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\nfinland_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(finland_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20),\n          axis.title.y=element_blank(),\n        strip.text = element_text(size = 20)) + \n    ghibli::scale_fill_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\nClustered into 6 topics: ‘Government Policy’, ‘Social Innovation’, ‘Governance’, ‘Human Rights’, ‘Resource Management’, ‘International Cooperation’.\nWhen looking at topics of ‘Government Policy’, ‘Governance’, it can be seen that the facilitator of the Sustainable Development Policy in Finland is not only the government but also various stakeholders participating in policy decision-making process\nIt can be seen that people are concerned about various topics related to society and people who have interest in ‘human rights’ and ‘social innovation’, and this contributes to attaining the sustainability of development\nInterest in ‘Resource Management’, which is an essential element of sustainable development, is highlighted by topics covering carbon emissions as well as water and food, indicating that Finland is paying attention to resources and the environment\nIn the case of Finland, it can be deduced that cooperation with other nations is promoted in that there is topic regarding ‘International Cooperation’"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#network-graph-2",
    "href": "posts/EBPPM LAB/index.html#network-graph-2",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\nfinland_coocur <- finland_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- finland_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=finland_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\nfinland_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nfinland_g <- as_tbl_graph(finland_network)\n\n\n\n\nNetwork Graph of inter-topic\n\n\nCode\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph within topic\n\n\nCode\nfinland_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 20),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1) +\n    facet_nodes(~ community, ncol=3, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\nLooking the Network Graph within topic, the topics of ‘Social Innovation’, ‘Government Policy’, and ‘International Cooperation’ show a stronger connection between words than other topics\nThis means that Finland’s efforts on achieving above three topics have been promoted relatively well\nIn particular, keywords such as ‘sustainable’ and ‘development’ show the strongest connection in the ‘Social Innovation’ topic, indicating that Finland is pursuing a high level of social innovation to achieve sustainable development\nLooking at the Network Graph of inter-topic, the topic of ‘Social Innovation’, which accounts for the largest portion of the network, is frequently linked to ‘government policy’ keywords such as ‘government’ and ‘policy’, indicating that sustainable development based on social innovation in Finland is the governmen-driven"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#preprocess-3",
    "href": "posts/EBPPM LAB/index.html#preprocess-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\njapan<-readLines(jp_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokenizing-words-3",
    "href": "posts/EBPPM LAB/index.html#tokenizing-words-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\njapan_tokenized <- japan %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#n-grams-3",
    "href": "posts/EBPPM LAB/index.html#n-grams-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\njapan_ngram  <-  japan %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-3",
    "href": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\njapan_tokenized <- japan_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"jap*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"asia[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\n\njapan_ngram <- japan_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"jap*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"asia[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#frequency-table-3",
    "href": "posts/EBPPM LAB/index.html#frequency-table-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(japan_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#BC002D',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Japan\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> d)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(japan_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#BC002D',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Japan\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> d1) \n\n\nSelecting by n"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-3",
    "href": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\njapan_tfidf <- japan_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \njapan_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    smoke \n    298 \n    5 \n    0.2083333 \n    6.003887 \n    1.2508098 \n  \n  \n    mekong \n    271 \n    4 \n    0.1428571 \n    6.003887 \n    0.8576982 \n  \n  \n    forest \n    347 \n    10 \n    0.2083333 \n    3.806662 \n    0.7930547 \n  \n  \n    expectancy \n    128 \n    5 \n    0.1351351 \n    5.310740 \n    0.7176676 \n  \n  \n    forest \n    348 \n    11 \n    0.1803279 \n    3.806662 \n    0.6864473 \n  \n  \n    suicide \n    299 \n    13 \n    0.1494253 \n    4.394449 \n    0.6566418 \n  \n  \n    household \n    284 \n    6 \n    0.1935484 \n    3.364830 \n    0.6512574 \n  \n  \n    reliance \n    286 \n    2 \n    0.1176471 \n    5.310740 \n    0.6247929 \n  \n  \n    litter \n    232 \n    5 \n    0.1515152 \n    4.057977 \n    0.6148450 \n  \n  \n    traffic \n    176 \n    7 \n    0.1129032 \n    5.310740 \n    0.5995997 \n  \n  \n    city \n    364 \n    3 \n    0.2307692 \n    2.477527 \n    0.5717369 \n  \n  \n    food \n    335 \n    4 \n    0.2105263 \n    2.708050 \n    0.5701158 \n  \n  \n    relation \n    21 \n    3 \n    0.1578947 \n    3.605992 \n    0.5693671 \n  \n  \n    forest \n    219 \n    7 \n    0.1458333 \n    3.806662 \n    0.5551383 \n  \n  \n    investment \n    358 \n    4 \n    0.1904762 \n    2.912845 \n    0.5548275 \n  \n  \n    railway \n    177 \n    3 \n    0.0909091 \n    6.003887 \n    0.5458079 \n  \n  \n    investor \n    359 \n    2 \n    0.1428571 \n    3.806662 \n    0.5438089 \n  \n  \n    accident \n    239 \n    5 \n    0.1086957 \n    4.905275 \n    0.5331820 \n  \n  \n    approximately \n    287 \n    3 \n    0.1578947 \n    3.364830 \n    0.5312889 \n  \n  \n    icon \n    361 \n    2 \n    0.0869565 \n    6.003887 \n    0.5220771"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#topic-modeling-3",
    "href": "posts/EBPPM LAB/index.html#topic-modeling-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\njapan_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> japan_dtm.old\n\nrowTotals <- apply(japan_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_dtm <- japan_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\njapan_split <- initial_split(japan_tfidf, prop = 0.8)\n\n# Train\njapan_split_train <- training(japan_split)\njapan_split_train  %>%\n    cast_dtm(paragraph, words, n) -> japan_train_dtm.old\n\nrowTotals <- apply(japan_train_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_train_dtm <- japan_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\njapan_split_test <- testing(japan_split)\njapan_split_test  %>%\n    cast_dtm(paragraph, words, n) -> japan_test_dtm.old\n\nrowTotals <- apply(japan_test_dtm.old, 1, sum) # Find the sum of words in each Document\njapan_test_dtm <- japan_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(japan_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = japan_test_dtm)\n}\n\njapan_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\njapan_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Nanum Gothic\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=10)\n\n\nCode\njapan_topic=\"covid, pandemic, system, poverty, life, population, age, service, people, local, government, city, promote, regional, aim, organization, private, public, initiative, disaster, increase, numb, water, reduction, survey, time, rate, worker, result, previous, compare, approximately, society, woman, gender, human, include, business, equality, activity, company, civil, child, education, school, national, awareness, raise, disability, center, student, generation, country, development, international, develop, cooperation, contribute, nation, provide, fund, peace,food, forest, resource, waste, marine, plastic, loss, management, fishery, biodiversity, change, economic, social, infrastructure, sustainable, quality, environment, technology, growth, innovation, energy, transportation, renewable, consumption, addition, emission, reduce, power, project, promotion, effort, plan, policy, action, achieve, target, goal, stakeholder\"\njapan_topic_update <- japan_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \njapan_topic_names=data.frame(\n  stringsAsFactors = T, topic = c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L),               topic2= c(\"Quality of Life\",\"Government Policy\",\"Disaster Management\",\n            \"Human Rights\",\"Education\",\"International Cooperation\",\n            \"Natural Resources\",\"Innovative Development\",\n            \"Renewable Energy\",\"Goal-oriented Development\"))\n\n\n\n\nCode\njapan_lda <-  LDA(japan_dtm, k = 10,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\njapan_tp_10 <- japan_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(japan_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\njapan_tp_10 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(japan_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\", ncol=4) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=20),\n          axis.text.x = element_text(size = 25),\n          axis.title.y=element_blank(),\n          strip.text = element_text(size = 18)) + \n    scale_fill_manual( values = colorRampPalette(ghibli::ghibli_palette(\"PonyoLight\"))(10))\n\n\n\n\n\n\n\n\n\n\nClustered into 10 topics: ‘Quality of Life’,‘Government Policy’,‘Disaster Management’,‘Human Rights’,‘Education’,‘International Cooperation’,‘Natural Resources’,‘Innovative Development’,‘Renewable Energy’,‘Goal-oriented Development’\nThrough topics such as “Government Policy,” Goal-Oriented Development” and “Innovative Growth,” it is expected that Japan’s policy process will be goal-oriented and emphasizing innovation\nIn addition, due to Japan’s frequent natural disasters such as earthquakes and tsunamis, preparation and response to disasters are considered an important topic (‘Disaster Management’). In addition, it is found that the government is focusing on utilizing renewable energy and fostering human resources to reduce the country’s development and dependence on energy by reflecting the fact that natural resources are highly dependent on foreign countries due to their low reserves (‘Education’ and ‘Renewable Energy’).\nFurthermore, efforts are being made for modern universal values and activities such as ‘human rights’ and ‘international cooperation’.\nWhen the frequency of the appearance of certain words is listed in the order of many, words such as ‘government’, ‘promote’, ‘social’, ‘development’, and ‘country’ are at the top, so it can be expected that Japan is implementing policies led by the government."
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#network-graph-3",
    "href": "posts/EBPPM LAB/index.html#network-graph-3",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\njapan_coocur <- japan_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) %>%\n    filter(!item1==\"gender\"&!item2==\"gender\")\n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- japan_coocur %>%\n    top_n(150) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=japan_tp_10,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\njapan_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\njapan_g <- as_tbl_graph(japan_network)\n\n\n\n\nNetwork Graph of inter-topic\n\n\nCode\njapan_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = topic) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    scale_color_manual(values = colorRampPalette(ghibli::ghibli_palette(\"PonyoLight\"))(10)[-8])\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph within topic\n\n\nCode\njapan_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 20),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    facet_nodes(~ community, ncol=5, scales = \"free\") +\n    scale_color_manual(values = colorRampPalette(ghibli::ghibli_palette(\"PonyoLight\"))(10)[-8])\n\n\n\n\n\n\n\n\n\n\nLooking at the Network Graph within topic, the connection between words is more vivid than those other topics in the case of ‘Government Policy’, ‘International Cooperation’, ‘Goal-oriented Development’, and ‘Human Rights’ topics\nThrough this, it can be inferred that govenrment efforts related to these four topics are relatively well constructed\nIn particular, in the ‘Government Policy’ topic, the relationship between keywords such as ‘government’ and ‘promote’, ‘government’ and ‘local’ is clear, indicating that the government-led top-down policy process is adopted\nLooking at the Network Graph of inter-topic, strong connection between keywords within ‘Governmental Policy’ and ‘International Cooperation’ topics is easy to recognize\nIn particular, keywords of ‘promote’ and ‘government’ of ‘Government Policy” topic showed a strong connection with ’international’, ‘development’, ‘cooperation’, and ‘human’ of ‘international cooperation’ topic, and in Japan, policies related to sustainable development at domestic and foreign are led by the central government"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#preprocess-4",
    "href": "posts/EBPPM LAB/index.html#preprocess-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\nuk <-readLines(uk_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokenizing-words-4",
    "href": "posts/EBPPM LAB/index.html#tokenizing-words-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\nuk_tokenized <- uk %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#n-grams-4",
    "href": "posts/EBPPM LAB/index.html#n-grams-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\nuk_ngram  <-  uk %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-4",
    "href": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\nuk_tokenized <- uk_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"uk[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"wale*\")) %>% \n    filter(!str_detect(words,pattern=\"scot*\")) %>% \n    filter(!str_detect(words,pattern=\"eng*\")) %>% \n    filter(!str_detect(words,pattern=\"irel*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"wt*\")) %>% \n    filter(!str_detect(words,pattern=\"rbmps*\")) %>% \n    filter(!str_detect(words,pattern=\"inn*\")) %>%\n    filter(!str_detect(words,pattern=\"whilst*\")) %>% \n    filter(!str_detect(words,pattern=\"npt*\")) %>%\n    filter(!str_detect(words,pattern=\"britain*\")) \n\nuk_ngram <- uk_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"uk[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"wale*\")) %>% \n    filter(!str_detect(words,pattern=\"scot*\")) %>% \n    filter(!str_detect(words,pattern=\"eng*\")) %>% \n    filter(!str_detect(words,pattern=\"irel*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>% \n    filter(!str_detect(words,pattern=\"wt*\")) %>% \n    filter(!str_detect(words,pattern=\"rbmps*\")) %>% \n    filter(!str_detect(words,pattern=\"inn*\")) %>%\n    filter(!str_detect(words,pattern=\"whilst*\")) %>% \n    filter(!str_detect(words,pattern=\"npt*\")) %>%\n    filter(!str_detect(words,pattern=\"britain*\"))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#frequency-table-4",
    "href": "posts/EBPPM LAB/index.html#frequency-table-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(uk_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00247D',\n    pattern_fill = '#CF142B',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"United Kingdom\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> e)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(uk_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00247D',\n    pattern_fill = '#CF142B',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"United Kingdom\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> e1) \n\n\nSelecting by n"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-4",
    "href": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\nuk_tfidf <- uk_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nuk_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    bus \n    705 \n    3 \n    0.7500000 \n    6.329721 \n    4.747291 \n  \n  \n    nihr \n    196 \n    2 \n    0.6666667 \n    7.022868 \n    4.681912 \n  \n  \n    daily \n    709 \n    2 \n    0.6666667 \n    5.636574 \n    3.757716 \n  \n  \n    abortion \n    201 \n    2 \n    0.6666667 \n    5.231109 \n    3.487406 \n  \n  \n    nhs \n    210 \n    2 \n    0.6666667 \n    4.943427 \n    3.295618 \n  \n  \n    hour \n    533 \n    2 \n    0.6666667 \n    4.624973 \n    3.083315 \n  \n  \n    contact \n    199 \n    2 \n    0.5000000 \n    5.636574 \n    2.818287 \n  \n  \n    victim \n    997 \n    8 \n    0.6666667 \n    4.189655 \n    2.793103 \n  \n  \n    lgbt \n    652 \n    3 \n    0.6000000 \n    4.624973 \n    2.774984 \n  \n  \n    day \n    429 \n    2 \n    0.6666667 \n    4.132496 \n    2.754998 \n  \n  \n    birth \n    1002 \n    2 \n    0.5000000 \n    5.413430 \n    2.706715 \n  \n  \n    cardiovascular \n    208 \n    3 \n    0.3750000 \n    7.022868 \n    2.633576 \n  \n  \n    landfill \n    769 \n    2 \n    0.5000000 \n    5.231109 \n    2.615554 \n  \n  \n    conviction \n    334 \n    2 \n    0.4000000 \n    6.329721 \n    2.531888 \n  \n  \n    participatory \n    1014 \n    2 \n    0.4000000 \n    6.329721 \n    2.531888 \n  \n  \n    paramilitarism \n    1027 \n    2 \n    0.4000000 \n    6.329721 \n    2.531888 \n  \n  \n    road \n    207 \n    5 \n    0.5555556 \n    4.537961 \n    2.521090 \n  \n  \n    pupil \n    312 \n    3 \n    0.6000000 \n    4.189655 \n    2.513793 \n  \n  \n    mobility \n    670 \n    6 \n    0.4615385 \n    5.231109 \n    2.414358 \n  \n  \n    product \n    758 \n    2 \n    0.6666667 \n    3.557132 \n    2.371421"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#topic-modeling-4",
    "href": "posts/EBPPM LAB/index.html#topic-modeling-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\nuk_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> uk_dtm.old\n\nrowTotals <- apply(uk_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_dtm <- uk_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\nuk_split <- initial_split(uk_tfidf, prop = 0.8)\n\n# Train\nuk_split_train <- training(uk_split)\nuk_split_train  %>%\n    cast_dtm(paragraph, words, n) -> uk_train_dtm.old\n\nrowTotals <- apply(uk_train_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_train_dtm <- uk_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nuk_split_test <- testing(uk_split)\nuk_split_test  %>%\n    cast_dtm(paragraph, words, n) -> uk_test_dtm.old\n\nrowTotals <- apply(uk_test_dtm.old, 1, sum) # Find the sum of words in each Document\nuk_test_dtm <- uk_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(uk_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = uk_test_dtm)\n}\n\nuk_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nuk_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Nanum Gothic\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\nuk_topic=\"\nplastic, quality, population, land, significant, impact, priority, monitor, air,\nchild, school, fund, girl, disability, family, adult, additional,\ngoal, community, local, national, public, risk, act, approach, opportunity, authority,\nplan, action, policy, publish, gap, launch, focus, fair, ambition,\nglobal, country, commit, build, job, capacity, poor, africa, bank,\nfood, supply, transport, company, standard, city, productivity, aim, product\"\nuk_topic_update <- uk_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \nuk_topic_names=data.frame(\n  stringsAsFactors = FALSE,\n                topic = c(1L, 2L, 3L, 4L, 5L, 6L),\n                topic2= c(\"Environment\", \"Human Rights\", \"Governance\", \"Government Policy\", \"International Cooperation\", \"Food Security\"))\n\n\n\n\nCode\nuk_lda <-  LDA(uk_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nuk_tp_6 <- uk_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(uk_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\nuk_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(uk_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20),\n          axis.title.y=element_blank(),\n        strip.text = element_text(size = 20)) + \n    ghibli::scale_fill_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\nClustered into 6 topics: ‘Environment’, ‘Human Rights’, ‘Governance’, ‘Government Policy’, ‘International Cooperation’, ‘Food Security’\nThe policy process is expected to take place in a way that cooperates with civil society, not just the top-down way in the past, despite the fact that the government plays a leading role.\nIn addition, due to the recent climate crisis and Russian-Ukraine war, food problems have emerged as an important issue not only in developing countries but also in major developed countries, and the concept of security is expanding to a area of food security.\nUK showed great effort ot attain mordern universial values such as ‘Human Rights’ and ‘International Cooperation’.\nWhen the keywords are sorted in the order of occurrence frequencies, keywords such as ‘goal’, ‘plan’, ‘action’, ‘national’, ‘community’, and ‘local’ are at the top, so it can be deduced that the UK has a cooperative approach with civil society, while total scheme being led by the central government."
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#network-graph-4",
    "href": "posts/EBPPM LAB/index.html#network-graph-4",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\nuk_coocur <- uk_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) %>% \n    filter(!(item1==\"school\"&item2==\"child\")) %>% \n    filter(!(item2==\"school\"&item1==\"child\")) %>% \n    filter(!(item2==\"family\"&item1==\"child\")) %>% \n    filter(!(item2==\"child\"&item1==\"family\"))\n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- uk_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=uk_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic) %>% \n    filter(!(nodes %in% c(\"school\",\"child\",\"family\")))\n\n\n\n\nBuilding networks\n\n\nCode\nuk_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nuk_g <- as_tbl_graph(uk_network)\n\n\n\n\nNetwork Graph of inter-topic\n\n\nCode\nuk_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph within topic\n\n\nCode\nuk_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 20),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1) +\n    facet_nodes(~ community, ncol=3, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\nLooking at the Network Graph within topic, the connection between words is more evident than other topics like ‘Governance’, ‘International Cooperation’, and ‘Government Policy’\nThis allows us to deduce that policy efforts related to the these four topics are organized relatively well\nIn particular, the relationship between keywords of ‘plan’ and ‘action’ in the ‘government’ topic is clear, and the relationship between ‘local’ and ‘community’, ‘authority’, ‘national’ and ‘goal’ in the government topic shows that the government forms and executes policies in cooperation with the community and civil society\nLooking at the Network Graph of inter-topic, there is a strong connection between the words constituting ‘Governance’ and ‘Government Policy’, ‘Governance’ and ‘International Cooperation’\nIn particular, the connection between keywords such as ‘Goal’, ‘Action’ and ‘Plan’ is relatively strong\nTherefore, it can be inferred that the British government is actively working on international cooperation and establishing policies in cooperation with civil society"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#preprocess-5",
    "href": "posts/EBPPM LAB/index.html#preprocess-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Preprocess",
    "text": "Preprocess\n\n\nCode\naustrailia<-readLines(astl_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tokenizing-words-5",
    "href": "posts/EBPPM LAB/index.html#tokenizing-words-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Tokenizing words",
    "text": "Tokenizing words\nbreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\nCode\naustrailia_tokenized <- austrailia %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#n-grams-5",
    "href": "posts/EBPPM LAB/index.html#n-grams-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "n-grams",
    "text": "n-grams\nAn n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence of\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\nn words.\n\n\nCode\naustrailia_ngram  <-  austrailia %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words)"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-5",
    "href": "posts/EBPPM LAB/index.html#remove-inadequate-words-for-analysis-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Remove inadequate words for analysis",
    "text": "Remove inadequate words for analysis\n\n\nCode\naustrailia_tokenized <- austrailia_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"austra*[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"torres*\"))%>% \n    filter(!str_detect(words,pattern=\"cent\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n\naustrailia_ngram <- austrailia_ngram %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"austrail[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"eur[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"eu*\")) %>%\n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>%\n    filter(!str_detect(words,pattern=\"inwh*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"torres*\"))%>% \n    filter(!str_detect(words,pattern=\"cent\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#frequency-table-5",
    "href": "posts/EBPPM LAB/index.html#frequency-table-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Frequency table",
    "text": "Frequency table\nThe frequency of specific words and n-grams in each paragraphs.\n\n\nCode\n(austrailia_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00008B',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Austrailia\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> f)\n\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nCode\n(austrailia_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#00008B',\n    pattern_fill = '#FFFFFF',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Austrailia\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> f1) \n\n\nSelecting by n"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-5",
    "href": "posts/EBPPM LAB/index.html#tf-idf-per-paragraph-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "TF-IDF per paragraph",
    "text": "TF-IDF per paragraph\n\n\nCode\naustrailia_tfidf <- austrailia_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \naustrailia_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    words \n    paragraph \n    n \n    tf \n    idf \n    tf_idf \n  \n \n\n  \n    broadband \n    206 \n    4 \n    0.3076923 \n    5.971262 \n    1.837311 \n  \n  \n    scarcity \n    150 \n    2 \n    0.4000000 \n    4.584968 \n    1.833987 \n  \n  \n    park \n    294 \n    2 \n    0.4000000 \n    4.361824 \n    1.744730 \n  \n  \n    fish \n    305 \n    7 \n    0.3888889 \n    4.361824 \n    1.696265 \n  \n  \n    child \n    64 \n    2 \n    0.5000000 \n    2.926739 \n    1.463370 \n  \n  \n    child \n    111 \n    3 \n    0.5000000 \n    2.926739 \n    1.463370 \n  \n  \n    carbon \n    283 \n    3 \n    0.3750000 \n    3.668677 \n    1.375754 \n  \n  \n    lgbtiqa \n    103 \n    2 \n    0.2222222 \n    5.971262 \n    1.326947 \n  \n  \n    discrimination \n    222 \n    2 \n    0.3333333 \n    3.774037 \n    1.258012 \n  \n  \n    world \n    293 \n    3 \n    0.6000000 \n    1.963929 \n    1.178357 \n  \n  \n    profit \n    350 \n    2 \n    0.2500000 \n    4.584968 \n    1.146242 \n  \n  \n    basin \n    163 \n    3 \n    0.2727273 \n    4.179502 \n    1.139864 \n  \n  \n    goal \n    7 \n    2 \n    0.3333333 \n    3.406312 \n    1.135438 \n  \n  \n    collaboration \n    219 \n    2 \n    0.3333333 \n    3.406312 \n    1.135438 \n  \n  \n    human \n    20 \n    5 \n    0.4545455 \n    2.474754 \n    1.124888 \n  \n  \n    rubbish \n    301 \n    3 \n    0.1875000 \n    5.971262 \n    1.119612 \n  \n  \n    park \n    303 \n    2 \n    0.2500000 \n    4.361824 \n    1.090456 \n  \n  \n    mhfa \n    102 \n    2 \n    0.1818182 \n    5.971262 \n    1.085684 \n  \n  \n    mikta \n    361 \n    4 \n    0.1818182 \n    5.971262 \n    1.085684 \n  \n  \n    wto \n    371 \n    4 \n    0.2352941 \n    4.584968 \n    1.078816"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#topic-modeling-5",
    "href": "posts/EBPPM LAB/index.html#topic-modeling-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Topic modeling",
    "text": "Topic modeling\n\n\nCode\nset.seed(2022)\n\naustrailia_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> austrailia_dtm.old\n\nrowTotals <- apply(austrailia_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_dtm <- austrailia_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n\n\nUsing perplexity for hold out set\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\nCode\naustrailia_split <- initial_split(austrailia_tfidf, prop = 0.8)\n\n# Train\naustrailia_split_train <- training(austrailia_split)\naustrailia_split_train  %>%\n    cast_dtm(paragraph, words, n) -> austrailia_train_dtm.old\n\nrowTotals <- apply(austrailia_train_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_train_dtm <- austrailia_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\naustrailia_split_test <- testing(austrailia_split)\naustrailia_split_test  %>%\n    cast_dtm(paragraph, words, n) -> austrailia_test_dtm.old\n\nrowTotals <- apply(austrailia_test_dtm.old, 1, sum) # Find the sum of words in each Document\naustrailia_test_dtm <- austrailia_test_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(austrailia_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = austrailia_test_dtm)}\n\naustrailia_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\naustrailia_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Nanum Gothic\",size=40))\n\n\n\n\n\n\n\n\n\n\n\nCast LDA (k=6)\n\n\nCode\naustrailia_topic=\" organisation, approach, policy, indicator, civil, social, financial, action,\nfood, supply, cost, world, agricultural, risk, aim, production,\nwoman, aboriginal, disability, child, school, participation, girl, family,\nhuman, national, law, public, activity, institution, commit, land, discrimination,\ncity, plan, community, sustainability, build, growth, local, urban, opportunity, council,\npacific, country, fund, island, indo, capacity, asia, aid, major, bank, assist\"\naustrailia_topic_update <- austrailia_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \naustrailia_topic_names=data.frame(\n  stringsAsFactors = FALSE,\n                topic = c(1L, 2L, 3L, 4L, 5L, 6L),\n                topic2= c(\"Government Policy\", \"Food Security\", \"Human Rights\", \"Public Value\", \"Governance\", \"International Cooperation\"))\n\n\n\n\nCode\naustrailia_lda <-  LDA(austrailia_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\naustrailia_tp_6 <- austrailia_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(austrailia_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\naustrailia_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(austrailia_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20),\n          axis.title.y=element_blank(),\n        strip.text = element_text(size = 20)) + \n    ghibli::scale_fill_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\nClustered into 6 topics: ‘Government Policy’, ‘Food Security’, ‘Human Rights’, ‘Public Value’, ‘Governance’, ‘International Cooperation’\nIt is expected that the policy process will be conducted in a way that cooperates with civil society, not the top-down way in the past, even if the government plays a leading role through the topics of ‘Governance’ and ‘Government Policy’.\nIn addition, it can be deduced that there is a high interest in community values and order such as ‘Public value’ and ‘Human rights’\nIn particular, the human rights issue seems to reflect the situation in Australia, which has a native (Aborigine) problem unlike other comparable countries\nWhen the keywords are sorted in the order of occurrence frequencies, the words ‘community’, ‘country’, ‘woman’, ‘national’, and ‘original’ are at the top, so Australia seems like promoting policies in cooperation with civil society and deal with human rights issues, especially women and natives"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#network-graph-5",
    "href": "posts/EBPPM LAB/index.html#network-graph-5",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Network Graph",
    "text": "Network Graph\n\nPairwise Count\n\n\nCode\naustrailia_coocur <- austrailia_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n\n\n\n\nBuilding links and nodes\n\n\nCode\nlinks <- austrailia_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\") \n\n\nSelecting by n\n\n\nCode\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=austrailia_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n\n\n\n\nBuilding networks\n\n\nCode\naustrailia_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\naustrailia_g <- as_tbl_graph(austrailia_network)\n\n\n\n\nNetwork Graph of inter-topic\n\n\nCode\naustrailia_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1)\n\n\n\n\n\n\n\n\n\n\n\nNetwork Graph within topic\n\n\nCode\naustrailia_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 20),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1) +\n    facet_nodes(~ community, ncol=3, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\nLooking at the Network Graph within topic, the connection between words is more evident in topics like ‘Governance’, ‘International Cooperation’, ‘Human Rights’, and ‘Government Policy’ than other topics\nThrough this, it can be inferred that policy efforts related to these four topics are relatively well constructed\nIn particular, the relationship between keywords such as ‘community’ and ‘build’, ‘local’, ‘city’, and ‘impact’ is evident in the ‘Governance’ topic, indicating that a bottom-up policy process in which the government cooperates with the community and civil society rather than simply leading the policy process\nThe relationship between keywords of ‘aboriginal’ and ‘straits’ in the ‘human rights’ topic and ‘indo’ and ‘country’ in the ‘international cooperation’ topic, we can see that the Australian government is paying attention to aboriginal poverty and international cooperation in the Indo-Pacific region\nLooking at the Network Graph of inter-topic, there is a strong connection between words that constitutes ‘Governance’ and ‘Government Policy’, ‘Governance’ and ‘Human Rights’\nIn particular, the connection between keywords such as ‘community’ and ‘social’, ‘organization’, ‘approach’, ‘country’, ‘Pacific’, ‘Aboriginal’, ‘straight’, and ‘focus’ is relatively strong\nTherefore, it can be inferred that the Australian government is continuously taking human rights issues into account while establishing policies in cooperation with civil society and actively engaging in international cooperation"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#comparing-word-frequencies-of-six-countries",
    "href": "posts/EBPPM LAB/index.html#comparing-word-frequencies-of-six-countries",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Comparing Word Frequencies of Six Countries",
    "text": "Comparing Word Frequencies of Six Countries\n\n\nCode\nkorea_tokenized %>% \n    mutate(key=\"Korea\") %>% \n    full_join(germany_tokenized %>%\n                  mutate(key=\"Germany\")) %>% \n    full_join(finland_tokenized %>%\n                  mutate(key=\"Finland\")) %>% \n    full_join(japan_tokenized %>% \n                mutate(key=\"Japan\")) %>%\n    full_join(uk_tokenized %>% \n                mutate(key=\"United Kingdom\")) %>%\n    full_join(austrailia_tokenized %>% \n                mutate(key=\"Austrailia\")) %>%\n    mutate(key=as_factor(key)) -> total_tokenized\n\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    group_by(key) %>% \n    top_n(n, n=20) %>%\n    arrange(desc(n)) %>% \n    mutate(words = reorder_within(words, n, key)) %>%\n    ggplot(aes(x=n, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 2, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing Word Frequencies of Six Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20))+\n    ggeasy::easy_center_title()"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#comparing-bigrams-of-six-countries",
    "href": "posts/EBPPM LAB/index.html#comparing-bigrams-of-six-countries",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Comparing bigrams of Six Countries",
    "text": "Comparing bigrams of Six Countries\n\n\nCode\ngrid.arrange(a1,b1,c1,d1,e1,f1, nrow=2,\n             top=textGrob(\"Comparing Bigrams of Six Countries\",\n                          gp = gpar(col = \"black\", \n                                    fontsize = 40,\n                                    fontfamily=\"Nanum Gothic\")))"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#comparing-tf-idf-of-six-countries",
    "href": "posts/EBPPM LAB/index.html#comparing-tf-idf-of-six-countries",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Comparing Tf-idf of Six Countries",
    "text": "Comparing Tf-idf of Six Countries\n\n\nCode\ntotal_tokenized %>% \n    group_by(key) %>%\n    count(key,words) %>% \n    bind_tf_idf(words, key, n) %>% \n    group_by(key) %>% \n    top_n(tf_idf, n=20) %>%\n    arrange(desc(tf_idf)) %>% \n    mutate(words = reorder_within(words, tf_idf, key)) %>%\n    ggplot(aes(x=tf_idf, y=words)) +\n    geom_col(aes(fill=key),show.legend = FALSE) +\n    scale_y_reordered() +\n    facet_wrap(~key, nrow = 2, scales = \"free\") +\n    labs(y = NULL, title=\"Comparing TF-IDF of Six Countries\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20))  +\n    ggeasy::easy_center_title()"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#comparing-the-word-frequencies",
    "href": "posts/EBPPM LAB/index.html#comparing-the-word-frequencies",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Comparing the word frequencies",
    "text": "Comparing the word frequencies\n\n\nCode\ncompare_six <- total_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    dplyr::select(-n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>%\n    pivot_longer(3:7,\n               names_to = \"key\", values_to = \"proportion\")\n\n\nWords that are far from the line are words that are found more in one set of texts than another.\n\n\nCode\n# expect a warning about rows with missing values being removed\nset.seed(2022)\ncompare_six %>%\n    ggplot(aes(x = proportion, y = Korea, color = abs(Korea - proportion))) +\n    geom_abline(color = \"darkgreen\", lty = 2, size=1.5) +\n    geom_jitter(alpha = 0.1, size = 1.5, width = 0.3, height = 0.3) +\n    geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5, family=\"Nanum Gothic\",size=9) +\n    scale_x_log10(labels = percent_format()) +\n    scale_y_log10(labels = percent_format()) +\n    scale_color_gradient(limits = c(0.001, 0.01), \n                         low = \"darkgreen\", high =  \"black\") +\n    facet_wrap(~key, nrow = 2) +\n    theme(legend.position=\"none\") +\n    labs(y = \"Republic of Korea\", x = NULL, title=NULL) +\n    theme_bw() +\n    theme(plot.title = element_text(size=40),\n          strip.text = element_text(size = 40),\n          text=element_text(family=\"Nanum Gothic\",size=25))+\n    guides(color=\"none\") +\n    ggeasy::easy_center_title()"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#correlation-of-words",
    "href": "posts/EBPPM LAB/index.html#correlation-of-words",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Correlation of words",
    "text": "Correlation of words\n\n\nCode\ntotal_tokenized %>% \n    count(key, words) %>% \n    group_by(key) %>%\n    mutate(proportion = n / sum(n)) %>%\n    dplyr::select(- n) %>% \n    pivot_wider(names_from = key, values_from = proportion) %>% \n    dplyr::select(- words) -> correlation_total  \n\ntestRes=corrplot::cor.mtest(correlation_total, conf.level = 0.95)\n\ncorrelation_total %>% \n    cor(use=\"pairwise.complete.obs\") %>% \n    .[order(.[ , 1],decreasing = T), order(.[ 1, ],decreasing = T)] %>% \n    corrplot(tl.col = \"black\",\n             diag=T, \n             type=\"lower\",\n             method=\"color\",\n             cl.pos=\"n\",\n             addCoef.col = 1,\n             number.cex = 4,\n             tl.cex = 3) \n\n\n\n\n\n\n\n\n\n\n\nCode\ncortrix = function (R, histogram = TRUE, method = c(\"pearson\", \"kendall\", \n  \"spearman\"), ...) \n{\n  x = as.matrix(R, method = \"matrix\")\n  if (missing(method)) \n    method = method[1]\n  cormeth <- method\n  panel.cor <- function(x, y, digits = 2, prefix = \"\", use = \"pairwise.complete.obs\", \n    method = cormeth, cex.cor, ...) {\n    usr <- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    r <- cor(x, y, use = use, method = method)\n    txt <- format(c(r, 0.123456789), digits = digits)[1]\n    txt <- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) \n      cex <- 2\n    test <- cor.test(as.numeric(x), as.numeric(y), method = method)\n    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, \n      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c(\"***\", \n        \"**\", \"*\", \".\", \" \"))\n    text(0.5, 0.5, txt, cex = cex)\n    text(0.8, 0.8, Signif, cex = 2, col = 2)\n  }\n  f <- function(t) {\n    dnorm(t, mean = mean(x), sd = sd.xts(x))\n  }\n  dotargs <- list(...)\n  dotargs$method <- NULL\n  rm(method)\n  hist.panel = function(x, ... = NULL) {\n    par(new = TRUE)\n    hist(x, col = \"light blue\", probability = TRUE, axes = FALSE, \n      main = \"\", breaks = \"FD\")\n    lines(density(x, na.rm = TRUE), col = \"red\", lwd = 2)\n    rug(x)\n  }\n  if (histogram) \n    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n      diag.panel = hist.panel, ...)\n  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, \n    ...)\n}\n\n\n\n\nCode\ncorrelation_total %>% \n    cortrix(histogram=F)\n\n\n\n\n\n\n\n\n\n***: 0 - 0.001"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#document-similarity",
    "href": "posts/EBPPM LAB/index.html#document-similarity",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Document Similarity",
    "text": "Document Similarity\n\n\nCode\nkorea_string <- korea %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \ngermany_string <- germany %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \nfinland_string <- finland %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \njapan_string <- japan %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \nuk_string <- uk %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \naustrailia_string <- austrailia %>% \n    dplyr::select(2) %>%\n    unlist() %>% \n    as.vector() %>% \n    str_c(.,collapse = \" \") \n\n\n\n\nCode\nimport matplotlib.pylab as plt \nimport matplotlib as mpl\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display, Markdown\nfrom tabulate import tabulate\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport warnings\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport string\n\n\n\n\nCode\nmpl.rc('font', family='NanumGothic') # 폰트 설정\nmpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정\n\n# 차트 스타일 설정\nsns.set(font=\"NanumGothic\", rc={\"axes.unicode_minus\":False}, style='white')\nwarnings.filterwarnings(\"ignore\")\n\n\n\nConvert data from R objects\n\n\nCode\ndocument_df = pd.DataFrame.from_dict(\n    {'filename': [\"Korea\", \"Germany\", \"Finland\", \"Japan\", \"UK\", \"Austrailia\"],\n    'opinion_text':[r.korea_string, r.germany_string, r.finland_string, \n    r.japan_string, r.uk_string, r.austrailia_string]})\n\n\n\n\nTokenizer\n\n\nCode\n# 단어 원형 추출 함수\nlemmar = WordNetLemmatizer()\n\ndef LemTokens(tokens):\n    return [lemmar.lemmatize(token) for token in tokens]\n\n# 특수 문자 사전 생성: {33: None ...}\n# ord(): 아스키 코드 생성\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n\n# 특수 문자 제거 및 단어 원형 추출\ndef LemNormalize(text):\n    text_new = text.lower().translate(remove_punct_dict)\n    word_tokens = nltk.word_tokenize(text_new)\n    \n    return LemTokens(word_tokens)\n\n# nltk.download('punkt')\n# nltk.download('wordnet')\n# nltk.download('omw-1.4')\ntfidf_vect = TfidfVectorizer(stop_words='english' , ngram_range=(1,2), \n                             tokenizer = LemNormalize, min_df=0.05, max_df=0.85)\n\nfeature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])    \n\n\n\n\nCosine Similarity\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_pair = cosine_similarity(feature_vect[0] , feature_vect)\ndocument_df[\"similarity\"] = similarity_pair.reshape(-1,1)\n\n\n\n\nVisualization\n\n\nCode\nplt.clf()\nvis=document_df.iloc[1:].sort_values(by=\"similarity\",ascending=False).reset_index(drop=True)\nplt.style.use(\"seaborn-pastel\")\nplt.figure(figsize = (10,7))\nsns.barplot(x=\"similarity\", y=\"filename\", data=vis)\nplt.title('Korea')"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#summary-of-vnr-analysis-results-in-korea",
    "href": "posts/EBPPM LAB/index.html#summary-of-vnr-analysis-results-in-korea",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "Summary of VNR Analysis Results in Korea",
    "text": "Summary of VNR Analysis Results in Korea\n\nThe major keywords used in the VNR report of Korea are classified into six topics:‘International Cooperation’, ‘Welfare’, ‘Human Rights’, and ‘Government Policy’, ‘Sustainable Development’ and ‘Integrated Goal-Setting’\nLooking at the analysis results of the network within and inter topics of 6 topics, it can be concluded that SDGs in Korea are being implemented centered on the connectivity between ‘sustainable development’ and ‘government policy’\nAlthough topics such as ‘Human Rights’, ‘Welfare’ are formed, it can be judged that the government-led policy does not receive attention sufficiently since those topics shows minimal connection with sustainable development policies"
  },
  {
    "objectID": "posts/EBPPM LAB/index.html#vnr-comparison-analysis-summary-with-korea-and-other-countries",
    "href": "posts/EBPPM LAB/index.html#vnr-comparison-analysis-summary-with-korea-and-other-countries",
    "title": "Textmining on UN SDG Voluntary National Review: 6 Countries",
    "section": "VNR Comparison Analysis Summary with Korea and Other Countries",
    "text": "VNR Comparison Analysis Summary with Korea and Other Countries\n\nComparing the results of VNR analysis in Korea with other countries, the same-word occurrence frequency correlation showed a relatively high correlation with Finland (0.8) and the UK (0.69), while Germany (0.58) and Australia (0.5) showed a relatively low correlation\nUnlike Korea, Finland operate the SDGs that social innovation played a major role in the establishment and implementation of SDGs policy goals, local community played a large role in establishing and implementing in Germany, Japan’s discussion on civil rights was more important than Korea\nConclusion 1: Deriving key features related to current SDGs in Korea\n\nAs a result of the analysis, it can be judged that the policy direction for SDGs in Korea is going in the right direction, such as carrying out the ‘integrated goal setting’ process to achieve ‘sustainable development’\nHowever, the biggest difference between Korea and other countries is that majority of the government policy-making and enforcement of the SDGs were led by the government, whereas influence of other actors such as ‘community’, ‘social innovation’, and ‘governance’ is weak\nIn conclusion, it is necessary to develop policies in a way that diversifies the measures of adopting them and attains sustainable development through cooperation with other actors rather than the direction of the government itself\n\nConclusion 2: Proposing the policy direction of SDGs in Korea\n\nAs a final conclusion of this analysis, it can be judged that the establishment and implementation of the SDGs of the Korea should adopt approaches of German, Britain and Austrailian models to take their advantages\nAs shown in the above analysis, the main characteristic of SDGs in these countries and the most different from that of Korea is the role of the ‘community’\nGermany’s “community” topic shows strong connectivity in the center along with ‘international cooperation’ and ‘government policy’\nThis can be inferred that the ‘community’ in the German SDGs discussion not only emphasizes the role as a community or as a part of the community in the city, but discusses the relationship between urban and rural areas, central and local areas\nOn the other hand, in Korea, the government-centered development that has extended since the 1960s and achieved rapid growth, but from the perspective of SDGs, it is no longer efficient nor effective in maintaining the development only resorting to pure government’s capacity\nIn order to pursue and attain sustainable development, it is imperative to construct international diplomatic relationships, central-local relationships prior to establishing and implementing SDGs goals\nIn conclusion, while maintaining the mediating and organizing role of the centralized government, it is necessary to diverge from the current SDGs structure in which all detailed topics are converges to the one centalized entity. Instead, promoting policy development in the direction of strengthening cooperation outward with other countries and strengthening connectivity inward with local communities is the most effective and efficient way to achieve sustainable development goals."
  },
  {
    "objectID": "posts/EDA1/EDA1.html",
    "href": "posts/EDA1/EDA1.html",
    "title": "EDA Assignment 1: Chapter 3",
    "section": "",
    "text": "3장. 줄기와 잎"
  },
  {
    "objectID": "posts/EDA1/EDA1.html#가-간헐천의-분출지속시간과-대기시간에는-유의미한-상관관계가-존재하는가",
    "href": "posts/EDA1/EDA1.html#가-간헐천의-분출지속시간과-대기시간에는-유의미한-상관관계가-존재하는가",
    "title": "EDA Assignment 1: Chapter 3",
    "section": "(가) 간헐천의 분출지속시간과 대기시간에는 유의미한 상관관계가 존재하는가?",
    "text": "(가) 간헐천의 분출지속시간과 대기시간에는 유의미한 상관관계가 존재하는가?\n\nplot(geyser$duration,geyser$waiting)\n\n\n\n\nBox Plot을 활용해 두개의 그래프를 그려 보았으나 Lower Fence와 Upper Fence를 벗어난 이상치는 발견되지 않았다."
  },
  {
    "objectID": "posts/EDA2/과제2.html",
    "href": "posts/EDA2/과제2.html",
    "title": "EDA Assignment 2: Chapter 4",
    "section": "",
    "text": "4장. 수치요약과 상자그림, 여러 그룹의 비교\n자료를 분석할 때 모든 부분을 R로 할 수도 없는 경우도 있다. R에서 이용할 수 있는 것들은 이용하되 수작업이 필요한 부분은 수작업으로 하여야 한다. 곱하기 나누기 등 간단한 계산은 계산기를 사용할 수도 있다."
  },
  {
    "objectID": "posts/EDA2/과제2.html#inner-fence와-outer-fence값들을-구하여라.",
    "href": "posts/EDA2/과제2.html#inner-fence와-outer-fence값들을-구하여라.",
    "title": "EDA Assignment 2: Chapter 4",
    "section": "(1) inner fence와 outer fence값들을 구하여라.",
    "text": "(1) inner fence와 outer fence값들을 구하여라.\n\ndata(islands)\nhead(islands)\n\n      Africa   Antarctica         Asia    Australia Axel Heiberg       Baffin \n       11506         5500        16988         2968           16          184 \n\nf=fivenum(islands)\nHl=as.numeric(f[2])\nM=as.numeric(f[3])\nHu=as.numeric(f[4])\nhspread=as.numeric(Hu-Hl)\nhspread\n\n[1] 163.5\n\n# Inner Fence\nIF=c(Hl-1.5*hspread,Hu+1.5*hspread)\nIF\n\n[1] -225.25  428.75\n\n# Outer Fence\nOF=c(Hl-3*hspread,Hu+3*hspread)\nOF\n\n[1] -470.5  674.0\n\nSKEW=((Hu-M)-(M-Hl))/((Hu-M)+(M-Hl))\nSKEW # +이기에 Skewed to the right\n\n[1] 0.7431193"
  },
  {
    "objectID": "posts/EDA2/과제2.html#letter-value-display와-boxplot을-만들고-자료의-특성을-요약하여라.",
    "href": "posts/EDA2/과제2.html#letter-value-display와-boxplot을-만들고-자료의-특성을-요약하여라.",
    "title": "EDA Assignment 2: Chapter 4",
    "section": "(2) letter value display와 boxplot을 만들고 자료의 특성을 요약하여라.",
    "text": "(2) letter value display와 boxplot을 만들고 자료의 특성을 요약하여라.\n\n# letter value display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\nlvd=lsum(islands)\nlvd\n\n                 letter depth lower     mid   upper  spread\nIceland               M  24.5  41.0   41.00    41.0     0.0\nTierra del Fuego      H  12.5  20.0  101.75   183.5   163.5\nTaiwan                E   6.5  14.5 1685.50  3356.5  3342.0\nPrince of Wales       D   3.5  13.0 4052.75  8092.5  8079.5\nHainan                C   2.0  13.0 5759.50 11506.0 11493.0\n\nlvd[,4]\n\n[1]   41.00  101.75 1685.50 4052.75 5759.50\n\n# boxplot\nboxplot(islands,horizontal=T,main=\"Boxplot of islands data\")\n\n\n\nboxplot(log(islands))\n\n\n\n\n\nletter value display median의 값은 41, Upper Hinge의 값은 183.5, Lower Hinge의 값은 20으로 확인 됨. depth가 24.5까지 들어가고 있다. H-Spread의 값은 163.5이고 이를 통해 계산한 Pseudo Variance값은 163.5/1.35=121.11 로 계산된다. mid 값이 41, 101.75, 1685.50, 4052.75, 5759.50 순으로 증가하고 있기 때문에 이 자료의 분포는 Skewed to the right이라고 생각하면 좋다.\nbox plot Symmetric하지 않고, Skewed to the right 하여 극단적으로 큰 값들이 존재한다는 것을 존재한다는 것은 Box Plot을 통해 재확인할 수 있다. Median의 위치는 매우 작아서 확인이 불가능하다. Spread는 약 11000 정도인 것으로 보이며 Whisker의 크기도 거의 확인할 수가 없다. 위의 문제에서 계산한 Outer Fence의 값은 -470과 674인데 Outer Fence밖의 Far Out(Outliers)값이 8개 존재하는 것을 확인 할 수 있다. 로그변환이나 지수변환을 통해서 데이터들을 Symmetric하게 만들고 Spread를 확인할 수 있게 만들어 데이터를 조금 더 해석하기 용이하게 해야 한다."
  },
  {
    "objectID": "posts/EDA3/과제3.html",
    "href": "posts/EDA3/과제3.html",
    "title": "EDA Assignment 3: Chapter 5",
    "section": "",
    "text": "5장. 자료의 재표현"
  },
  {
    "objectID": "posts/EDA3/과제3.html#크롤링을-통해-삼성전자-주식가격-데이터-확인",
    "href": "posts/EDA3/과제3.html#크롤링을-통해-삼성전자-주식가격-데이터-확인",
    "title": "EDA Assignment 3: Chapter 5",
    "section": "크롤링을 통해 삼성전자 주식가격 데이터 확인",
    "text": "크롤링을 통해 삼성전자 주식가격 데이터 확인\n\npacman::p_load(\"rvest\",\"httr\",\"lubridate\",\"R6\",\"tidyverse\",\n               \"xml2\",\"stringr\",\"readr\",\"tqk\",\"XML\")\nremotes::install_github(\"mrchypark/tqk\")\n\nSkipping install of 'tqk' from a github remote, the SHA1 (79997bbe) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\ncode_samsung=function () \n{    . <- MKT_TP_NM <- ISU_ABBRV <- ISU_SRT_CD <- ISU_NM <- ISU_ENG_NM <- ISU_CD <- NULL\n    \"http://data.krx.co.kr/comm/bldAttendant/getJsonData.cmd\" %>% \n        httr::POST(body = list(bld = \"dbms/MDC/STAT/standard/MDCSTAT01901\", \n            locale = \"ko_KR\", mktId = \"ALL\", share = 1, csvxls_isNo = \"false\"), \n            encode = \"form\") %>% \n        httr::content(\"text\") %>% \n        jsonlite::fromJSON() %>% \n        .$OutBlock_1 %>% \n        tibble() %>% \n        dplyr::transmute(\n            market = MKT_TP_NM, \n            name = ISU_ABBRV, \n            code = ISU_SRT_CD, \n            name_full = ISU_NM,\n            name_eng = ISU_ENG_NM, \n            code_full = ISU_CD) %>%\n    \n        dplyr::filter(stringr::str_detect(name,pattern=\"삼성+\")) %>% \n        return() \n}\n\n\n# 삼성전자 코드\ncode_samsung()\n\n# A tibble: 24 × 6\n   market name              code   name_full                 name_eng    code_…¹\n   <chr>  <chr>             <chr>  <chr>                     <chr>       <chr>  \n 1 KOSPI  삼성SDI우         006405 삼성SDI1우선주            SAMSUNG SD… KR7006…\n 2 KOSPI  삼성SDI           006400 삼성SDI보통주             SAMSUNG SD… KR7006…\n 3 KOSPI  삼성공조          006660 삼성공조보통주            SamsungCli… KR7006…\n 4 KOSDAQ 삼성스팩4호       377630 삼성기업인수목적4호       SAMSUNG SP… KR7377…\n 5 KOSDAQ 삼성스팩6호       425290 삼성기업인수목적6호       SAMSUNG SP… KR7425…\n 6 KOSDAQ 삼성머스트스팩5호 380320 삼성머스트기업인수목적5호 SAMSUNG MU… KR7380…\n 7 KOSPI  삼성물산우B       02826K 삼성물산1우선주(신형)     SAMSUNG C&… KR7028…\n 8 KOSPI  삼성물산          028260 삼성물산보통주            SAMSUNG C&… KR7028…\n 9 KOSPI  삼성바이오로직스  207940 삼성바이오로직스보통주    SAMSUNG BI… KR7207…\n10 KOSPI  삼성생명          032830 삼성생명보험보통주        Samsung Li… KR7032…\n# … with 14 more rows, and abbreviated variable name ¹​code_full\n\nsamsung <- code_samsung() %>% \n    select(name,code) %>% \n    filter(name==\"삼성전자\")\n\n삼성전자 코드는 005930\n\nsamsung$code\n\n[1] \"005930\"\n\nurl=\"https://fchart.stock.naver.com/sise.nhn?timeframe=day&count=6000&requestType=0&symbol=\"\nsamsung_data=read_xml(glue::glue({url},samsung$code))\n\nsplitting=function(a){\n    result=str_split(a,pattern=\"\\\\|\",simplify=T)\n    result=data.frame(result)\n    names(result)=c('date','시가','고가','저가','종가',\"거래량\")\n    return(result)}\n\nsamsung <- samsung_data %>%\n    xmlParse(useInternalNodes = TRUE) %>% \n    xmlToList() %>% \n    plyr::ldply(tibble) %>%\n    select(data1=\"<named list>\") %>% \n    unlist() %>% \n    tibble(data1=as.character(.)) %>% \n    select(data1) %>% \n    do(head(., nrow(.)-6)) %>% \n    transmute(data2=map(data1,splitting)) %>% \n    unnest(data2) %>% \n    mutate_all(.funs=as.numeric) %>% \n    mutate(date=ymd(date)) %>% \n    filter(date>=ymd(20200101) & date<=ymd(20210406))\n\nlibrary(ggplot2)\nggplot(samsung, aes(x = date, y = 시가)) +\n    theme_bw()+\n    geom_line(size=1, color = \"blue\") + \n    ggtitle(\"삼성전자 시가 변화 추이\") +\n    ggeasy::easy_center_title()\n\n\n\n# install.packages(\"reshape\")\nlibrary(reshape)\nsamsung1=samsung %>% \n    select(-거래량) %>% \n    pivot_longer(!date,names_to=\"variable\",values_to=\"value\")\n\n# 주가 추이 시각화\nggplot(samsung1, aes(x = date, y = value)) +\n    theme_minimal()+\n    geom_line(size=1, aes(colour=variable)) + \n    ggtitle(\"삼성전자 주가 변화 추이\")+\n    scale_color_discrete(name=\"주가 유형\")+\n    labs(y=\"주가\",x=\"거래일\")+\n    scale_x_date(date_breaks = \"months\" , date_labels = \"%b/%y\")+\n    scale_y_continuous(limits = c(40000,96000), breaks=seq(40000,96000,5000))+\n    ggeasy::easy_center_title()+\n    ggeasy::easy_rotate_x_labels()\n\n\n\nmax(samsung$고가)-min(samsung$저가)\n\n[1] 54500\n\nfivenum(samsung$시가)[4]-fivenum(samsung$시가)[2]\n\n[1] 18450\n\n18450/1.35\n\n[1] 13666.67\n\n\n고점과 저점 비교시 54500 정도의 변동성 존재\nhspread: 18450정도이고 pseudo-sigma 13666.67 정도. 한국 코스피 변동성을 계산하여 표준편차를 확인하고 이를 계산된 변동성과 비교한다면 삼성전자 주식의 안정성에 대해서 예측해볼 수 있다. (단순히 상승했다, 상승하지 않았다 정도의 지식에 더해 주식의 변동성을 이해한다면 안정적인 주식인지 안정적이지 않은 주식인지 확인할 수 있다.)\n이후 시계열 분석을 활용해 추가적인 분석이 요구됨\n주식 가격을 인터넷에서 찾아 줄기잎 전시와 상자그림을 그려 대칭성을 점검 - 주식 가격은 시가를 기준으로 파악한다.\n\n#Stem and Leaf\nstem(samsung$시가)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  42 | 69\n  44 | 2\n  46 | 24913455588\n  48 | 00447778899000001244466677899\n  50 | 0011238800124489\n  52 | 0111125568012346789\n  54 | 001233344567788890122355678888\n  56 | 000112223344445701123445666778899\n  58 | 0002223344788899001111234444556678889\n  60 | 000123333344455577777789900011223689\n  62 | 009\n  64 | 0187\n  66 | 10199\n  68 | 044\n  70 | 14\n  72 | 124457134568\n  74 | 11\n  76 | 4\n  78 | 80\n  80 | 0000124466778\n  82 | 0123445666888889112233558889\n  84 | 0001555818\n  86 | 26605\n  88 | 780088\n  90 | 03\n\n#Boxplot\nggplot(samsung1, aes(x = variable, y = value))+\n    ggtitle(\"삼성전자 주가 변화 추이\")+\n    geom_boxplot(aes(fill=variable))+\n    labs(y=\"주가\",x='주가 유형')+\n    theme_minimal()+\n    ggeasy::easy_center_title()\n\n\n\n\n박스플롯 상으로는 자료들의 대칭성을 어느정도 확인할 수 있다. 특이값이 없는 대신 큰쪽의 Whisker의 길이가 작은쪽의 Whisker의 길이에 비해서 긴 것으로 확인된다. 또한 Median의 값이 Lower-hinge 쪽에 더 가까운 것 이 확인되기 때문에 Skewed to the right에 해당하는 분포로 보인다. Stem-leaf Display에서는 두개의 cluster를 확인할 수 있는데, stem 58을 기준으로한 하나의 cluster와 stem 82를 기준으로 한 또 하나의 cluster가 있다. 작은 쪽의 cluster가 더 큰쪽의 cluster에 비해서 더 많은 숫자들을 가지고 있는 것을 확인할 수 있다.\n\nskewness = function(x) {\n  hl=fivenum(x)[2]\n  median=fivenum(x)[3]\n  hu=fivenum(x)[4]\n  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))\n  return(skew)\n}\nskewness(samsung$시가)\n\n[1] 0.4634146\n\n\nA measure of symmetry로 skewness 값을 계산할 경우 0.46정도로 skewed to the right 되어 있음을 확인할 수 있다.\nSkewness를 조절하기 위해서 수업시간에 제시한 4가지 방법들을 적용해보고자 한다.\n\n# Log\nstem(log(samsung$시가))\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  106 | 69\n  107 | 04\n  107 | 566677777788999999\n  108 | 000000000011111111122222223444444\n  108 | 55666666667777888899999\n  109 | 000000000111111112222222333333333334444444444\n  109 | 555556666666666677777777778888888899999999999999\n  110 | 000000000001111111111111111112222222222233333\n  110 | 77789\n  111 | 01133333\n  111 | 66999999\n  112 | 00011111\n  112 | 678\n  113 | 000000111111112222222222222233333333344444444444\n  113 | 5566777899\n  114 | 001111\n\nboxplot(log(samsung$시가))\n\n\n\n# sqrt\nstem(sqrt(samsung$시가))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  20 | 69\n  21 | 0\n  21 | 5577788889999\n  22 | 001111111111112222233333333444444\n  22 | 5566667788888888999\n  23 | 000111222222333333334444444\n  23 | 5555566666666777777777777778899999\n  24 | 000000000011111111112222223333333334444444444\n  24 | 555555555666666666666666667777777777889999\n  25 | 333\n  25 | 56799\n  26 | 11122\n  26 | 5599999\n  27 | 001111222\n  27 | 8\n  28 | 11\n  28 | 5555555566666677777777788888888889999999\n  29 | 00000111123444\n  29 | 568888\n  30 | 0000\n\nboxplot(sqrt(samsung$시가))\n\n\n\n# minus inverse\nstem(-1/(samsung$시가))\n\n\n  The decimal point is 6 digit(s) to the left of the |\n\n  -23 | 5\n  -22 | 87\n  -21 | 663321111\n  -20 | 99987765555444444443332222111100000\n  -19 | 997766655533222222000\n  -18 | 998887766655554444433332222211110000\n  -17 | 99999998888888777776555554444443333332222222211000000\n  -16 | 99999999888888888777777766666666665555555554444444433322211\n  -15 | 666421\n  -14 | 997776632\n  -13 | 99888876666655\n  -12 | 9773333333333222222211111111111100000000\n  -11 | 999999998888876555433221111\n\nboxplot(-1/(samsung$시가))\n\n\n\n# minus inverse of sqrt\nstem(-1/sqrt((samsung$시가)))\n\n\n  The decimal point is 4 digit(s) to the left of the |\n\n  -48 | 5\n  -47 | 86\n  -46 | 54210\n  -45 | 999988765533333222222211000\n  -44 | 9999988877776644332211\n  -43 | 9998888866654443322110000\n  -42 | 999998888777766665444433333332222211111110\n  -41 | 9888877777766666655555544443222222211111100000000\n  -40 | 9999988888777777777776666666555555544432222\n  -39 | 65530\n  -38 | 96644322\n  -37 | 872222110\n  -36 | 9999877\n  -35 | 9661111110000000\n  -34 | 99998888888888877777666655555555444331100\n  -33 | 9866554433\n\nboxplot(-1/sqrt((samsung$시가)))\n\n\n\n\nSkewness 값을 변환전후로 비교\n\nskewness(samsung$시가)\n\n[1] 0.4634146\n\nskewness(log(samsung$시가))\n\n[1] 0.4031337\n\nskewness(sqrt(samsung$시가))\n\n[1] 0.433612\n\nskewness(-1/(samsung$시가)) # 가장 낮은 수준의 Skewness를 보여줌\n\n[1] 0.3404237\n\nskewness(-1/sqrt(samsung$시가))\n\n[1] 0.372047\n\n\n일반적으로 사용하는 변환으로는 만족스러운 수준으로 skewness 값이 감소되었는지 알수가 없었다. 따라서 skewness 값을 감소시키기 위해 함수를 활용하여 가능한 여러 경우의 값들을 비교해보고자 한다. (skewness값은 0일때 최소가 되고, 이때 자료가 대칭적이 되었다고 가정할 수 있다.)\n\ns=c()\nstart=samsung$시가\nfor (i in seq(-10,10,0.1)) {\n  if(i>0){\n    s = c(s,skewness(start^i))\n  }\n  \n  else if(i==0){\n    s = c(s,skewness(log(start)))\n  }\n  \n  else{\n    s = c(s,skewness(-(start^i)))\n  }\n}\ny=cbind(seq(-10,10,0.1),s)\nplot(y,xlab=\"Power Value\",ylab=\"Skewness\")\nabline(h=0)\n\n\n\n# -6정도일때 Skewness 자체는 최소가 된다. \nboxplot(-(start^-6))\n\n\n\n\nskewness공식은 ((Hu-M)-(M-Hl))/((Hu-M)+(M-Hl))이다.\nskewness의 공식이 H-Spread 범위의 Lower Hinge Upper Hinge 등에 의해서 만들어지기 때문에 i=-6을 활용할 경우 Spread 내부의 Symmery는 보정이 되었지만 Whisker의 길이가 오히려 길어졌다는 점에서 처음에 사용하였던 기존의 변환을 활용해보고자 한다.\n\npar(mfrow=c(2,3))\nboxplot(log(samsung$시가))\nboxplot(sqrt(samsung$시가))\nboxplot(-1/sqrt((samsung$시가)))\nboxplot(-1/(samsung$시가)) # 선택\nboxplot(-1/(samsung$시가)^2)\nboxplot(-1/(samsung$시가)^3)\n\n\n\n\n다시한번 확인해보면, H-spread 내의 대칭성 정도 및 Whisker의 길이 까지 고려하여 가장 적절한 변환을 선택한다면 minus inverse 변환이나 minus sqrt inverse 변환 둘중에서 선택하면 된다고 본다. skewness가 더 0에 가까운 minus inverse 변환을 활용해서 데이터를 변환한다면 H-spread내 Hinge와 Median의 대칭성 그리고 whisker들의 대칭성 모두 다 확보할수 있다고 생각한다.\n\ndecided=-1/(samsung$시가)\nhist(decided,breaks=20) # 그럼에도 불구하고 stem and leaf display에서 확인하였던 오른쪽의 두번째 Cluster의 흔적은 사라지지 않고 남아 있다.\nabline(v=median(decided),col=\"red\")"
  },
  {
    "objectID": "posts/EDA4/과제4.html",
    "href": "posts/EDA4/과제4.html",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "",
    "text": "6장. 확률 플롯의 활용"
  },
  {
    "objectID": "posts/EDA4/과제4.html#번",
    "href": "posts/EDA4/과제4.html#번",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "1번)",
    "text": "1번)\n\n# rivers data\n\ndata(rivers)\nstem(rivers)\n\n\n  The decimal point is 2 digit(s) to the right of the |\n\n   0 | 4\n   2 | 011223334555566667778888899900001111223333344455555666688888999\n   4 | 111222333445566779001233344567\n   6 | 000112233578012234468\n   8 | 045790018\n  10 | 04507\n  12 | 1471\n  14 | 56\n  16 | 7\n  18 | 9\n  20 | \n  22 | 25\n  24 | 3\n  26 | \n  28 | \n  30 | \n  32 | \n  34 | \n  36 | 1\n\nrivers.sort <- sort(rivers)\n(n.rivers <- length(rivers))\n\n[1] 141\n\ni <- 1:n.rivers\n\nhist(rivers,breaks=20,probability = T,col=\"pink\")\nlines(density(rivers),col=\"red\",lwd=2)\n\n\n\n# 평균과 왜도,첨도 확인\nboxplot(rivers)\n\n\n\n(mean.rivers <- mean(rivers))\n\n[1] 591.1844\n\n(var.rivers <- var(rivers))\n\n[1] 243908.4\n\n(shape.rivers <- mean.rivers^2/var.rivers)\n\n[1] 1.432911\n\n(scale.rivers <- mean.rivers/var.rivers)\n\n[1] 0.002423797\n\nq.gamma.rivers <- qgamma((i-0.5)/n.rivers, shape.rivers, scale.rivers)\n\nplot(q.gamma.rivers, rivers.sort, main=\"Gamma prob plot\")\n\n\n\n# clutter near the lower left part --> HOMEWORK\n# 3승근변환\nplot(q.gamma.rivers^(1/3), rivers.sort^(1/3), main=\"Gamma prob plot\",sub=\"reexpression; power=1/3\")\n\n\n\n# 3승근 변환을 하기 전의 자료가 왼쪽 아래에 직선으로 몰려있는 것 처럼 보였으나 3승근 변환을 하고 나니 대각선 직선에서 멀어져 실제로는 감마분포로부터 데이터가 오히려 멀어 진 것으로 보인다.\n# 따라서 Rivers Data는 감마분포를 따르고 있지 않다."
  },
  {
    "objectID": "posts/EDA4/과제4.html#번-1",
    "href": "posts/EDA4/과제4.html#번-1",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "2번)",
    "text": "2번)\n\n# Try to fit a theoretical distribution to log(rivers) data \nlogrivers=log(rivers)\n\nlogrivers.sort <- sort(logrivers)\n(n.logrivers <- length(logrivers))\n\n[1] 141\n\ni <- 1:n.logrivers\n\n# skewed to the right\nstem(logrivers)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  48 | 1\n  50 | \n  52 | 15578\n  54 | 44571222466689\n  56 | 023334677000124455789\n  58 | 00122366666999933445777\n  60 | 122445567800133459\n  62 | 112666799035\n  64 | 00011334581257889\n  66 | 003683579\n  68 | 0019156\n  70 | 079357\n  72 | 89\n  74 | 84\n  76 | 56\n  78 | 4\n  80 | \n  82 | 2\n\nhist(logrivers.sort,breaks=20,probability = T , col=\"pink\",main=\"Histogram of Log(rivers)\")\nlines(density(logrivers),col=\"red\",lwd=2)\n\n\n\nboxplot(logrivers.sort,main=\"Boxplot of log(rivers)\")\n\n\n\n# 지수분포\nq.exp.logrivers <- -log(1-(i-0.5)/n.logrivers)\nplot(q.exp.logrivers, logrivers.sort, main=\"Exponential prob plot\")\nabline(5.5,0.7,col=\"orange\",lwd=2)\nabline(5.5,0.65,col=\"blue\",lwd=2)\nabline(5.5,0.6,col=\"red\",lwd=2)\n\n\n\nline(qqplot(q.exp.logrivers,logrivers.sort))[2]\n\n\n\n\n$coefficients\n[1] 5.5451592 0.6899594\n\n# 왼쪽 아래에 데이터가 몰려있어서 정확하게 파악이 어려움 / 삼승근 변환 사용\n#\nplot(q.exp.logrivers^(1/3), logrivers.sort^(1/3), main=\"Exponential prob plot\",\n     sub=\"reexpression; power=1/3\")\nabline(1.6786397,0.1698099, col=\"red\",lwd=2)\n\n\n\nline(qqplot(q.exp.logrivers^(1/3),logrivers.sort^(1/3)))[2]\n\n\n\n\n$coefficients\n[1] 1.6786397 0.1698099\n\n# 데이터들이 직선 위에 잘 존재하는 것으로 보인다.\n\n# 와이블 분포\nq.weibull.logrivers <- log(q.exp.logrivers)\nplot(q.weibull.logrivers, log(logrivers.sort), main=\"Weibull prob plot\")\nabline(1.84728968, 0.07871442,col=\"red\",lwd=2)\n\n\n\nline(qqplot(q.weibull.logrivers,log(logrivers.sort)))[2]\n\n\n\n\n$coefficients\n[1] 1.84728968 0.07871442\n\n# 빨간 선을 따르고 있는 것처럼 보이지는 않는다. 데이터들이 곡선의 형태를 가지고 있기 때문에 와이블 분포를 따르고 있다고 보기 어렵다.\n# 점들이 직선에 분포하고 있지 않지만 와이블 분포의 shape를 나타내는 a와 scale을 나타내는 b의 값은 빨간 선을 기준으로 a=e^(intercept*(-b)),b=1/slope 이다.\n(b=1/0.7871442)\n\n[1] 1.270415\n\n(a=exp(1.84728968*(-b)))\n\n[1] 0.09567244\n\n# 감마분포\n(mean.logrivers <- mean(logrivers))\n\n[1] 6.175879\n\n(var.logrivers <- var(logrivers))\n\n[1] 0.3498534\n\n(shape.logrivers <- mean.logrivers^2/var.logrivers)\n\n[1] 109.0213\n\n(scale.logrivers <- mean.logrivers/var.logrivers)\n\n[1] 17.65276\n\nq.gamma.logrivers <- qgamma((i-0.5)/n.logrivers, shape.logrivers, scale.logrivers)\nplot(q.gamma.logrivers, logrivers.sort, main=\"Gamma prob plot\")\nabline(a=0.14058180, b=0.9718083,col=\"blue\",lwd=1)\nabline(a=0,b=1,col=\"red\",lwd=2)\n\n\n\nline(qqplot(q.gamma.logrivers, logrivers.sort))[2]\n\n\n\n\n$coefficients\n[1] 0.1405818 0.9718083\n\n# 감마분포에 log(rivers)데이터는 Fitting 되어지는 것으로 보인다. \n# 북아메리카의 강의 길이에 로그를 취한 데이터는 감마분포를 따르고 있는 것으로 보인다. \n# 비록 양 끝의 데이터가 다소 대각선에서 떨어져 있는 것으로 보이기는 하지만 대다수의 데이터가 직선 위에 존재하므로 그렇게 가정한다."
  },
  {
    "objectID": "posts/EDA4/과제4.html#section",
    "href": "posts/EDA4/과제4.html#section",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "1)",
    "text": "1)\n미터와 피트 단위로 예상한 거리가 각각 정규분포를 따르는지 검토하여라.\n\nreal_metre_mean=13.1\nreal_feet_mean=43.0\n\n# 극단값의 영향을 줄이기 위해 Pseudosigma값과 Median값을 활용하여 정규분포와 데이터의 분포를 비교하여 본다.\n\n\n# 미터기준 측정\nmetre=c(8,9,10,10,10,10,10,10,11,11,11,11,12,12,13,13,13,14,14,14,15,15,15,15,15,15,15,15,16,16,16,17,17,17,17,18,18,20,22,25,27,35,38,40)\nlength(metre)\n\n[1] 44\n\nstem(metre)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 89\n  1 | 000000111122333444\n  1 | 55555555666777788\n  2 | 02\n  2 | 57\n  3 | \n  3 | 58\n  4 | 0\n\nhist(metre,breaks=10,probability = T)\nlines(density(metre),lwd=2,col=\"orange\")\nk=seq(-100,100,0.01)\n(x=fivenum(metre))\n\n[1]  8 11 15 17 40\n\n(pseudosigmam = (x[4]-x[2])/1.34)\n\n[1] 4.477612\n\nt=dnorm(k,mean=fivenum(metre)[3],sd=pseudosigmam) #normal density\nlines(t~k,type='l',col=\"red\",lwd=3)\n\n\n\n# 극단값을 고려하지 않는다면 데이터의 미디안 근처의 값들은 정규분포를 잘 따르고 있는 것으로 보인다.\nboxplot(metre, main=\"Boxplot of metre estimation\") # 이상치 위에서 4개\n\n\n\n\n\n# 피트 기준 측정\nfeet=c(24,25,27,30,30,30,30,30,30,32,32,33,34,34,34,35,35,36,36,36,37,37,40,40,40,40,40,40,40,40,40,41,41,42,42,42,42,43,43,44,44,44,45,45,45,45,45,\n45,46,46,47,48,48,50,50,50,51,54,54,54,55,55,60,60,63,70,75,80,94)\nlength(feet)\n\n[1] 69\n\nstem(feet)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  2 | 457\n  3 | 0000002234445566677\n  4 | 0000000001122223344455555566788\n  5 | 000144455\n  6 | 003\n  7 | 05\n  8 | 0\n  9 | 4\n\nhist(feet,breaks=10,probability = T)\nlines(density(feet),lwd=2,col=\"brown\")\n(y=fivenum(feet))\n\n[1] 24 36 42 48 94\n\n(pseudosigmaf = (y[4]-y[2])/1.34)\n\n[1] 8.955224\n\nt2=dnorm(k,mean=fivenum(feet)[3],sd=pseudosigmaf) #normal density\nlines(t2~k,type='l',col=\"darkgreen\",lwd=3)\n\n\n\n# 극단값을 고려하지 않는다면 데이터의 미디안 근처의 값들은 정규분포를 잘 따르고 있는 것으로 보인다.\nboxplot(feet,main=\"Boxplot of feet estimation\")  # 이상치 위에서 4개\n\n\n\n\n\n# metre\nstem(metre)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 89\n  1 | 000000111122333444\n  1 | 55555555666777788\n  2 | 02\n  2 | 57\n  3 | \n  3 | 58\n  4 | 0\n\nqqnorm(metre,ylab=\"metre Qunatiles\",sub=\"red: actual qqline, blue= estimation by pseudosigma\")\nqqline(metre,col=\"red\",lwd=1,lty=2)\nx=fivenum(metre)\n(pseudosigmam = (x[4]-x[2])/1.34)\n\n[1] 4.477612\n\nabline(x[3],pseudosigmam,col=\"blue\",lwd=1,lty=2)\n\n\n\n# feet\nstem(feet)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  2 | 457\n  3 | 0000002234445566677\n  4 | 0000000001122223344455555566788\n  5 | 000144455\n  6 | 003\n  7 | 05\n  8 | 0\n  9 | 4\n\nqqnorm(feet,ylab=\"Feet Qunatiles\",sub=\"red: actual qqline, blue= estimation by pseudosigma\")\nqqline(feet,col=\"red\",lwd=1,lty=2)\ny=fivenum(feet)\n(pseudosigmaf = (y[4]-y[2])/1.34)\n\n[1] 8.955224\n\nabline(y[3],pseudosigmaf,col=\"blue\",lwd=1,lty=2)\n\n\n\n\n중앙지점의 점들은 미터측정이나 피트측정이나 정규분포를 잘 따르고 있는 것으로 보이나 양끝으로 갈수록 정규분포에서 벗어나고 있는 것으로 보인다. 따라서, 두 측정방식 모두 대각 직선에서 과도하게 벗어난 것으로 보이는 극단값들을 제외하고 다시 Plot을 그려보고자 한다.\n\n# metre 측정방식은 위에서 4개 제외\nadjusted_metre=metre[1:(length(metre)-4)]\n\nqqnorm(adjusted_metre,ylab=\"adjusted metre Qunatiles\",sub=\"red: actual qqline, blue= estimation by pseudosigma\")\nqqline(adjusted_metre,col=\"red\",lwd=1,lty=2)\n(x1=fivenum(adjusted_metre))\n\n[1]  8.0 11.0 14.5 16.0 25.0\n\n(pseudosigmaam= (x1[4]-x1[2])/1.34); (sd(adjusted_metre))\n\n[1] 3.731343\n\n\n[1] 3.624613\n\nabline(x1[3],pseudosigmaam,col=\"blue\",lwd=1,lty=2)\n\n\n\n# 4개를 제외한 데이터들을 기준으로 qqplot을 그렸을 때, 대부분의 데이터가 직선위에 있는 것으로 확인된다.\n# \nq=c(0,0.25,0.5,0.75)\nqnorm(q,mean(adjusted_metre),sd(adjusted_metre))\n\n[1]     -Inf 11.68024 14.12500 16.56976\n\nx1\n\n[1]  8.0 11.0 14.5 16.0 25.0\n\n# 1분위수, Median, 3분위수가 실제 평균과 분산을 활용한 정규분포의 값과 비슷하다는 점에서 이상치를 제외한 데이터들은 정규분포를 따르고 있다고 볼 수 있다.\n\n# Feet 측정방식도 위에서 4개 제외\nadjusted_feet=feet[1:(length(feet)-4)]\n\nqqnorm(adjusted_feet,ylab=\"adjusted feet Qunatiles\",sub=\"red: actual qqline, blue= estimation by pseudosigma\")\nqqline(adjusted_feet,col=\"red\",lwd=1,lty=2)\n(x2=fivenum(adjusted_feet))\n\n[1] 24 35 41 46 63\n\n(pseudosigmaaf= (x2[4]-x2[2])/1.34); (sd(adjusted_feet))\n\n[1] 8.208955\n\n\n[1] 8.642388\n\nabline(x2[3],pseudosigmaaf,col=\"blue\",lwd=1,lty=2)\n\n\n\n# 5개를 제외한 데이터들을 기준으로 qqplot을 그렸을 때, 대부분의 데이터가 직선위에 있는 것으로 확인된다.\nq=c(0,0.25,0.5,0.75)\nqnorm(q,mean(adjusted_feet),sd(adjusted_feet))\n\n[1]     -Inf 35.64772 41.47692 47.30613\n\nx2\n\n[1] 24 35 41 46 63\n\n# 1분위수, Median, 3분위수가 실제 평균과 분산을 활용한 정규분포의 값과 비슷하다는 점에서 이상치를 제외한 데이터들은 정규분포를 따르고 있다고 볼 수 있다.\n\n# 다만 이상치를 제외한 분석은 자료를 전부 사용하지는 못한다는 점에서 한계가 있다. 데이터 분석에 있어서 이상치를 제외하지 말고 분석하는 법을 고민해야 한다. \n\n(length(metre)-4)/length(metre)\n\n[1] 0.9090909\n\n(length(feet)-4)/length(feet)\n\n[1] 0.942029\n\n\n두 데이터들을 전부 활용하지는 못했지만 metre측정의 경우 90%, feet측정의 경우 94%에 해당하는 자료들을 활용해 분석할 경우 정규분포를 따르고 있는 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/EDA4/과제4.html#section-1",
    "href": "posts/EDA4/과제4.html#section-1",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "2)",
    "text": "2)\n두 자료가 같은 분포를 하는지 검토하여라. 만약 같다면 각각의 평균과 분산을 qq plot과 표본에서 구하여라.\n\n# qqplot with original data\nqqplot(metre,feet,xlab=\"metre estimation\",ylab=\"feet estimation\",\n       main=\"Q-Q P3lot with original data\");\n\n# Trial Error 과정을 거쳐 적절한 직선을 예측해보자\nabline(14,2)\nabline(15,1.7)\nabline(15,1.5)\n\n\n\n# Tukey's robust line estimation\n# qqline 식을 그릴경우 대부분의 데이터는 직선위에 있으나 위에서부터 5개의 데이터는 직선위에서 과도하게 벗어나 있다. 따라서 이 5개의 데이터는 이상치라고 보는 것이 적절해 보인다. \nabline(line(qqplot(metre,feet,xlab=\"metre estimation\",ylab=\"feet estimation\",\n                   main=\"Q-Q Plot with original data\")),col=\"blue\",lwd=2)\nabline(14,2)\nabline(15,1.7)\nabline(15,1.5)\n\n\n\n# Tukey's robust line estimation이 가장 정확해보인다.\n\npar(mfrow=c(1,2))\nhist(feet,breaks=10,probability = T)\nlines(density(feet),col=\"yellow\",lwd=2)\nhist(metre,breaks=10,probability = T)\nlines(density(metre),col=\"green\",lwd=2)\n\n\n\n\n두 자료 모두 정규분포를 따르고 있지는 않은 것으로 보인다. Skewed to the right 되어 있기 때문에 상당히 퍼져있다.다른말로 하면 두 분포 모두 이상치로 인해서 분포가 왜곡되어있을 가능성이 상당히 높다. 실제로 Stem and leaf plot을 통해 확인할 경우 이상치의 가능성이 확인되고 있다. (두 데이터들은 Discrete한 데이터이기 때문에 density를 나타낸 곡선은 참고만 한다.)\n\nstem(feet)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  2 | 457\n  3 | 0000002234445566677\n  4 | 0000000001122223344455555566788\n  5 | 000144455\n  6 | 003\n  7 | 05\n  8 | 0\n  9 | 4\n\nstem(metre)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 89\n  1 | 000000111122333444\n  1 | 55555555666777788\n  2 | 02\n  2 | 57\n  3 | \n  3 | 58\n  4 | 0\n\nboxplot(metre,main=\"metre boxplot\")\n\n\n\nboxplot(feet,main=\"feet boxplot\")\n\n\n\npar(mfrow=c(1,1))\n# Box plot을 통해 확인할 경우 Outer-Fence범위를 벗어난 극단치가 각각 4개씩 발견되고 있다. \n\n# 그렇다면 feet 데이터를 metre데이터로 변환하고 두 데이터를 합친 qqplot을 그린다면 어떨까?\n# install.packages(\"measurements\")\nlibrary(measurements)\nfeet_to_metre=conv_unit(feet,\"feet\",\"m\")\nleng=c(metre,feet_to_metre)\nqqnorm(leng);qqline(leng,col=\"red\",lwd=2)\n\n\n\n# 혼합분포를 따른다고 보기 어렵다. 교과서 114pg를 참고할 경우 혼합집단으로 생성된 자료의 경우 직선식을 만들기보다는 s자 곡선 혹은 역 s자 곡선을 만드는 경우가 많은데 해당 자료는 이상치들을 제외할경우 대부분의 자료들이 직선식 위에 있다고 볼수 있다.\n# 그러므로 두 자료 모두 동일한 정규분포에서 추출된 자료라고 보는 것이 타당하다.\n\n\n# Skewed to the right 되어 있음 / Outlier을 전부 포함한 분석이 진행된다면 두 데이터 모두 정규분포를 따른다고 보기 힘듦\nhist(leng,breaks=20,probability=T)\nlines(density(leng),col=\"darkgreen\",lwd=2)\n\n\n\n\n\n# qqplot with adjusted data\nlength(adjusted_metre);length(adjusted_feet)\n\n[1] 40\n\n\n[1] 65\n\nqqplot(adjusted_metre,adjusted_feet,xlab=\"metre estimation\",ylab=\"feet estimation\",\n       main=\"Q-Q Plot with adjusted data\");\nabline(line(qqplot(adjusted_metre,adjusted_feet,xlab=\"metre estimation\",ylab=\"feet estimation\",\n                   main=\"Q-Q Plot with adjusted data\")),col=\"#900020\",lwd=2)\n\n\n\n# outlier을 제외한 경우 모든 데이터가 직선식 위에 존재하는 것으로 보인다.\n\npar(mfrow=c(1,2))\nhist(adjusted_feet,breaks=10,probability = T)\nlines(density(adjusted_feet),col=\"yellow\",lwd=2)\nhist(adjusted_metre,breaks=10,probability = T)\nlines(density(adjusted_metre),col=\"green\",lwd=2)\n\n\n\n# 피트로 측정된 데이터는 Skewedness가 상당히 개선되었고, 미터의 경우에도 대칭성이 어느정도 확보되었다.\n\n# 피트로 측정된 데이터를 미터로 Conversion하고 데이터를 합친 후 정규 분포 QQplot위로 올려보았다.\nadjusted_feet_to_metre=conv_unit(adjusted_feet,\"feet\",\"m\")\nleng2=c(adjusted_metre,adjusted_feet_to_metre)\npar(mfrow=c(1,1))\nqqnorm(leng2);qqline(leng2,col=\"red\",lwd=2)\n\n\n\n# QQplot 하에서 양 극단의 일부 데이터를 제외한 나머지 데이터들은 전부 qqline위에 있다는 점에서 두 데이터는 동일한 정규분포에서 나타난 것을 확인할 수 있다.\n# 두 데이터는 동일한 모수를 가진 정규분포라고 추정할 수 있다. 그러나 눈대중으로 판단하는 것은 한계가 있기 때문에 이 판단을 재확인하기 위해서는 Tuckey의 Mean-Difference Plot을 활용해야한다. \n\nlibrary(lattice)\n\n# Outlier를 제외한 데이터를 Tuckey_Mean_Difference_Plot 위로 올려보고자 한다. \n\nqqplot(metre, feet_to_metre,xlim=c(min(metre, feet_to_metre),max(metre, feet_to_metre)),\n       ylim=c(min(metre, feet_to_metre),max(metre, feet_to_metre))\n       ,main=\"QQ_plot of metre and feet(conversion to metre) estimation\")\nabline(0,1,lty=2,col=\"#00BFFF\",lwd=2)\n\n\n\n# 초반의 일부 데이터들을 제외하면 대부분이 주대각선 밑에 존재한다. 따라서 두 Estimation의 평균은 같다고 보기 힘들다.\n\n(qqplot(metre, feet_to_metre))\n\n$x\n [1]  8  9 10 10 10 10 10 10 11 11 11 11 12 12 13 13 13 14 14 14 15 15 15 15 15\n[26] 15 15 15 16 16 16 17 17 17 17 18 18 20 22 25 27 35 38 40\n\n$y\n [1]  7.315200  7.974419  9.144000  9.144000  9.144000  9.144000  9.753600\n [8] 10.079665 10.363200 10.434084 10.668000 10.972800 10.972800 11.277600\n[15] 12.192000 12.192000 12.192000 12.192000 12.192000 12.206177 12.496800\n[22] 12.801600 12.801600 12.915014 13.106400 13.411200 13.446642 13.716000\n[29] 13.716000 13.716000 13.850679 14.027888 14.509898 14.743814 15.240000\n[36] 15.346326 16.395405 16.459200 16.764000 17.791814 18.521916 20.988670\n[43] 23.497953 28.651200\n\nqq.x <- qqplot(metre, feet_to_metre)$x\n\n\n\nqq.y <- qqplot(metre, feet_to_metre)$y\n\nplot((qq.x+qq.y)/2, qq.y-qq.x, main=\"Tukey mean difference plot\", \n     ylab=\"feet_to_metre - metre\", xlab=\"mean\")\nabline(0,0)\n\n\n\n# Outlier를 제외한 데이터를 Tuckey_Mean_Difference_Plot 위로 올려보고자 한다. \n\nqqplot(adjusted_metre, adjusted_feet_to_metre,xlim=c(min(adjusted_metre,adjusted_feet_to_metre),max(adjusted_metre,adjusted_feet_to_metre)),\n       ylim=c(min(adjusted_metre,adjusted_feet_to_metre),max(adjusted_metre,adjusted_feet_to_metre))\n       ,main=\"QQ_plot of metre and feet(conversion to metre) estimation\")\nabline(0,1,lty=2,col=\"#00BFFF\",lwd=2)\n\n\n\n# 일부 데이터들을 제외하면 대부분의 데이터들이 주대각선 밑에 존재한다. 따라서 두 Estimation의 평균은 같다고 보기 힘들다.\n\n(qqplot(adjusted_metre,adjusted_feet_to_metre))\n\n$x\n [1]  8  9 10 10 10 10 10 10 11 11 11 11 12 12 13 13 13 14 14 14 15 15 15 15 15\n[26] 15 15 15 16 16 16 17 17 17 17 18 18 20 22 25\n\n$y\n [1]  7.315200  8.010769  9.144000  9.144000  9.144000  9.269046  9.753600\n [8] 10.206892 10.363200 10.597662 10.793046 10.972800 11.183815 11.582400\n[15] 12.192000 12.192000 12.192000 12.192000 12.192000 12.496800 12.746892\n[22] 12.801600 12.832862 13.106400 13.411200 13.419015 13.716000 13.716000\n[29] 13.716000 13.895754 14.091138 14.591323 14.943015 15.240000 15.482277\n[36] 16.459200 16.482646 16.764000 18.288000 19.202400\n\nqq.x1 <- qqplot(adjusted_metre,adjusted_feet_to_metre)$x\n\n\n\nqq.y1 <- qqplot(adjusted_metre,adjusted_feet_to_metre)$y\n\nplot((qq.x1+qq.y1)/2, qq.y1-qq.x1, main=\"Tukey mean difference plot\", \n     ylab=\"adjusted_feet_to_metre - adjusted_metre\", xlab=\"mean\")\nabline(0,0)\n\n\n\n\n정리하면 feet로 측정한 강의실 폭의 길이가 metre로 측정한 길이가 보다 더 길다. Mean difference Plot상으로 보았을 때 Outlier을 고려하건 고려하진 않건 대다수가 0 미만에 존재한다는 점에서 Feet로 측정하였을 때 강의실 길이를 더 길게 측정하게 된다. 앞의 QQPLOT 상으로는 두 데이터가 동일한 모수를 가진 정규분포에서 추출되어진 것으로 보였지만 실제로는 미미한 차이지만 서로 다른 모수를 가진 정규분포에서 표본추출되었음을 확인할 수 있다.\n두 데이터 모두 정규분포를 따르는 것이 확인되었기 때문에 Meidan과 Psudosigma를 활용해 평균과 표준편차를 계산할 수 있다.\n\nreal_metre_mean=13.1\n\n# qqplot에서 미터 표본평균 분산 \nqqnorm(metre, ylab=\"metre quantiles\",sub=\"Orange: 절편: 15 (median); 기울기: 4.477612 (pseudosigma)\") \nqqline(metre, col='Blue')\n(q1=fivenum(metre))\n\n[1]  8 11 15 17 40\n\n(pseudosigma = (q1[4]-q1[2])/1.34)\n\n[1] 4.477612\n\nabline(q1[3],pseudosigma,col=\"Orange\")\n\n\n\nq1[3]; pseudosigma^2\n\n[1] 15\n\n\n[1] 20.04901\n\nmean(metre);var(metre)\n\n[1] 16.02273\n\n\n[1] 51.04598\n\nqqnorm(adjusted_metre, ylab=\"adjusted_metre quantiles\",sub=\"Orange: 절편: 14.5 (median); 기울기: 3.731343 (pseudosigma)\") \nqqline(adjusted_metre, col='Blue')\n(q2=fivenum(adjusted_metre))\n\n[1]  8.0 11.0 14.5 16.0 25.0\n\n(pseudosigma2 = (q2[4]-q2[2])/1.34)\n\n[1] 3.731343\n\nabline(q2[3],pseudosigma2,col=\"Orange\")\n\n\n\nq2[3];pseudosigma2^2\n\n[1] 14.5\n\n\n[1] 13.92292\n\nmean(adjusted_metre);var(adjusted_metre)\n\n[1] 14.125\n\n\n[1] 13.13782\n\n# 매우 근접\n\n# 피트 표본평균 분산 \nreal_feet_mean=43.0\n\n# qqplot에서 피트 표본평균 분산 \nqqnorm(feet, ylab=\"feet quantiles\",sub=\"Orange: 절편: 42 (median); 기울기: 8.955224 (pseudosigma)\") \nqqline(feet, col='Blue')\n(q3=fivenum(feet))\n\n[1] 24 36 42 48 94\n\n(pseudosigma3 = (q3[4]-q3[2])/1.34)\n\n[1] 8.955224\n\nabline(q3[3],pseudosigma3,col=\"Orange\")\n\n\n\nq3[3]; pseudosigma3^2\n\n[1] 42\n\n\n[1] 80.19603\n\nmean(feet);var(feet)\n\n[1] 43.69565\n\n\n[1] 156.1854\n\nqqnorm(adjusted_feet, ylab=\"adjusted_feet quantiles\",sub=\"Orange: 절편: 41 (median); 기울기: 8.208955 (pseudosigma)\") \nqqline(adjusted_feet, col='Blue')\n(q4=fivenum(adjusted_feet))\n\n[1] 24 35 41 46 63\n\n(pseudosigma4 = (q4[4]-q4[2])/1.34)\n\n[1] 8.208955\n\nabline(q4[3],pseudosigma4,col=\"Orange\")\n\n\n\nq4[3]; pseudosigma4^2\n\n[1] 41\n\n\n[1] 67.38695\n\nmean(adjusted_feet);var(adjusted_feet)\n\n[1] 41.47692\n\n\n[1] 74.69087\n\n\n\n이유를 추측해보고자 한다.\n\n\n\n\n\n1m가 1ft보다 더 길다. metre로 측정하나 feet로 측정하나 어차피 정수부분까지만 Estimation이 이루어지기 때문에 소수부분에 해당하는 정보가 측정에 반영되지 않는다. 소수점 이하에 해당하는 정보들이 반올림 혹은 버림 등의 수단을 통해서 측정에서 제외되기 때문에 metre를 활용해 강의실 폭을 측정하였을 경우 Feet로 측정하였을 때보다 반영되어지지 못하는 정보가 많아지게 된다. 따라서 metre의 예측치들이 feet의 예측치들에 비해서 작은 값을 가지게 될 수 밖에 없다. Metre 단위를 도입하면서 혼란을 최소화하기 위한 해결책으로는 소숫점 미만의 숫자를 포함한 자료들를 Conversion할 때에는 반드시 소숫점 미만의 값들도 변환에 반영하는 것, metre로 변경하더라도 feet형식의 자료를 도입 후 일정기간동안 병기하는 것 등이 있다.\n\n# 참고\ntmd(qqmath(adjusted_metre,adjusted_feet_to_metre))\n\nWarning in qqmath.numeric(adjusted_metre, adjusted_feet_to_metre): explicit\n'data' specification ignored\n\n\n\n\ntmd(qqmath(metre, feet_to_metre))\n\nWarning in qqmath.numeric(metre, feet_to_metre): explicit 'data' specification\nignored\n\n\n\n\n\n#3번 [airplane data] 비행기의 유리창이 깨질 때까지의 사간 자료이다. 지수분포, 와이블분포, 감마분포에 적합하는지 검토하여라. 가장 적합한 분포를 찾았다면 모수를 추정값을 제시하여라.\n\nlibrary(readxl)\nairplane=c(18.83, 20.8, 21.657, 23.03, 23.23, 24.05, 24.321, 25.5, 25.52, 25.8, 26.69, 26.77, 26.78, 27.05, 27.67, 29.9, 31.11, 33.2, 33.73, 33.76, 33.89, 34.76, 35.75, 35.91, 36.98, 37.08, 37.09, 39.58, 44.045, 45.29, 45.381)\n\n\nstem(airplane) \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  1 | 9\n  2 | 123344\n  2 | 66677778\n  3 | 013444\n  3 | 566777\n  4 | 04\n  4 | 55\n\n# 어느정도 정규 분포를 따르고 있는 것으로 보인다. 하지만, QQPLOT에서 확인하기 전까지는 가정일 뿐이다.\n\nhist(airplane,probability = T,col=\"Pink\")\nlines(density(airplane),col=\"Red\",lwd=2)\n\n\n\nboxplot(airplane,main=\"Boxplot of Airplane Crash Data\") # 이상치 없음"
  },
  {
    "objectID": "posts/EDA4/과제4.html#지수분포",
    "href": "posts/EDA4/과제4.html#지수분포",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "지수분포",
    "text": "지수분포\n\nairplane.sort <- sort(airplane)\n(n.airplane <- length(airplane))\n\n[1] 31\n\ni <- 1:n.airplane\n\nq.exp.airplane <- -log(1-(i-0.5)/n.airplane)\nplot(q.exp.airplane, airplane.sort, main=\"Exponential prob plot\")\nabline(line(q.exp.airplane, airplane.sort),col=\"#B24040\",lwd=2)\n\n\n\n# 3승근 변환 전에는 데이터들이 잘 피팅하고 있지 않은 것으로 보인다. \n\nplot(q.exp.airplane^(1/3), airplane.sort^(1/3), main=\"Exponential prob plot\",sub=\"reexpression; power=1/3\")\nabline(line(q.exp.airplane^(1/3), airplane.sort^(1/3)),col=\"#B24040\",lwd=1) \n\n\n\n# 3승근 변환을 진행 한 이후에는 데이터들이 분포를 잘 따르고 있다고 본다.\n# qqline을 통해 추정된 직선이 데이터들을 잘 피팅하고 있다고 보인다. 데이터들이 곡선을 이루고 Outlier도 다소 존재하는 것으로 보임에도 불구하고 대체적으로는 직 선위에 데이터들이 존재하는 것으로 보인다.  \n# 다만 변환이 요구된다는 점에서 감마분포에 비해서 한계가 있다."
  },
  {
    "objectID": "posts/EDA4/과제4.html#와이블-분포",
    "href": "posts/EDA4/과제4.html#와이블-분포",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "와이블 분포",
    "text": "와이블 분포\n\nq.weibull.airplane <- log(q.exp.airplane)\nplot(q.weibull.airplane, log(airplane.sort), main=\"Weibull prob plot\")\nabline(line(q.weibull.airplane, log(airplane.sort)),col=\"#B24040\",lwd=1) \n\n\n\n# qqline을 통해 추정된 직선이 데이터들을 잘 피팅하고 있다고 보이지는 않는다. 데이터들이 곡선을 이루고 있고 직선에서 벗어난 경우도 많다."
  },
  {
    "objectID": "posts/EDA4/과제4.html#감마-분포",
    "href": "posts/EDA4/과제4.html#감마-분포",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "감마 분포",
    "text": "감마 분포\n\n(mean.airplane <- mean(airplane))\n\n[1] 30.81142\n\n(var.airplane <- var(airplane))\n\n[1] 52.61154\n\n(shape.airplane <- mean.airplane^2/var.airplane)\n\n[1] 18.0444\n\n(scale.airplane <- mean.airplane/var.airplane)\n\n[1] 0.58564\n\nq.gamma.airplane <- qgamma((i-0.5)/n.airplane, shape.airplane, scale.airplane)\nplot(q.gamma.airplane, airplane.sort, main=\"Gamma prob plot\")\nabline(line(q.gamma.airplane, airplane.sort),col=\"#B24040\",lwd=1) \n\n\n\n# qqline을 통해 추정된 직선이 데이터들을 잘 피팅하고 있다고 보인다. 데이터들이 곡선을 이루고 Outlier도 다소 존재하는 것으로 보임에도 불구하고 대체적으로는 직 선위에 데이터들이 존재하는 것으로 보인다.  \n# 따라서 해당 데이터는 감마분포에 가장 잘 적합된다고 본다.\n\n# 감마분포의 parameter는 alpha,beta이며 alpha은 shape parameter, beta는 scale parameter이다.\n\n(shape.airplane); (scale.airplane)\n\n[1] 18.0444\n\n\n[1] 0.58564"
  },
  {
    "objectID": "posts/EDA4/과제4.html#추정1-method-of-moments",
    "href": "posts/EDA4/과제4.html#추정1-method-of-moments",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "추정1 Method of moments",
    "text": "추정1 Method of moments\n원래 추정방식\n\ngamma <- function(x){ x_bar <- mean(x) \n                    x2_bar <- mean(x^2)\n                    beta <- (x2_bar - x_bar^2) / x_bar\n                    alpha <- x_bar / beta \n                    return(c(alpha,1/beta))\n                    }\n\ngamma(airplane)\n\n[1] 18.6458786  0.6051613\n\nq.gamma.airplane <- qgamma((i-0.5)/n.airplane, gamma(airplane)[1] , gamma(airplane)[2])\nplot(q.gamma.airplane, airplane.sort, main=\"Gamma prob plot\")\nabline(line(q.gamma.airplane, airplane.sort),col=\"#B24040\",lwd=1)"
  },
  {
    "objectID": "posts/EDA4/과제4.html#추정2-mle",
    "href": "posts/EDA4/과제4.html#추정2-mle",
    "title": "EDA Assignment 4: Chapter 6",
    "section": "추정2 MLE",
    "text": "추정2 MLE\n데이터가 감마분포에서 왔다는 전제하에 모수 alpha, beta를 추정해본다.\n\nlibrary(MASS)\na=fitdistr(airplane, dgamma, start=list(shape=gamma(airplane)[1], rate=gamma(airplane)[2]))$estimate\n# MLE\n(alpha_hat=as.numeric(a[1]));(beta_hat=as.numeric(a[2]))\n\n[1] 18.93215\n\n\n[1] 0.6144529\n\nq.gamma.airplane <- qgamma((i-0.5)/n.airplane, alpha_hat , beta_hat)\nplot(q.gamma.airplane, airplane.sort, main=\"Gamma prob plot\")\nabline(line(q.gamma.airplane, airplane.sort),col=\"#C21141\",lwd=1)\n\n\n\n# robust\n(shape.airplane); (scale.airplane)\n\n[1] 18.0444\n\n\n[1] 0.58564\n\n# robust하게 추정했던 모수값이랑 크게 차이가 나지 않는 것으로 보인다."
  },
  {
    "objectID": "posts/EDA5/과제5.html",
    "href": "posts/EDA5/과제5.html",
    "title": "EDA Assignment 5: Chapter 7",
    "section": "",
    "text": "7장. 2원 자료, 빈도 표의 탐색"
  },
  {
    "objectID": "posts/EDA5/과제5.html#section",
    "href": "posts/EDA5/과제5.html#section",
    "title": "EDA Assignment 5: Chapter 7",
    "section": "1)",
    "text": "1)\nOn your own; The survival rates differ in Children?\n\nmosaicplot(~ Age+Sex+Survived, data = Titanic,color = c(\"brown\",\"darkgreen\"))\n\n\n\n\n성인 집단에서는 여성과 남성 사이의 생존율에는 큰 차이가 보여지고 있는데에 비해, 어린아이들의 집단에서는 여성과 남성의 생존율이 큰 차이가 있지 않는 것으로 보인다. 일반적인 상황에서 선원들의 통제가 잘 이루어지지 않는 경우 여성들과 아이들의 생존율이 매우 낮아지지만, 타이타닉호 사건 때는 승객들에 대한 통제도와 선원들의 상선사관들의 명령 복종정도가 높았기 때문에 여자와 아이들 먼저 규칙에 의해 남자아이들의 생존율이 성인 남자의 생존율에 비해서 상당히 높게 나타났다.\n\nmosaicplot(~ Class + Age  + Survived, data = Titanic, color = c(\"brown\",\"darkgreen\"))\n\n\n\n\n1등석과 2등석의 아이들은 1명빼고 전부 살아남았지만, 3등실에서 구조되지 못한 어린이는 구조된 수의 거의 두 배에 가까웠다. 3등석의 아이들이 1등석과 2등석에 비해서 살아남기 힘들었던 이유는 배 안의 구조가 복잡했기 때문이다. 3등석에는 영어를 잘 하지 못하는 승객들도 많았기에 갑판과 적절한 의사소통을 하지못해 밖으로 나가는 것을 실패한 사람들이 많다는 점에서 그 이유를 찾을 수 있다.\n\nmosaicplot(~ Sex + Class + Age + Survived, data = Titanic, color = c(\"brown\",\"darkgreen\"))\n\n\n\n\n모든 요인을 합친 그래프이다. 1등석, 2등석 모두 어린아이들이 어른에 비해, 여자들이 남자들에 비해 생존확률이 월등히 높다. 3등석의 경우에도 여자 아이들이 남자아이들에 비해서 생존 확률이 월등히 높았다는 점에서 “여자와 아이”먼저 원칙이 지켜지고 있음이 확인되고 있다. 함장 로버트 샐먼드(Robert Salmond) 해군 대령 이하 육해군 장병들은 갑판에 부동 자세로 도열한 채로 질서를 유지하였기 대문에 규칙이 지켜질 수 있었고, 승객들은 명령에 복종할 수 밖에 없었던 것으로 보인다."
  },
  {
    "objectID": "posts/EDA5/과제5.html#심프슨-역설의-예시",
    "href": "posts/EDA5/과제5.html#심프슨-역설의-예시",
    "title": "EDA Assignment 5: Chapter 7",
    "section": "심프슨 역설의 예시",
    "text": "심프슨 역설의 예시\n\n미국 중위소득의 감소 https://economix.blogs.nytimes.com/2013/05/01/can-every-group-be-worse-than-average-yes/ 2000년 이래로 미국의 중위소득은 인플레이션을 고려하더라도 1%만큼 증가한 것이 확인된다. 그러나, 고등학교 중퇴자 집단, 고등학교 졸업자 집단, 전문대학 졸업자 집단 그리고 4년제 대학 졸업자 및 대학원 졸업자 집단의 소득은 모두 감소하였다. (모든 교육 하위집단에서 중위소득은 2000년도보다 낮은 수준으로 확인되어진다.) 그러한 요인이 발생한 이유는 교육수준에 따른 하위집단에 속하는 인구 비율이 달라졌기 때문이다. (2000년 이래로 13년동안 대학교 졸업자수는 더 낮은 교육을 받은 사람들에 비해서 더 많이 증가했지만, 그들의 임금은 종합적으로 보았을 때 1.2%정도 훨씬 더 천천히 감소하였기 때문이다. ) 전체적인 관점에서는 임금이 증가하였다는 결론을 낼 수 있지만, 개인의 관점에서는 임금이 오히려 감소하였다고 보는 것이 가능하다.\n벨기에의 COVID-19 사망률 (성별, 나이)\n\nhttps://link.springer.com/article/10.1007/s11192-020-03830-w https://www.medrxiv.org/content/10.1101/2020.06.20.20136234v2\n\nbelgium=matrix(c(0,0,0.02,0.01,0.29,0.14,2.92,1.61,5.56,3.35,13.20,11.07,1.18,1.31),byrow=T,ncol=2)\nbel=array(belgium,c(7,2),dimnames=list(age=c(\"0~24\",\"25~44\",\"45~64\",\"65~74\",\"75~84\",\"85 and older\",\"All Ages\"),gender=c(\"Male\",\"Female\")))\nbel\n\n              gender\nage             Male Female\n  0~24          0.00   0.00\n  25~44         0.02   0.01\n  45~64         0.29   0.14\n  65~74         2.92   1.61\n  75~84         5.56   3.35\n  85 and older 13.20  11.07\n  All Ages      1.18   1.31\n\n\nR.R.’s local Flemish newspaper, De Standaard에서 2020년 6월 기사 인용. 모든 나이 Group들에서 남성의 COVID-19 감염 후 사망률이 높게 측정되고 있음에도 불구하고 전체 인구를 고려하였을 때 여성의 COVID-19 감염 후 사망률이 오히려 높게 나타나는 결과로 이어짐. 이러한 결과가 나타나는 인구는 벨기에에서는 남자노인들보다 여자 노인들이 훨신 더 많기 때문임으로 보인다.\n\nTitanic Dataset\n\n\n# Titanic\ntdf <- as.data.frame(Titanic)\nTita <- tdf[rep(1:nrow(tdf), tdf$Freq), -5]\n\n\nlibrary(reshape2)\nA=dcast(Tita,Class~Survived)\n\nUsing Survived as value column: use value.var to override.\n\n\nAggregation function missing: defaulting to length\n\nA$\"prop(%)\"=round(A$Yes/(A$No+A$Yes),4)*100\nA\n\n  Class  No Yes prop(%)\n1   1st 122 203   62.46\n2   2nd 167 118   41.40\n3   3rd 528 178   25.21\n4  Crew 673 212   23.95\n\nB=dcast(Tita,Class~Survived+Sex)\n\nUsing Survived as value column: use value.var to override.\nAggregation function missing: defaulting to length\n\nB$\"Maleprop(%)\"=round(B$Yes_Male/(B$Yes_Male+B$No_Male),4)*100\nB$\"Femaleprop(%)\"=round(B$Yes_Female/(B$Yes_Female+B$No_Female),4)*100\n\n\nFinal=B[,c(4,2,6,5,3,7)]\nrownames(Final)=A$Class\n\nA;Final\n\n  Class  No Yes prop(%)\n1   1st 122 203   62.46\n2   2nd 167 118   41.40\n3   3rd 528 178   25.21\n4  Crew 673 212   23.95\n\n\n     Yes_Male No_Male Maleprop(%) Yes_Female No_Female Femaleprop(%)\n1st        62     118       34.44        141         4         97.24\n2nd        25     154       13.97         93        13         87.74\n3rd        88     422       17.25         90       106         45.92\nCrew      192     670       22.27         20         3         86.96\n\n# 3등석과 선원들 비교 비교\n\nmosaicplot(~Class+Survived,Titanic,color=T,main=\"Class~Survived\")\n\n\n\nmosaicplot(~Class+Survived+Sex,Titanic,color=T,main=\"Class~Survived Under Sex\")\n\n\n\n\n3등석과 선원들의 생존률을 비교해보자. 남성과 여성 모두 선원들이 3등석 승객들에 비해 생존확률이 높았음에도 불구하고 (남성 승객 17.25% < 선원 22.27% , 여성 승객 45.92% < 선원 86.96%) 성별에 상관없이 모든 데이터를 비교하였을 때 3등석 승객들의 생존확률이 선원들의 생존확률보다 높은 것으로 나타난다. (승객 25.21% > 선원 23.95%) 이것의 요인으로는 내생변수 (숨겨진 변수)가 그러한 결과가 나타나는데 영향을 미쳤음을 확인할 수 있다. 3등석의 여성의 비율이 선원의 여성의 비율에 비해서 훨씬 높다는 것을 확인할 수 있다. 3등석 승객들과 선원들의 성비의 차이가 이러한 결과로 이어지게 되었음을 확인할 수 있다."
  },
  {
    "objectID": "posts/EDA5/과제5.html#년-과-최근-자료를-각각-분석한다.",
    "href": "posts/EDA5/과제5.html#년-과-최근-자료를-각각-분석한다.",
    "title": "EDA Assignment 5: Chapter 7",
    "section": "(1) 1988년 과 최근 자료를 각각 분석한다.",
    "text": "(1) 1988년 과 최근 자료를 각각 분석한다."
  },
  {
    "objectID": "posts/EDA5/과제5.html#년-자료와-kosis-최근-자료의-범주가-약간-다른-점에-주의하면서-이들의-시간차약-30여년에-따른-태도의-변화를-서로-비교하여-분석한다.",
    "href": "posts/EDA5/과제5.html#년-자료와-kosis-최근-자료의-범주가-약간-다른-점에-주의하면서-이들의-시간차약-30여년에-따른-태도의-변화를-서로-비교하여-분석한다.",
    "title": "EDA Assignment 5: Chapter 7",
    "section": "(2) 1988년 자료와 KOSIS 최근 자료의 범주가 약간 다른 점에 주의하면서 이들의 시간차(약 30여년)에 따른 태도의 변화를 서로 비교하여 분석한다.",
    "text": "(2) 1988년 자료와 KOSIS 최근 자료의 범주가 약간 다른 점에 주의하면서 이들의 시간차(약 30여년)에 따른 태도의 변화를 서로 비교하여 분석한다.\n\npop1988=matrix(c(16.4,14.5,0.8,44.4,17.4,6.5,16.8,16.3,0.4,43.5,17.1,5.9,19.5,17.6,0.4,38.9,18.5,5.1,29.0,18.8,0.5,31.1,16.4,4.2,36.6,21.2,0.3,25.6,12.6,3.7,48.1,20.2,0.4,19.8,8.1,3.4),byrow=T,ncol=6)\npop1988=as.data.frame(pop1988)\ncolnames(pop1988)=c(\"장남\",\"아들모두\",\"딸\",\"아들딸모두\",\"자립\",\"사회및기타\")\nrownames(pop1988)=c(\"15~19세\",\"20~29세\",\"30~39세\",\"40~49세\",\"50~59세\",\"60세이상\")\npop1988\n\n         장남 아들모두  딸 아들딸모두 자립 사회및기타\n15~19세  16.4     14.5 0.8       44.4 17.4        6.5\n20~29세  16.8     16.3 0.4       43.5 17.1        5.9\n30~39세  19.5     17.6 0.4       38.9 18.5        5.1\n40~49세  29.0     18.8 0.5       31.1 16.4        4.2\n50~59세  36.6     21.2 0.3       25.6 12.6        3.7\n60세이상 48.1     20.2 0.4       19.8  8.1        3.4\n\npop2018=data.frame(\n  stringsAsFactors = FALSE,\n                연령 = c(\"13∼19세\",\"20∼29세\",\"30∼39세\",\n                       \"40∼49세\",\"50∼59세\",\"60세이상\"),\n                자립 = c(17.5, 19.2, 17.3, 16.9, 17.8, 25),\n                장남 = c(0.6, 0.6, 0.8, 1.1, 1.3, 2.9),\n              아들모두 = c(1.2, 0.7, 0.9, 0.8, 0.8, 1.6),\n                 딸 = c(0.4, 0.3, 0.2, 0.2, 0.2, 0.4),\n             모든_자녀 = c(26.4, 21, 19.1, 18, 17.6, 17.4),\n                능력 = c(3.9, 4.7, 4.6, 4.4, 4.7, 6),\n            가족정부사회 = c(45.8, 49.5, 52.4, 53.1, 51.7, 38.8),\n              정부사회 = c(4.2, 4, 4.7, 5.5, 5.9, 7.9)\n)\nrownames(pop2018)=pop2018$연령\npop2018=pop2018[,c(2:9)]\n\n\npop2020=data.frame(\n  stringsAsFactors = FALSE,\n                        연령 = c(\"15∼19세\",\n                               \"20∼29세\",\"30∼39세\",\"40∼49세\",\"50∼59세\",\"60세이상\",\n                               \"65세이상\"),\n            부모_스스로 = c(11.1, 11.6, 14.4, 11, 10.7, 16, 17),\n                가족 = c(27.2, 21.8, 19.4, 19.4, 19.2, 26, 27.3),\n        가족정부사회가_함께 = c(58.9, 64.4, 63.7, 66.5, 66.5, 52.8, 49.9),\n             정부_사회 = c(2.7, 2.2, 2.5, 3.1, 3.6, 5.2, 5.8),\n                기타 = c(0.1, 0, 0, 0, 0, 0, 0)\n        )\nrownames(pop2020)=c(pop2020$연령)\npop2020=pop2020[,c(2:6)]\n\n\n\n\npop1988; pop2018; pop2020 #2010년도 데이터부터 65세 이상 데이터가 추가됨\n\n         장남 아들모두  딸 아들딸모두 자립 사회및기타\n15~19세  16.4     14.5 0.8       44.4 17.4        6.5\n20~29세  16.8     16.3 0.4       43.5 17.1        5.9\n30~39세  19.5     17.6 0.4       38.9 18.5        5.1\n40~49세  29.0     18.8 0.5       31.1 16.4        4.2\n50~59세  36.6     21.2 0.3       25.6 12.6        3.7\n60세이상 48.1     20.2 0.4       19.8  8.1        3.4\n\n\n         자립 장남 아들모두  딸 모든_자녀 능력 가족정부사회 정부사회\n13∼19세 17.5  0.6      1.2 0.4      26.4  3.9         45.8      4.2\n20∼29세 19.2  0.6      0.7 0.3      21.0  4.7         49.5      4.0\n30∼39세 17.3  0.8      0.9 0.2      19.1  4.6         52.4      4.7\n40∼49세 16.9  1.1      0.8 0.2      18.0  4.4         53.1      5.5\n50∼59세 17.8  1.3      0.8 0.2      17.6  4.7         51.7      5.9\n60세이상 25.0  2.9      1.6 0.4      17.4  6.0         38.8      7.9\n\n\n         부모_스스로 가족 가족정부사회가_함께 정부_사회 기타\n15∼19세        11.1 27.2                58.9       2.7  0.1\n20∼29세        11.6 21.8                64.4       2.2  0.0\n30∼39세        14.4 19.4                63.7       2.5  0.0\n40∼49세        11.0 19.4                66.5       3.1  0.0\n50∼59세        10.7 19.2                66.5       3.6  0.0\n60세이상        16.0 26.0                52.8       5.2  0.0\n65세이상        17.0 27.3                49.9       5.8  0.0\n\n# 2020년도의 경우 가족 형태가 바뀌면서 노인 부양을 담당하는 주체가 변경되어 column name들이 변경되어진 것으로 보인다.\n\n총 세개 시점의 데이터가 존재한다. 만약 그 데이터들이 가법적 모형을 따를경우 다음과 같은 모형으로 표현할 수 있을 것이다. y_ijk=μk+α_ik+β_jk+e_ijk ,e_ijk~(0,σ),i=(1,…,6) (row effect),j=(1,…,5 or 6 or 8)(column effect), k=(1,2,3) (Time)을 고려한다. 1988데이터는 row * col = 66 2010데이터는 row col = 68 2020데이터는 row col = 6*5 개의 효과를 가짐\n\npop1988_polished=medpolish(pop1988)\n\n1: 155.4\n2: 144.8\nFinal: 144.8\n\npop1988_polished\n\n\nMedian Polish Results (Dataset: \"pop1988\")\n\nOverall: 17.45\n\nRow Effects:\n 15~19세  20~29세  30~39세  40~49세  50~59세 60세이상 \n   0.525    0.175    0.125   -0.125   -0.625   -0.725 \n\nColumn Effects:\n      장남   아들모두         딸 아들딸모두       자립 사회및기타 \n      6.80       0.75     -17.00      17.55      -0.75     -12.80 \n\nResiduals:\n           장남 아들모두     딸 아들딸모두   자립 사회및기타\n15~19세  -8.375   -4.225 -0.175      8.875  0.175      1.325\n20~29세  -7.625   -2.075 -0.225      8.325  0.225      1.075\n30~39세  -4.875   -0.725 -0.175      3.775  1.675      0.325\n40~49세   4.875    0.725  0.175     -3.775 -0.175     -0.325\n50~59세  12.975    3.625  0.475     -8.775 -3.475     -0.325\n60세이상 24.575    2.725  0.675    -14.475 -7.875     -0.525\n\npop2018_polished=medpolish(pop2018)\n\n1: 63.8\n2: 58.5\nFinal: 58.4\n\npop2018_polished$residuals=round(pop2018_polished$residuals,5)\npop2018_polished$col=round(pop2018_polished$col,5)\npop2018_polished$row=round(pop2018_polished$row,5)\npop2018_polished\n\n\nMedian Polish Results (Dataset: \"pop2018\")\n\nOverall: 4.85\n\nRow Effects:\n13∼19세 20∼29세 30∼39세 40∼49세 50∼59세 60세이상 \n-0.36250  0.00000  0.00000  0.00000  0.06875  1.11250 \n\nColumn Effects:\n        자립         장남     아들모두           딸    모든_자녀         능력 \n    12.94688     -3.81875     -4.08438     -4.65000     13.70000     -0.23438 \n가족정부사회     정부사회 \n    45.71562      0.25000 \n\nResiduals:\n             자립     장남 아들모두       딸 모든_자녀     능력 가족정부사회\n13∼19세  0.06562 -0.06875  0.79688  0.56250    8.2125 -0.35312      -4.4031\n20∼29세  1.40312 -0.43125 -0.06562  0.10000    2.4500  0.08438      -1.0656\n30∼39세 -0.49687 -0.23125  0.13438  0.00000    0.5500 -0.01562       1.8344\n40∼49세 -0.89688  0.06875  0.03438  0.00000   -0.5500 -0.21562       2.5344\n50∼59세 -0.06562  0.20000 -0.03438 -0.06875   -1.0188  0.01562       1.0656\n60세이상  6.09063  0.75625 -0.27812 -0.91250   -2.2625  0.27187     -12.8781\n         정부사회\n13∼19세 -0.53750\n20∼29세 -1.10000\n30∼39세 -0.40000\n40∼49세  0.40000\n50∼59세  0.73125\n60세이상  1.68750\n\npop2020_polished=medpolish(pop2020)\n\n1: 86.5\n2: 76\nFinal: 76\n\npop2020_polished$residuals=round(pop2020_polished$residuals,5)\npop2020_polished$col=round(pop2020_polished$col,5)\npop2020_polished$row=round(pop2020_polished$row,5)\npop2020_polished\n\n\nMedian Polish Results (Dataset: \"pop2020\")\n\nOverall: 11.7\n\nRow Effects:\n15∼19세 20∼29세 30∼39세 40∼49세 50∼59세 60세이상 65세이상 \n     0.0     -0.1      0.2     -0.1     -0.1      2.9      3.5 \n\nColumn Effects:\n        부모_스스로                가족 가족정부사회가_함께           정부_사회 \n                0.0                10.2                51.8                -9.4 \n               기타 \n              -11.6 \n\nResiduals:\n         부모_스스로 가족 가족정부사회가_함께 정부_사회 기타\n15∼19세        -0.6  5.3                -4.6       0.4  0.0\n20∼29세         0.0  0.0                 1.0       0.0  0.0\n30∼39세         2.5 -2.7                 0.0       0.0 -0.3\n40∼49세        -0.6 -2.4                 3.1       0.9  0.0\n50∼59세        -0.9 -2.6                 3.1       1.4  0.0\n60세이상         1.4  1.2               -13.6       0.0 -3.0\n65세이상         1.8  1.9               -17.1       0.0 -3.6\n\n\nMedian Polish의 시행과 이후 Tukey Additivity Plot & 잔차 분석\n\n# 1988년\nplot(pop1988_polished)\nabline(0,1,col=\"red\",lty=2) #직선이 경향성을 정확히 보인다고 보기 어렵다.\nabline(lm(as.vector(pop1988_polished$residuals) ~ \n            as.vector(outer(pop1988_polished$row,pop1988_polished$col, \"*\")/pop1988_polished$overall)),col=\"orange\",lty=2)\n\n\n\nlm(as.vector(pop1988_polished$residuals) ~ \n     as.vector(outer(pop1988_polished$row,pop1988_polished$col, \"*\")/pop1988_polished$overall))[1]\n\n$coefficients\n                                                                               (Intercept) \n                                                                                 0.2173017 \nas.vector(outer(pop1988_polished$row, pop1988_polished$col, \"*\")/pop1988_polished$overall) \n                                                                                 2.8429224 \n\n# 회귀 직선의 기울기가 2.8429224 이므로 1과 같다고 보기 어렵다.  따라서 로그 변환을 해서는 안된다.\n\n# 잔차의 Boxplot을 통해서 분석할 경우 \nboxplot(pop1988_polished$residuals) #Outlier는 보이지 않았다. 따라서 제거할 값은 보이지 않는다.\n\n\n\n# 행 효과 크기 순으로 재정렬한 잔차표\nround(pop1988_polished$residuals[order(pop1988_polished$row),],1) \n\n         장남 아들모두   딸 아들딸모두 자립 사회및기타\n60세이상 24.6      2.7  0.7      -14.5 -7.9       -0.5\n50~59세  13.0      3.6  0.5       -8.8 -3.5       -0.3\n40~49세   4.9      0.7  0.2       -3.8 -0.2       -0.3\n30~39세  -4.9     -0.7 -0.2        3.8  1.7        0.3\n20~29세  -7.6     -2.1 -0.2        8.3  0.2        1.1\n15~19세  -8.4     -4.2 -0.2        8.9  0.2        1.3\n\n# 나이가 많을수록 아들, 특히 장남이 부모님을 부양하고자 하는 경우가 많다. (60세이상의 집단의 잔차 24.6)\n# 나이가 적을수록 모든 자녀(아들,딸)이 부모님 부양을 분담하고자 하는 경향성이 나타난다.\n\n# Check Decomposition\ndecomposed_88=(pop1988_polished$overall + outer(pop1988_polished$row,pop1988_polished$col, \"+\") + pop1988_polished$residuals)\nall(round(decomposed_88,3)==pop1988)\n\n[1] TRUE\n\n# 가법성 모형이 확인되어지고 있다.\n# Comparison Values\nround(outer(pop1988_polished$row,pop1988_polished$col, \"*\")/pop1988_polished$overall,2)\n\n          장남 아들모두    딸 아들딸모두  자립 사회및기타\n15~19세   0.20     0.02 -0.51       0.53 -0.02      -0.39\n20~29세   0.07     0.01 -0.17       0.18 -0.01      -0.13\n30~39세   0.05     0.01 -0.12       0.13 -0.01      -0.09\n40~49세  -0.05    -0.01  0.12      -0.13  0.01       0.09\n50~59세  -0.24    -0.03  0.61      -0.63  0.03       0.46\n60세이상 -0.28    -0.03  0.71      -0.73  0.03       0.53\n\n# Comparison value 기준으로 0 값이 확인되지는 않는다. 그러나 대다수는 0근처에 존재하고 있으므로 Fitting에서 많이 벗어나지 않았다.\n\nstem(pop1988_polished$residuals) # Gap이 있는 것을 제외하고는 전체적으로 대칭적으로 보인다.\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -1 | 4\n  -0 | 98885\n  -0 | 443211000000\n   0 | 0000011112344\n   0 | 589\n   1 | 3\n   1 | \n   2 | \n   2 | 5\n\n\n\n# 2018년\nplot(pop2018_polished) \nabline(0,1,col=\"red\",lty=2) #직선이 경향성을 정확히 보인다고 보기 어렵다.\nabline(lm(as.vector(pop2018_polished$residuals) ~ \n            as.vector(outer(pop2018_polished$row,pop2018_polished$col, \"*\")/pop2018_polished$overall)),col=\"orange\",lty=2)\n\n\n\nlm(as.vector(pop2018_polished$residuals) ~ \n     as.vector(outer(pop2018_polished$row,pop2018_polished$col, \"*\")/pop2018_polished$overall))[1]\n\n$coefficients\n                                                                               (Intercept) \n                                                                                 0.2056009 \nas.vector(outer(pop2018_polished$row, pop2018_polished$col, \"*\")/pop2018_polished$overall) \n                                                                                -0.8088495 \n\n# 회귀 직선의 기울기가  -0.8088495 이므로 1과 같다고 보기 어렵다. 따라서 로그 변환을 해서는 안된다.\n\n# 잔차의 Boxplot을 통해서 분석할 경우 \n\nboxplot(pop2018_polished$residuals) #Residual들의 Outlier가 확인되어진다 (5개). 만약 제거된다면 Additivity plot이 어떻게 변할까?\n\n\n\npop2018_polished2=pop2018_polished\npop2018_polished2$residuals[,1][6]=mean(pop2018_polished$residuals[,1])\npop2018_polished2$residuals[,3][1]=mean(pop2018_polished$residuals[,3])\npop2018_polished2$residuals[,5][1]=mean(pop2018_polished$residuals[,5])\nboxplot(pop2018_polished2$residuals)\n\n\n\nplot(pop2018_polished2)  # 경향성이 크게 바뀌지 않기 때문에 Raw Data 사용\n\n\n\n# 행 효과 크기 순으로 재정렬한 잔차표\nround(pop2018_polished$residuals[order(pop2018_polished$row),],1) \n\n         자립 장남 아들모두   딸 모든_자녀 능력 가족정부사회 정부사회\n13∼19세  0.1 -0.1      0.8  0.6       8.2 -0.4         -4.4     -0.5\n20∼29세  1.4 -0.4     -0.1  0.1       2.5  0.1         -1.1     -1.1\n30∼39세 -0.5 -0.2      0.1  0.0       0.6  0.0          1.8     -0.4\n40∼49세 -0.9  0.1      0.0  0.0      -0.6 -0.2          2.5      0.4\n50∼59세 -0.1  0.2      0.0 -0.1      -1.0  0.0          1.1      0.7\n60세이상  6.1  0.8     -0.3 -0.9      -2.3  0.3        -12.9      1.7\n\n# 나이에 따른 경향성이 거의 나타나지 않는 것으로 보인다. \n\n# Check Decomposition\ndecomposed_18=(pop2018_polished$overall + outer(pop2018_polished$row,pop2018_polished$col, \"+\") + pop2018_polished$residuals)\nall(round(decomposed_18,2)==pop2018)\n\n[1] TRUE\n\n# 가법성 모형이 확인되어지고 있다.\n\n# Comparison Values\nround(outer(pop2018_polished$row,pop2018_polished$col, \"*\")/pop2018_polished$overall,2)\n\n          자립  장남 아들모두    딸 모든_자녀  능력 가족정부사회 정부사회\n13∼19세 -0.97  0.29     0.31  0.35     -1.02  0.02        -3.42    -0.02\n20∼29세  0.00  0.00     0.00  0.00      0.00  0.00         0.00     0.00\n30∼39세  0.00  0.00     0.00  0.00      0.00  0.00         0.00     0.00\n40∼49세  0.00  0.00     0.00  0.00      0.00  0.00         0.00     0.00\n50∼59세  0.18 -0.05    -0.06 -0.07      0.19  0.00         0.65     0.00\n60세이상  2.97 -0.88    -0.94 -1.07      3.14 -0.05        10.49     0.06\n\n# Comparison value 기준으로 23개의 0이 발견됨. 대다수의 값들이 0근처에 존재하고 있으므로 Fitting에서 많이 벗어나지 않았다. \n\nstem(pop2018_polished$residuals) # Gap이 있는데 이건 Boxplot을 통해 확인된 outlier들을 재확인해준다.\n\n\n  The decimal point is at the |\n\n  -12 | 9\n  -10 | \n   -8 | \n   -6 | \n   -4 | 4\n   -2 | 3\n   -0 | 11099655444322111100\n    0 | 000011111234667881478\n    2 | 55\n    4 | \n    6 | 1\n    8 | 2\n\n# 대다수의 값들이 0 근처에 몰려있다. \n\n\n# 2020년\nplot(pop2020_polished) \nabline(0,1,col=\"red\",lty=2) #직선이 경향성을 정확히 보인다고 보기 어렵다.\nabline(lm(as.vector(pop2020_polished$residuals) ~ \n            as.vector(outer(pop2020_polished$row,pop2020_polished$col, \"*\")/pop2020_polished$overall)),col=\"orange\",lty=2)\n\n\n\nlm(as.vector(pop2020_polished$residuals) ~ \n     as.vector(outer(pop2020_polished$row,pop2020_polished$col, \"*\")/pop2020_polished$overall))[1]\n\n$coefficients\n                                                                               (Intercept) \n                                                                                -0.2362669 \nas.vector(outer(pop2020_polished$row, pop2020_polished$col, \"*\")/pop2020_polished$overall) \n                                                                                -0.8937232 \n\n# 회귀 직선의 기울기가 -0.9335803 이므로 1과 같다고 보기 어렵다. 따라서 로그 변환을 해서는 안된다.\n\n# Boxplot으로 Outlier Check\nboxplot(pop2020_polished$residuals)  #Outlier는 보이지 않았다. 따라서 제거할 값은 보이지 않는다.\n\n\n\n\n\n# 행 효과 크기 순으로 재정렬한 잔차표\nround(pop2020_polished$residuals[order(pop2020_polished$row),],1) \n\n         부모_스스로 가족 가족정부사회가_함께 정부_사회 기타\n20∼29세         0.0  0.0                 1.0       0.0  0.0\n40∼49세        -0.6 -2.4                 3.1       0.9  0.0\n50∼59세        -0.9 -2.6                 3.1       1.4  0.0\n15∼19세        -0.6  5.3                -4.6       0.4  0.0\n30∼39세         2.5 -2.7                 0.0       0.0 -0.3\n60세이상         1.4  1.2               -13.6       0.0 -3.0\n65세이상         1.8  1.9               -17.1       0.0 -3.6\n\n\n60세 이상 집단과 30~39세 집단에서는 본인 스스로 부모님을 부양해야 한다고 생각하고 있는 것으로 보인다. 60대 집단의 경우 정부 사회 등 외부에 의존하기보다는 본인이 직접 노후 대비를 해야한다는 인식이 있는 것으로 보인다. 1988년 조사에서 노인 집단에서 장남이 부모 부양을 담당해야 한다고 생각했던 것에서 변화한 것으로 보인다. 반면 40대 50대는 가족정부사회와 함께 부모를 부양해야 한다는 인식이 많다. (본인이 직접 본인 부모의 노후 대비를 담당해야하는 상황이기 때문에 이러한 경향성이 나타나는 것으로 예측된다.)\n\n# Check Decomposition\ndecomposed_20=(pop2020_polished$overall + outer(pop2020_polished$row,pop2020_polished$col, \"+\") + pop2020_polished$residuals)\nall(round(decomposed_20,1)==pop2020)\n\n[1] TRUE\n\n# 가법성 모형이 확인되어지고 있다.\n\n# Comparison Values\nround(outer(pop2020_polished$row,pop2020_polished$col, \"*\")/pop2020_polished$overall,2)\n\n         부모_스스로  가족 가족정부사회가_함께 정부_사회  기타\n15∼19세           0  0.00                0.00      0.00  0.00\n20∼29세           0 -0.09               -0.44      0.08  0.10\n30∼39세           0  0.17                0.89     -0.16 -0.20\n40∼49세           0 -0.09               -0.44      0.08  0.10\n50∼59세           0 -0.09               -0.44      0.08  0.10\n60세이상           0  2.53               12.84     -2.33 -2.88\n65세이상           0  3.05               15.50     -2.81 -3.47\n\n# Comparison value 기준으로 0값이 총 5개 확인됨. \n# 대다수의 값들이 Additivity Plot의 x축과 y축 근처에 존재하는 것으로 보아 몇몇 피팅에서 벗어난 값을 제외하면 대부분 0근처에서 존재하고 있는 것이 확인된다. \n\nstem(pop2020_polished$residuals) \n\n\n  The decimal point is at the |\n\n  -16 | 1\n  -14 | \n  -12 | 6\n  -10 | \n   -8 | \n   -6 | \n   -4 | 6\n   -2 | 60764\n   -0 | 9663\n    0 | 0000000000049024489\n    2 | 511\n    4 | 3\n\n# Gap이 있음에도 Boxplot 상으로는 Outlier는 보이지 않았다. 3~4개 값을 제외 하면 잔차들이 0근처에 잘 모여있는 것이 확인된다.\n\n#log 변환해서 경향성이 있는지 확인하기 (어느정도 y=x 경향성을 따르는 것이 아닐까?)\npop20202=pop2020\npop20202$기타=ifelse(pop20202$기타==0,0.1,pop20202$기타) # 책에서 사용하던 기법 활용 (log0은 존재하지 않기에)\nk=medpolish(log(pop20202))\n\n1: 5.073833\n2: 4.168534\nFinal: 4.168534\n\nplot(k) \n\n\n\nstem(k$residuals) #Gap이 줄어들은 것은 사실이지만 아직도 존재함 & 변환이전이후 둘다 대칭성은 차이 없음\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -5 | 3\n  -4 | 30\n  -3 | \n  -2 | 974\n  -1 | 5\n  -0 | 8653\n   0 | 000000000001445689\n   1 | 00\n   2 | 228\n   3 | 3\n\nboxplot(k$residuals) #원래 Oulier가 없었는데 변환이후 생기게 됨 \n\n\n\n# 따라서 로그 변환을 하지 않고 원래 자료를 기준으로 그대로 분석할 예정임. \n\nRaw Data를 활용하여 연도별 Column Effect(부양책임)와 Row Effect(나이)를 시각화해보고자 한다.\n\n# 1988\n# Row Effect \nbarplot(pop1988_polished$row, ylim=c(-1,1), main=\"나이_1988년\")\n\n\n\npop1988_polished$row\n\n 15~19세  20~29세  30~39세  40~49세  50~59세 60세이상 \n   0.525    0.175    0.125   -0.125   -0.625   -0.725 \n\n# Column Effect\nbarplot(pop1988_polished$col, ylim=c(-20,20), main=\"부양유형_1988년\")\n\n\n\npop1988_polished$col\n\n      장남   아들모두         딸 아들딸모두       자립 사회및기타 \n      6.80       0.75     -17.00      17.55      -0.75     -12.80 \n\n\nRow 젊은 사람들일 수록 부모님을 부양하는 것을 부담스럽지 않게 여기다가 나이를 먹음에 따라 직접 부담을 지게되면서 어려움을 느끼거나 타인에게 부양을 요구하는 것을 부담스럽게 여기는 것으로 보인다.\nColumn 1988년도에도 장남 또는 맏며느리인 경우 딸 또는 사위일 때에 비해 부모님 부양에 대해서 긍정적인 태도를 보였던 것으로 보인다.모든 자녀,아들과 딸이 그러한 부담을 분담하고자 하는 인식이 가장 많았다는 점에서 1988년에도 부모님 부양을 한 집안에서 책임지는 것은 쉽지 않았다는 것을 확인할 수 있다.\n\n# 2018\n# Row Effect \nbarplot(pop2018_polished$row, ylim=c(-0.5,1.5), main=\"나이_2018년\")\n\n\n\npop2018_polished$row\n\n13∼19세 20∼29세 30∼39세 40∼49세 50∼59세 60세이상 \n-0.36250  0.00000  0.00000  0.00000  0.06875  1.11250 \n\n# Column Effect\nbarplot(pop2018_polished$col, ylim=c(-10,50), main=\"부양유형_2018년\")\n\n\n\npop2018_polished$col\n\n        자립         장남     아들모두           딸    모든_자녀         능력 \n    12.94688     -3.81875     -4.08438     -4.65000     13.70000     -0.23438 \n가족정부사회     정부사회 \n    45.71562      0.25000 \n\n\nRow 1988년도와는 달리 2018년에는 15~19세의 경우 모든 연령대중에서 부모님 부양에 대한 인식이 가장 좋지 못한 것으로 확인된다. 나머지 나이대에서는 60대를 제외 하면 나이에 따른 인식에 대한 영향이 크게 나타나지 않는 것으로 보인다. 직접적으로 부양을 받는 세대인 은퇴를 앞둔 60대의 입장에서는 본인의 은퇴이후의 삶에 대한 고민이 반영되었기 때문에 부모부양에 대한 강한 인식을 보이고 있지만, 이외의 세대에서는 부양에 대한 인식이 그렇게 높지 않은 것이 확인되어 진다.\nColumn 남녀 평등 교육이 시작되면서 장남들이 부양을 전적으로 책임져야한다는 인식은 감소하게 된 것으로 보인다. 또한 여성들의 부모님 부양에 대한 부정적 인식도 다소 감소한 것으로 보인다. 설령 여유가 있더라도 한 사람이 모든 부양을 책임지라는 인식 보다는 자립하거나 자녀들이 분담하여 부모님을 부양하고자 하는 인식이 강해진 것을 확인할 수 있다. 정부와 사회의 외부집단에 부모님 부양을 전적으로 의존하기 보다는 가족이 일정정도는 기여하여야 한다는 인식이 높은 것으로 나타난다.\n\n# 2020\n# Row Effect \nbarplot(pop2020_polished$row, ylim=c(-1,4), main=\"나이_2020년\")\n\n\n\npop2020_polished$row\n\n15∼19세 20∼29세 30∼39세 40∼49세 50∼59세 60세이상 65세이상 \n     0.0     -0.1      0.2     -0.1     -0.1      2.9      3.5 \n\n# Column Effect\nbarplot(pop2020_polished$col, ylim=c(-10,60), main=\"부양유형_2020년\")\n\n\n\npop2020_polished$col\n\n        부모_스스로                가족 가족정부사회가_함께           정부_사회 \n                0.0                10.2                51.8                -9.4 \n               기타 \n              -11.6 \n\n\nRow 앞의 자료들과 엄밀한 비교가 하기 어려운 것이 Column의 항목들이 바뀌었기 때문이다. 2020년도 데이터는 가족에 의한 부양 항목들이 세분화되지 않았기 때문에 해석에 유의가 필요하다. 직접적으로 부양을 받는 세대인 은퇴를 앞둔 60대의 입장에서는 본인의 은퇴이후의 삶에 대한 고민이 반영되었기 때문에 부모부양에 대한 강한 인식을 보이고 있지만, 이외의 세대에서는 부양에 대한 인식이 그렇게 높지 않은 것이 확인되어 진다.\nColumn 단순히 가족 외부의 정부_사회적 제도에 의존 (-8.512)하려는 인식 보다는 가족 정부 사회가 함께(52.3) 부양에 대한 부담을 분담 해야 한다는 인식이 공유되어 있음이 확인되어 진다. 자립 혹은 가족에 대한 긍정적 인식은 적은데 한국사회의 경우 대부분의 노후자금이 토지에 묶여 있는 경우가 많고 자녀 교육 비용이 많이 지출되기 때문에 부모 스스로 부양을 하는 것이 쉽지 않다는 인식이 공유된 것으로 보인다.\n\nresidual Analysis\n\n# 1988\npar\n\nfunction (..., no.readonly = FALSE) \n{\n    .Pars.readonly <- c(\"cin\", \"cra\", \"csi\", \"cxy\", \"din\", \"page\")\n    single <- FALSE\n    args <- list(...)\n    if (!length(args)) \n        args <- as.list(if (no.readonly) \n            .Pars[-match(.Pars.readonly, .Pars)]\n        else .Pars)\n    else {\n        if (all(unlist(lapply(args, is.character)))) \n            args <- as.list(unlist(args))\n        if (length(args) == 1) {\n            if (is.list(args[[1L]]) || is.null(args[[1L]])) \n                args <- args[[1L]]\n            else if (is.null(names(args))) \n                single <- TRUE\n        }\n    }\n    value <- .External2(C_par, args)\n    if (single) \n        value <- value[[1L]]\n    if (!is.null(names(args))) \n        invisible(value)\n    else value\n}\n<bytecode: 0x000001f4940bb3e8>\n<environment: namespace:graphics>\n\nfor (i in 1:6) \n  {barplot(pop1988_polished$residuals[i,],main=rownames(pop1988)[i])}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor (i in 1:6)\n  {barplot(pop1988_polished$residuals[,i],main=names(pop1988)[i])}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10대 부터 30대 까지는 장남의 부양이 아닌 남녀에 상관없이 사회에 의존하거나 자녀 모두가 부모 부양을 분담해야한다는 인식이 많았으나, 40대 이후에서는 이러한 인식이 정 반대인 것으로 나타난다. 장남이 부모님의 부양을 담당해야한다는 인식이 가장 많았고 분담에 대한 부정적인 인식이 많았다.\n\n# 2018\n\nfor (i in 1:6) \n  {barplot(pop2018_polished$residuals[i,],main=rownames(pop2018)[i])}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor (i in 1:8) \n  {barplot(pop2018_polished$residuals[,i],main=names(pop2018)[i])}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n88년도에 20대였던 사람들이 50대, 30대였던 사람들이 60대인 시점 전 세대에서 장남 혼자, 딸 혼자, 능력있는 자식 등 한명이 부모님 부양에 대한 책임을 독점하는 것에 대해 부정적인 시각을 가지고 있다. 10~20대 까지는 자녀들이 부모님 부양에 대한 책임을 분담해야 한다고 주장하다가, 30대와 40대, 50대에서는 가족 정부 사회가 부모님 부양 책임을 분담해야한다는 인식이 가장 크다. 50대에서는 자녀들이 부모님 부양하는 것을 부정적으로 생각하는 것이 확인된다. 신기했던 포인트는 60대이상 계층에서 자립을 강조하는 모습이다. 가족이나 정부 사회에 의존한 부양에 대해서 부정적인 인식을 가지고 있는 것으로 보이므로 의존하지 않고 본인이 본인의 노후 설계를 해야 한다는 생각이 공유되는 것이 확인된다.\n\n#2020\n\nfor (i in 1:6) \n  {barplot(pop2020_polished$residuals[i,],main=rownames(pop2020)[i])}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor (i in 1:5)\n  {barplot(pop2020_polished$residuals[,i],main=names(pop2020)[i])}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n40대부터 50대 까지는 부모님 부양에 대한 직접 책임을 지기 때문에 가족에 의한 부양에 대해서 부정적인 인식을 보이고 있는 것이 확인되어 진다. 60대 이상의 계층에서는 가족정부사회가 함께 부양을 해야 한다고 보지 않고 전통적인 노인 부양 방식에 대해서 오히려 긍정적인 인식을 가진 것을 확인할 수 있다. 10대의 20대 경우 가족에 의한 노인 부양에 긍정적인 시선을 가지고 있는데 이는 교육에 의한 규범적인 요인들에 의해 형성된 시각이라고 생각한다. 특이점은 이전 조사들과는 다르게 30대에서 부모님의 자립에 긍정적인 인식이 있는 것이 확인되어 지는데 일시적으로 확인되는 이상치일 가능성이 있다."
  },
  {
    "objectID": "posts/EDA5/과제5.html#two-way-plot-과-coded-plot",
    "href": "posts/EDA5/과제5.html#two-way-plot-과-coded-plot",
    "title": "EDA Assignment 5: Chapter 7",
    "section": "two-way plot 과 Coded plot",
    "text": "two-way plot 과 Coded plot\n\n1988\n\n1988년도 Two way plot\n\n\nfit_1988=pop1988_polished$overall + outer(pop1988_polished$row,pop1988_polished$col, \"+\")\nfit_1988=fit_1988[c(order(fit_1988[,1],decreasing=T)),]\nfit_1988=fit_1988[,c(order(fit_1988[1,],decreasing=T))]\n(resid_1988=pop1988_polished$residuals)\n\n           장남 아들모두     딸 아들딸모두   자립 사회및기타\n15~19세  -8.375   -4.225 -0.175      8.875  0.175      1.325\n20~29세  -7.625   -2.075 -0.225      8.325  0.225      1.075\n30~39세  -4.875   -0.725 -0.175      3.775  1.675      0.325\n40~49세   4.875    0.725  0.175     -3.775 -0.175     -0.325\n50~59세  12.975    3.625  0.475     -8.775 -3.475     -0.325\n60세이상 24.575    2.725  0.675    -14.475 -7.875     -0.525\n\nfivenum(as.vector(resid_1988)) # hinge 밖에 있는 값들은 Residual로 표현 안함\n\n[1] -14.475  -2.775   0.000   1.500  24.575\n\n\n\ntwoway::twoway(pop1988,method=\"median\")\n\n\nMedian polish decomposition (Dataset: \"pop1988\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n           장남    아들모두 딸      아들딸모두 자립    사회및기타   roweff \n         + ------- -------  ------- -------    ------- -------    + -------\n15~19세  |  -8.375  -4.225   -0.175   8.875      0.175   1.325    :   0.525\n20~29세  |  -7.625  -2.075   -0.225   8.325      0.225   1.075    :   0.175\n30~39세  |  -4.875  -0.725   -0.175   3.775      1.675   0.325    :   0.125\n40~49세  |   4.875   0.725    0.175  -3.775     -0.175  -0.325    :  -0.125\n50~59세  |  12.975   3.625    0.475  -8.775     -3.475  -0.325    :  -0.625\n60세이상 |  24.575   2.725    0.675 -14.475     -7.875  -0.525    :  -0.725\n         + ....... .......  ....... .......    ....... .......    + .......\ncoleff   |   6.800   0.750  -17.000  17.550     -0.750 -12.800    :  17.450\n\nplot(twoway::twoway(pop1988,method=\"median\"))\n\n\n\n\n모든 자녀들이 부모님에 대한 부양을 분담해야 한다는 인식이 가장 높게 나타나고 그 다음은 장남이 책임져야 한다는 인식이 있는 것이 확인된다. 딸이 부모님 부양하는 것에 대한 인식은 낮은 편이다. 15~19 세 그룹에서 가장 부모님 부양에 대한 긍정적 인식을 보이고 있으며, 반면 60 세 이상 그룹에서는 부모님 부양에 대한 부정적 인식이 있는 것이 확인된다. 나이가 어릴수록 부모님 부양에 대해서 긍정적인 인식이 있는 것이 확인 되어진다.\n\n1988년도 coded plot\n\n\nstandard=as.vector(resid_1988)\nfivenum(standard)\n\n[1] -14.475  -2.775   0.000   1.500  24.575\n\nHspread_1988=fivenum(standard)[4]-fivenum(standard)[2]\ninner_1988=c(fivenum(standard)[4]+1.5*Hspread_1988,fivenum(standard)[2]-1.5*Hspread_1988)\nouter_1988=c(fivenum(standard)[4]+3*Hspread_1988,fivenum(standard)[2]-3*Hspread_1988)\nHspread_1988; inner_1988; outer_1988\n\n[1] 4.275\n\n\n[1]  7.9125 -9.1875\n\n\n[1]  14.325 -15.600\n\nresid_1988=as.data.frame(resid_1988)\n# 교과서 기준과 동일하게 \"M\" - Far outside low, \"=\"- below low inner fence(outise), \"-\" - Below lower hinge but within inner fence\n# \".\" - Between Hinges , \"+\"- Above upper hinge, \"#\" - Above High inner fence (outside), \"F\" - Far outside high\nresid_1988_coded=ifelse(resid_1988 >= outer_1988[1], \"F\",\n                        ifelse(resid_1988 >= inner_1988[1], \"#\",\n                               ifelse(resid_1988 >= fivenum(standard)[4],\"+\",\n                                      ifelse(resid_1988 >=fivenum(standard)[2] ,\".\",\n                                             ifelse(resid_1988 >= inner_1988[2],\"-\",\n                                                    ifelse(resid_1988 >= outer_1988[2],\"=\",\"M\"))))))\nresid_1988\n\n           장남 아들모두     딸 아들딸모두   자립 사회및기타\n15~19세  -8.375   -4.225 -0.175      8.875  0.175      1.325\n20~29세  -7.625   -2.075 -0.225      8.325  0.225      1.075\n30~39세  -4.875   -0.725 -0.175      3.775  1.675      0.325\n40~49세   4.875    0.725  0.175     -3.775 -0.175     -0.325\n50~59세  12.975    3.625  0.475     -8.775 -3.475     -0.325\n60세이상 24.575    2.725  0.675    -14.475 -7.875     -0.525\n\n(resid_1988_coded=as.data.frame(resid_1988_coded))\n\n         장남 아들모두 딸 아들딸모두 자립 사회및기타\n15~19세     -        -  .          #    .          .\n20~29세     -        .  .          #    .          .\n30~39세     -        .  .          +    +          .\n40~49세     +        .  .          -    .          .\n50~59세     #        +  .          -    -          .\n60세이상    F        +  .          =    -          .\n\n\n60 세 이상 집단에서는 자립을 강조하고 정부나 사회에 의존하는 것에 대한 부정적인 인식이 확인되어진다. 반면 10 대나 20 대의 경우 모든 자녀들이 부모님 부양에 대한 부담을 분담해야 한다고 보고 있는 것으로 보인다. 30~50 대에서는 부모님 부양에 대한 책임을 가족 정부 사회가 분담해야 한다고 보고 있다. 이러한 이유를 부모님 부양에 대한 부담을 느끼기 때문으로 보고 있다.\n\n\n2018\n\n2018년도 Two way plot\n\n\nfit_2018=pop2018_polished$overall + outer(pop2018_polished$row,pop2018_polished$col, \"+\")\nfit_2018=fit_2018[c(order(fit_2018[,1],decreasing=T)),]\nfit_2018=fit_2018[,c(order(fit_2018[1,],decreasing=T))]\n(resid_2018=pop2018_polished$residuals)\n\n             자립     장남 아들모두       딸 모든_자녀     능력 가족정부사회\n13∼19세  0.06562 -0.06875  0.79688  0.56250   8.21250 -0.35312     -4.40313\n20∼29세  1.40312 -0.43125 -0.06562  0.10000   2.45000  0.08438     -1.06563\n30∼39세 -0.49687 -0.23125  0.13438  0.00000   0.55000 -0.01562      1.83437\n40∼49세 -0.89688  0.06875  0.03438  0.00000  -0.55000 -0.21562      2.53437\n50∼59세 -0.06562  0.20000 -0.03438 -0.06875  -1.01875  0.01562      1.06563\n60세이상  6.09063  0.75625 -0.27812 -0.91250  -2.26250  0.27187    -12.87813\n         정부사회\n13∼19세 -0.53750\n20∼29세 -1.10000\n30∼39세 -0.40000\n40∼49세  0.40000\n50∼59세  0.73125\n60세이상  1.68750\n\nfivenum(as.vector(resid_2018)) # hinge 밖에 있는 값들은 Residual로 표현 안함\n\n[1] -12.878130  -0.415625   0.000000   0.556250   8.212500\n\n\n\ntwoway::twoway(pop2018,method=\"median\")\n\n\nMedian polish decomposition (Dataset: \"pop2018\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n           자립      장남      아들모두  딸        모든_자녀 능력     \n         + --------- --------- --------- --------- --------- ---------\n13∼19세 |   0.06562  -0.06875   0.79688   0.56250   8.21250  -0.35312\n20∼29세 |   1.40312  -0.43125  -0.06562   0.10000   2.45000   0.08438\n30∼39세 |  -0.49687  -0.23125   0.13438   0.00000   0.55000  -0.01562\n40∼49세 |  -0.89688   0.06875   0.03438   0.00000  -0.55000  -0.21562\n50∼59세 |  -0.06562   0.20000  -0.03438  -0.06875  -1.01875   0.01562\n60세이상 |   6.09063   0.75625  -0.27812  -0.91250  -2.26250   0.27187\n         + ......... ......... ......... ......... ......... .........\ncoleff   |  12.94688  -3.81875  -4.08438  -4.65000  13.70000  -0.23438\n         가족정부사회 정부사회    roweff   \n         ---------    --------- + ---------\n13∼19세  -4.40313     -0.53750 :  -0.36250\n20∼29세  -1.06563     -1.10000 :   0.00000\n30∼39세   1.83437     -0.40000 :   0.00000\n40∼49세   2.53437      0.40000 :   0.00000\n50∼59세   1.06563      0.73125 :   0.06875\n60세이상 -12.87813      1.68750 :   1.11250\n         .........    ......... + .........\ncoleff    45.71562      0.25000 :   4.85000\n\nplot(twoway::twoway(pop2018,method=\"median\"))\n\n\n\n\n가족 정부 사회가 협력하여 부모님 부양을 담당해야 한다는 인식이 높은 것으로 보인다. 특이하게, 가족이 배제된 부모님 부양에 대한 인식은 부정적인 것으로 보인다. 60 세 이상에서 88 년도와는 다르게 부모님 부양에 대해 인식이 개선됨 (BUT 88 년도와 20 년도의 Column 항목들이 다르기 때문에 단순히 전반적 인식이 개선되었다라고 보기에는 한계가 있음) 60 세 이상,65 세 이상 집단에서 부모님 부양에 대한 인식이 높은 것으로 보이며 30 대,40 대,50 대가 서로 비슷한 수준의 부모님 부양에 대한 인식을 가지고 있는 것으로 보인다.\n\n2018년도 coded plot\n\n\nresid_2018=pop2018_polished$residuals\nstandard=as.vector(resid_2018)\nfivenum(standard)\n\n[1] -12.878130  -0.415625   0.000000   0.556250   8.212500\n\nHspread_2018=fivenum(standard)[4]-fivenum(standard)[2]\ninner_2018=c(fivenum(standard)[4]+1.5*Hspread_2018,fivenum(standard)[2]-1.5*Hspread_2018)\nouter_2018=c(fivenum(standard)[4]+3*Hspread_2018,fivenum(standard)[2]-3*Hspread_2018)\nHspread_2018; inner_2018; outer_2018\n\n[1] 0.971875\n\n\n[1]  2.014063 -1.873438\n\n\n[1]  3.471875 -3.331250\n\nresid_2018=as.data.frame(resid_2018)\n# 교과서 기준과 동일하게 \"M\" - Far outside low, \"=\"- below low inner fence(outise), \"-\" - Below lower hinge but within inner fence\n# \".\" - Between Hinges , \"+\"- Above upper hinge, \"#\" - Above High inner fence (outside), \"F\" - Far outside high\nresid_2018_coded=ifelse(resid_2018 >= outer_2018[1], \"F\",\n       ifelse(resid_2018 >= inner_2018[1], \"#\",\n              ifelse(resid_2018 >= fivenum(standard)[4],\"+\",\n                     ifelse(resid_2018 >=fivenum(standard)[2] ,\".\",\n                            ifelse(resid_2018>= inner_2018[2],\"-\",\n                                   ifelse(resid_2018 >= outer_2018[2],\"=\",\"M\"))))))\nresid_2018\n\n             자립     장남 아들모두       딸 모든_자녀     능력 가족정부사회\n13∼19세  0.06562 -0.06875  0.79688  0.56250   8.21250 -0.35312     -4.40313\n20∼29세  1.40312 -0.43125 -0.06562  0.10000   2.45000  0.08438     -1.06563\n30∼39세 -0.49687 -0.23125  0.13438  0.00000   0.55000 -0.01562      1.83437\n40∼49세 -0.89688  0.06875  0.03438  0.00000  -0.55000 -0.21562      2.53437\n50∼59세 -0.06562  0.20000 -0.03438 -0.06875  -1.01875  0.01562      1.06563\n60세이상  6.09063  0.75625 -0.27812 -0.91250  -2.26250  0.27187    -12.87813\n         정부사회\n13∼19세 -0.53750\n20∼29세 -1.10000\n30∼39세 -0.40000\n40∼49세  0.40000\n50∼59세  0.73125\n60세이상  1.68750\n\n(resid_2018_coded=as.data.frame(resid_2018_coded))\n\n         자립 장남 아들모두 딸 모든_자녀 능력 가족정부사회 정부사회\n13∼19세    .    .        +  +         F    .            M        -\n20∼29세    +    -        .  .         #    .            -        -\n30∼39세    -    .        .  .         .    .            +        .\n40∼49세    -    .        .  .         -    .            #        .\n50∼59세    .    .        .  .         -    .            +        +\n60세이상    F    +        .  -         =    .            M        +\n\n\n\n\n2020\n\n2020년도 Two way plot\n\n\nf=pop2020_polished$overall + outer(pop2020_polished$row,pop2020_polished$col, \"+\")\n\nf=f[c(order(f[,1],decreasing=T)),]\nf=f[,c(order(f[1,],decreasing=T))]\n\n(resid_2020=pop2020_polished$residuals)\n\n         부모_스스로 가족 가족정부사회가_함께 정부_사회 기타\n15∼19세        -0.6  5.3                -4.6       0.4  0.0\n20∼29세         0.0  0.0                 1.0       0.0  0.0\n30∼39세         2.5 -2.7                 0.0       0.0 -0.3\n40∼49세        -0.6 -2.4                 3.1       0.9  0.0\n50∼59세        -0.9 -2.6                 3.1       1.4  0.0\n60세이상         1.4  1.2               -13.6       0.0 -3.0\n65세이상         1.8  1.9               -17.1       0.0 -3.6\n\nfivenum(as.vector(resid_2020)) # hinge 밖에 있는 값들은 Residual로 표현 안함\n\n[1] -17.10  -0.75   0.00   1.10   5.30\n\n\n\ntwoway::twoway(pop2020,method=\"median\")\n\n\nMedian polish decomposition (Dataset: \"pop2020\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n           부모_스스로 가족  가족정부사회가_함께 정부_사회 기타    roweff\n         + -----       ----- -----               -----     ----- + ----- \n15∼19세 |  -0.6         5.3  -4.6                 0.4       0.0 :   0.0 \n20∼29세 |   0.0         0.0   1.0                 0.0       0.0 :  -0.1 \n30∼39세 |   2.5        -2.7   0.0                 0.0      -0.3 :   0.2 \n40∼49세 |  -0.6        -2.4   3.1                 0.9       0.0 :  -0.1 \n50∼59세 |  -0.9        -2.6   3.1                 1.4       0.0 :  -0.1 \n60세이상 |   1.4         1.2 -13.6                 0.0      -3.0 :   2.9 \n65세이상 |   1.8         1.9 -17.1                 0.0      -3.6 :   3.5 \n         + .....       ..... .....               .....     ..... + ..... \ncoleff   |   0.0        10.2  51.8                -9.4     -11.6 :  11.7 \n\nplot(twoway::twoway(pop2020,method=\"median\"))\n\n\n\n\n가족 정부 사회가 협력하여 부모님 부양을 담당해야 한다는 인식이 높은 것으로 보인다. 특이하게, 가족이 배제된 부모님 부양에 대한 인식은 부정적인 것으로 보인다. 60 세 이상에서 88 년도와는 다르게 부모님 부양에 대해 인식이 개선됨 (BUT 88 년도와 20 년도의 Column 항목들이 다르기 때문에 단순히 전반적 인식이 개선되었다라고 보기에는 한계가 있음) 60 세 이상,65 세 이상 집단에서 부모님 부양에 대한 인식이 높은 것으로 보이며 30 대,40 대,50 대가 서로 비슷한 수준의 부모님 부양에 대한 인식을 가지고 있는 것으로 보인다.\n\n2020년도 coded plot\n\n\nresid_2020=pop2020_polished$residuals\nstandard=as.vector(resid_2020)\nfivenum(standard)\n\n[1] -17.10  -0.75   0.00   1.10   5.30\n\nHspread_2020=fivenum(standard)[4]-fivenum(standard)[2]\ninner_2020=c(fivenum(standard)[4]+1.5*Hspread_2020,fivenum(standard)[2]-1.5*Hspread_2020)\nouter_2020=c(fivenum(standard)[4]+3*Hspread_2020,fivenum(standard)[2]-3*Hspread_2020)\nHspread_2020; inner_2020; outer_2020\n\n[1] 1.85\n\n\n[1]  3.875 -3.525\n\n\n[1]  6.65 -6.30\n\nresid_2020=as.data.frame(resid_2020)\n# 교과서 기준과 동일하게 \"M\" - Far outside low, \"=\"- below low inner fence(outise), \"-\" - Below lower hinge but within inner fence\n# \".\" - Between Hinges , \"+\"- Above upper hinge, \"#\" - Above High inner fence (outside), \"F\" - Far outside high\nresid_2020_coded=ifelse(resid_2020 >= outer_2020[1], \"F\",\n       ifelse(resid_2020 >= inner_2020[1], \"#\",\n              ifelse(resid_2020 >= fivenum(standard)[4],\"+\",\n                     ifelse(resid_2020 >=fivenum(standard)[2] ,\".\",\n                            ifelse(resid_2020>= inner_2020[2],\"-\",\n                                   ifelse(resid_2020 >= outer_2020[2],\"=\",\"M\"))))))\nresid_2020\n\n         부모_스스로 가족 가족정부사회가_함께 정부_사회 기타\n15∼19세        -0.6  5.3                -4.6       0.4  0.0\n20∼29세         0.0  0.0                 1.0       0.0  0.0\n30∼39세         2.5 -2.7                 0.0       0.0 -0.3\n40∼49세        -0.6 -2.4                 3.1       0.9  0.0\n50∼59세        -0.9 -2.6                 3.1       1.4  0.0\n60세이상         1.4  1.2               -13.6       0.0 -3.0\n65세이상         1.8  1.9               -17.1       0.0 -3.6\n\n(resid_2020_coded=as.data.frame(resid_2020_coded))\n\n         부모_스스로 가족 가족정부사회가_함께 정부_사회 기타\n15∼19세           .    #                   =         .    .\n20∼29세           .    .                   .         .    .\n30∼39세           +    -                   .         .    .\n40∼49세           .    -                   +         .    .\n50∼59세           -    -                   +         +    .\n60세이상           +    +                   M         .    -\n65세이상           +    +                   M         .    =\n\n\n60 세 이상집단에서 외부의 도움보다는 가족 혹은 본인이 부모님 부양을 담당해야 한다고 생각하는 것으로 보인다. 이러한 이유는 나이가 많은 집단에서 외부의 도움을 별로 좋아하지 않기 때문이라고 보인다. 88 년도 자료에서와 엄밀한 비교는 어렵지만 정부나 사회에 도움을 받는 것을 꺼려하는 60 대 이상 계층들의 태도는 일관적일 것으로 보인다. 반면, 40 대 50 대 집단에서는 가족정부사회가 함께 부모님 부양을 담당해야 한다는 인식이 있는데 실제로 40 대~50 대가 부모님 부양을 담당하면서 부담을 느끼기 때문이라고 추측할 수 있다."
  },
  {
    "objectID": "posts/EDA6/과제6.html",
    "href": "posts/EDA6/과제6.html",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "",
    "text": "8장. 시계열 자료의 탐색"
  },
  {
    "objectID": "posts/EDA6/과제6.html#번",
    "href": "posts/EDA6/과제6.html#번",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "1번",
    "text": "1번\nFRIDAY.DAT 자료는 영국에서 1986년 금요일에 발생한 자동차 사고 사망자 수이다. 첫 열은 하루를 24시간으로 표시한 것이고, 두 번째 열은 사망자 수이다. 시간에 따른 사망자 수를 평활하고 시간과 사망자 수의 관계를 서술하여라.\n데이터를 불러오고,첫번째 줄에 있는 데이터를 0시부터 23시로 변경하여 Time-series Plot을 그릴 때 보다 가독성을 좋게 만들고자 하였다. Time은 시작시간을 기준으로 한시간의 데이터를 포함하며 시작시간은 당일 0시부터 끝나는 시간은 익일 0시까지이다.\n\npacman::p_load(\"haven\",\"sleekts\")\nfriday=data.frame(\n  stringsAsFactors = FALSE,\n                V1 = c(\"0-1\",\"1-2\",\"2-3\",\"3-4\",\n                       \"4-5\",\"5-6\",\"6-7\",\"7-8\",\"8-9\",\"9-10\",\"10-11\",\"11-12\",\n                       \"12-13\",\"13-14\",\"14-15\",\"15-16\",\"16-17\",\"17-18\",\n                       \"18-19\",\"19-20\",\"20-21\",\"21-22\",\"22-23\",\"23-0\"),\n                V2 = c(938L,621L,455L,207L,138L,\n                       215L,526L,1933L,3377L,2045L,2078L,2351L,3015L,\n                       2966L,2912L,4305L,4923L,4427L,3164L,2950L,2601L,2420L,\n                       2557L,4319L)\n)\nnames(friday)=c(\"Time\", \"Deaths\")\nfriday$Time=0:23\nfriday\n\n   Time Deaths\n1     0    938\n2     1    621\n3     2    455\n4     3    207\n5     4    138\n6     5    215\n7     6    526\n8     7   1933\n9     8   3377\n10    9   2045\n11   10   2078\n12   11   2351\n13   12   3015\n14   13   2966\n15   14   2912\n16   15   4305\n17   16   4923\n18   17   4427\n19   18   3164\n20   19   2950\n21   20   2601\n22   21   2420\n23   22   2557\n24   23   4319\n\n\n결측치를 확인하고 Box plot을 활용해서 Outlier가 존재하는지를 확인한다.\n\nsum(is.na(friday)) # 0 결측치는 존재하지 않음.\n\n[1] 0\n\nboxplot(friday$Deaths,main= \"Checking Outlier of Deaths Data\") \n\n\n\n#outlier는 존재하지 않는 것이 확인되어진다. \n\nBoxplot을 통해 확인 할 경우 outlier는 확인되어지지는 않으나 Whisker의 길이가 길게 뻗어 있는 것이 확인되어 진다. Whisker의 길이와 median의 위치를 고려해볼 때, 자료가 완벽하게 대칭적이지는 않은 것으로 확인되어 진다.\n\nstem(friday$Deaths)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 1225569\n  1 | 9\n  2 | 0144669\n  3 | 00024\n  4 | 3349\n\nstem(friday$Deaths,2)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 122\n  0 | 5569\n  1 | \n  1 | 9\n  2 | 0144\n  2 | 669\n  3 | 00024\n  3 | \n  4 | 334\n  4 | 9\n\n\nStem and Leaf Plot을 통해 데이터를 분석할 경우 데이터의 특성을 분석하려고 하였으나 어떠한 특징을 보이고 있는 것으로 보이지는 않는다. 다만, Stem Plot의 Scale을 약간 조정할 경우 Gap이 있는 것으로 보이는데 세개의 그룹으로 나누어지는 것 처럼 보인다.\n\nskewness = function(x) {\n  hl=fivenum(x)[2]\n  median=fivenum(x)[3]\n  hu=fivenum(x)[4]\n  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))\n  return(skew)\n}\nskewness(friday$Deaths)\n\n[1] -0.4796537\n\n\n데이터의 Skewness를 고려해 볼 떄 Box plot에서 확인되였던 것 처럼, 해당 데이터는 Skewed to the left되어 있다는 사실을 확인할 수 있다.\n\nskewness(log(friday$Deaths))\n\n[1] -0.6903753\n\nskewness(sqrt(friday$Deaths))\n\n[1] -0.5900806\n\nskewness(-1/(friday$Deaths))\n\n[1] -0.8456372\n\nskewness(-1/sqrt(friday$Deaths))\n\n[1] -0.7763614\n\n# 변환을 하면 오히려 skewness가 더 커지고 있다.\n\n\n# letter value display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\n(lvd=lsum(friday$Deaths,6)) # mid 값이 점차 커지는 중\n\n  letter depth  lower     mid  upper spread\n1      M  12.5 2488.5 2488.50 2488.5    0.0\n2      H   6.5  779.5 1934.50 3089.5 2310.0\n3      E   3.5  335.0 2323.50 4312.0 3977.0\n4      D   2.0  207.0 2317.00 4427.0 4220.0\n5      C   1.5  172.5 2423.75 4675.0 4502.5\n6      B   1.0  138.0 2530.50 4923.0 4785.0\n\n# Kurtosis (E-spread / H-spread - 1.705)\n(lvd[3,5]-lvd[3,3])/(lvd[2,5]-lvd[2,3]) -1.705 # more peaked than normal\n\n[1] 0.01664502\n\n\n\n데이터의 Five Numbers와 요약 지표\n\n\nfivenum(friday$Deaths)\n\n[1]  138.0  779.5 2488.5 3089.5 4923.0\n\nsummary(friday$Deaths)# Median, Mean이 큰 차이가 없다.\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  138.0   858.8  2488.5  2310.1  3052.2  4923.0 \n\nas.numeric(summary(friday$Deaths)[6]-summary(friday$Deaths)[1]) # 최고점과 최저점 사이의 차이는 4785\n\n[1] 4785\n\n\n최고값의 경우 4923회의 사고 / 최소값의 경우 138회의 사고 / 평균적으로 2370회의 사고가 발생하였다.\n정규성을 따르는가?\n\nqqnorm(friday$Deaths, ylab=\"Death quantiles\");qqline(friday$Deaths, col='red',lty=2)\n\n\n\nfiv=fivenum(friday$Deaths)\n(pseudosigma = (fiv[4]-fiv[2])/1.34)\n\n[1] 1723.881\n\nsd(friday$Deaths)\n\n[1] 1447.733\n\n\n\nqqnorm(friday$Deaths, ylab=\"Death quantiles\");qqline(friday$Deaths, col='red',lty=2)\nabline(fiv[3],pseudosigma,col=\"blue\",lty=2)\ntitle(sub=\"intercept=2488.5 (median); slope=1723.9 (pseudo-sigma) blue line\")\n\n\n\n\npseudo-sigma와 sigma가 큰 차이가 없고 자료들이 어느정도 직선을 따르고 있는 것을 보인다. (완벽히 직선을 이루는 것은 아니나 - 중간에 곡선형태를 보임 - 직선위에 있는 것처럼 ) 다만 위에서 Stem-Leaf Plot을 그리면서 언급하였던 것 처럼 세개의 cluster가 나타나고 있는 것이 확인되어 진다.\n\nlibrary(ggplot2)\nfriday2=friday\n((friday2$Deaths)/sum(friday2$Deaths))*100\n\n [1] 1.6918276 1.1200693 0.8206627 0.3733564 0.2489043 0.3877857 0.9487221\n [8] 3.4864636 6.0909402 3.6884728 3.7479934 4.2403910 5.4380174 5.3496384\n[15] 5.2522410 7.7647313 8.8793896 7.9847772 5.7067619 5.3207799 4.6913046\n[22] 4.3648432 4.6119438 7.7899825\n\nggplot(friday,aes(x=Deaths))+\n    geom_histogram(aes(y = stat(count) / sum(count)),binwidth=400, fill = \"pink\", colour = \"black\")+\n    labs(title=\"Histogram of deaths in friday\", x =\"# of Deaths\", y = \"Density\")+ \n    theme(plot.title = element_text(hjust = 0.5))+ \n    scale_y_continuous(labels = scales::percent)+\n    theme_minimal()+\n    ggeasy::easy_center_title()\n\n\n\n\nCluster를 재확인 할 수 있음. # K-means Clustering 사용하여 세개의 군집분류\n\nkmeans(friday$Deaths, 3) # 13개, 4개, 7개\n\nK-means clustering with 3 clusters of sizes 13, 7, 4\n\nCluster means:\n       [,1]\n1 2643.7692\n2  442.8571\n3 4493.5000\n\nClustering vector:\n [1] 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 3 3 3 1 1 1 1 1 3\n\nWithin cluster sum of squares by cluster:\n[1] 2544594.3  484446.9  254875.0\n (between_SS / total_SS =  93.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n군집분류를 할 경우 세개의 군집이 나타나는데 0시부터 ~ 6까지의 첫번째 Cluster는 심야시간대의 사고로 인한 사망이므로 수가 많지 않은 편이다. 반면, 두번째 Cluster는 일과시간대인 7시부터 15시, 퇴근시간 이후인 18시부터 23시의 경우 사망건수가 중간수준으로 나타나고, 15시부터 18시 사이와 23시부터 24시 사이의 3번째 Cluster에서 사망건수가 가장 많이 나타난다. 퇴근 시간대보다 일과시간대에서 사고가 더 많이 확인되는 것이 확인된다. 특이시간대는 23~24시 사이 구간인데, 음주를 하거나 모임을 가질 때 보통 23시 이후 집에 귀가를 많이 하는데 귀가 과정에서 음주운전 사고가 발생한 것으로 추측되어지지만 확실하지 않다.\n\nts_cluster=cbind(friday,as.factor(kmeans(friday$Deaths, 3)$cluster))\nnames(ts_cluster)=c(\"Time\",\"Deaths\",\"Cluster\")\nts_cluster\n\n   Time Deaths Cluster\n1     0    938       3\n2     1    621       3\n3     2    455       3\n4     3    207       3\n5     4    138       3\n6     5    215       3\n7     6    526       3\n8     7   1933       1\n9     8   3377       1\n10    9   2045       1\n11   10   2078       1\n12   11   2351       1\n13   12   3015       1\n14   13   2966       1\n15   14   2912       1\n16   15   4305       2\n17   16   4923       2\n18   17   4427       2\n19   18   3164       1\n20   19   2950       1\n21   20   2601       1\n22   21   2420       1\n23   22   2557       1\n24   23   4319       2\n\n\n\nggplot(data=ts_cluster,aes(x=Time, y=Deaths, colour=Cluster)) + \n   geom_point(shape=19, size=4) + \n   geom_text(label=ts_cluster$Time,hjust=0, vjust=-1)+\n   ggtitle(\"Scatter Plot of Deaths' K-means clusters\")+\n   theme_bw()\n\n\n\nts.plot(friday,main=\"Timeseries plot of deaths\")\n\n\n\n\n교통사고 사망자 수를 평활하기 위해서 수업시간에 교수님께서 Recommend 하신 3RSSH Twice방법과 4253H Twice방법을 활용하고자 한다.\nRecommend 해주신 방법들을 사용하기에 앞서 3RSS방법과 3RS3R방법으로 평활을 진행해 보았다.\n\nts.plot(friday,main=\"Default, 3RSS twice, 3RS3R twice\",ylab=\"Deaths\",lty=2)\nlines(ts(smooth(friday$Deaths, kind=\"3RSS\",twiceit=T)),col=\"red\")\nlines(ts(smooth(friday$Deaths, kind=\"3RS3R\",twiceit=T)),col=\"blue\")\nlegend(x = 1.5, y = 5000, c(\"Default\", \"3RSS Twice\",\"3RS3R Twice\"), \n      lty=c(2,1,1),lwd=2,col = c(\"black\",\"red\",\"blue\"))\n\n\n\n\nHanning을 적용하기 전 데이터이다. 23시부터 24시 구간의 이상치가 어느정도 완화된 것을 확인할 수 있다. (다만 그 정도가 커보이기 때문에 조정이 필요할 것으로 보인다.) 또한, 8시 부근의 뾰족한 지점이 어느정도 완화되었고, 다른 Cluster 3에 해당하던 값들의 크기도 다소 작아진 것이 확인되어진다. 다만, 평평한 부분이 연속적으로 존재하는 것이 확인되어지기에 Hanning의 적용이 필요할 것으로 보인다. (전반적으로 3RSS(split 두번)나 3RS3R(Repeated 3 두번)에서는 큰 차이가 나타나고 있지는 않은 것으로 보인다.)\n다음으로는, 3RSSH, twice방식을 적용한 평활을 적용하였다.\n다만, smoothing 함수에 3RSSH twice 방식의 평활을 지원하지 않기 때문에 함수를 만들어 smooting을 진행해야 할 것으로 보인다. https://blog.daum.net/wonil2480/13 의 방식을 참고하여 함수를 형성하였음.\n\nsmooth_3RSSH=function(data){\n    smooth3RSS=smooth(data, kind=\"3RSS\")\n    \n    n=length(data)\n    smooth3RSSH=smooth3RSS\n    \n    \n    for (i in 2:(n-1)) {smooth3RSSH[i] <- smooth3RSS[i-1]/4 + smooth3RSS[i]/2 + smooth3RSS[i+1]/4}\n    smooth3RSSH[1] <- smooth3RSS[1]; smooth3RSSH[n] <- smooth3RSS[n]\n    rough=data-smooth3RSSH\n    roughH=rough\n    \n    smooth3RSS2=smooth(rough,kind=\"3RSS\")\n    \n    for (i in 2:(n-1)) roughH[i] <- smooth3RSS2[i-1]/4 + smooth3RSS2[i]/2 + smooth3RSS2[i+1]/4\n    roughH[1] <- smooth3RSS2[1]; roughH[n] <- smooth3RSS2[n]\n    out=smooth3RSSH+roughH\n    out=as.vector(out)\n    return(out)}\n\n#install.packages(\"LearnEDA\")\n#library(LearnEDA)\n\n#smooth.3RSSH.twice=function(data)\n#{\n#SMOOTH=han(smooth(attend,kind=”3RSS”)) # 3RSSH smooth\n#ROUGH=data-SMOOTH                      # computes the rough\n#SMOOTH+han(smooth(ROUGH,kind=”3RSS”)) \n\n#원래 있는 패키지를 활용한 방식과도 비교하였으나 차이가 없었다. \n#(CRAN에서 현재 삭제되어 이전 패키지를 다운받아 사용하였음)\n\n\nts.plot(friday,main=\"Default, 3RSSH Twice\",ylab=\"Deaths\",lty=2)\nlines(ts(smooth_3RSSH(friday$Deaths)),col=\"goldenrod4\")\nlegend(x = 1.5, y = 5000, c(\"Default\", \"3RSSH Twice\"), \n      lty=c(2,1),lwd=2,col = c(\"black\",\"goldenrod4\"))\n\n\n\n\n3RSS나 3RS3R와 전반적인 추세선은 크게 다르지 않은 것으로 보인다. 다만 Cluster 3 (15시부터 18시 사이와 23시부터 24시 사이)에 해당하는 크기가 큰 자료들이 3RSSH자료에서 조금 더 잘 드러나고 있다. 동일하게 8시부터 9시 부근에 형성된 뾰족한 Outlier로 보이는 자료는 Smoothing 된것으로 확인된다.\n마지막으로 4253H, Twice 방식으로 평활한 데이터를 만들고자 한다. Sleekts 패키지의 sleek 활용하여 4253H, Twice 평활을 진행할 것이다.\n\n#install.packages(\"sleekts\")\nlibrary(sleekts)\nsleek(friday$Death)\n\n [1]  902.6289  609.1992  462.4844  345.0000  268.0039  230.3789  454.5977\n [8]  974.8359 1599.0742 2027.2812 2178.5938 2281.4062 2448.0938 2668.3320\n[15] 2987.7344 3400.8086 3741.2070 3926.9414 3626.9219 3201.0547 2775.1875\n[22] 2649.3398 3092.6641 3092.6641\n\nts.plot(friday,main=\"Default, 4253H Twice\",ylab=\"Deaths\",lty=2)\nlines(ts(sleek(friday$Death)),col=\"cyan3\")\n\nlegend(x = 1.5, y = 5000, c(\"Default\", \"4253H Twice\"), \n      lty=c(2,1),lwd=2,col = c(\"black\",\"cyan3\"))\n\n\n\n\n4253H 방식의 평활은 다른 방식들에 비해서 곡선의 형태가 부드럽게 나타나는 것이 확인된다. 또한 각 Cluster들 별로 특성이 명확하게 드러나고 있다. Cluster 3 (15시부터 18시 사이와 23시부터 24시 사이)에 해당하는 값들이 부드럽게 시계열 그래프 안에서 표현되고 있는 것을 확인할 수 있다. Raw Data에 존재하지 않는 값을 활용해서 양끝값을 극단치에 영향을 최소화한 형태로 표현했고, Hanning을 통해서 평평한 지점들을 보다 부드러운 곡선 형태로 만들었다는 점에서 다른 평활법에 비해서 더 잘 표현되어 있다고 볼 수 있다.\n\nts.plot(friday,main=\"ALL\",ylab=\"Deaths\",lty=2)\nlines(ts(smooth(friday$Deaths, kind=\"3RSS\")),col=\"red\")\nlines(ts(smooth(friday$Deaths, kind=\"3RS3R\")),col=\"blue\")\nlines(ts(smooth_3RSSH(friday$Deaths)),col=\"goldenrod4\")\nlines(ts(sleek(friday$Death)),col=\"cyan3\")\nlegend(x = 1.5, y = 5000, c(\"Default\",\"3RSS Twice\",\"3RS3R Twice\",\"3RSSH Twice\",\"4253H Twice\"), \n      lty=c(2,1,1,1,1),lwd=2,col = c(\"black\",\"red\",\"blue\",\"goldenrod4\",\"cyan3\"))\n\n\n\n\n모든 평활법을 비교할경우 다음과 같은 그래프를 그릴 수 있다. 앞에서 언급하였듯이 4253H, Twice 평활법이 가장 적절한 방법이라고 생각된다. 8시부터 9시 사이의 값, 16시 부터 17시 사이의 값 그리고 23시부터 24시 사이의 값이 변환전 자료에 비해 많이 감소되었다. 앞의 두 시간대는 출퇴근 시간이기 때문에 교통량이 많으므로 교통사고가 날 확률이 높아지고 사망사고가 발생할 확률도 다른 시간대에 비해서 높다고 볼 수 있다. 또한 23시부터 24시 시간대의 경우 앞에서 언급한 것과 같이 모든 평활법에서 실제 데이터보다 낮게 평활이 나타났는데, 전체적인 곡선을 고려할 때 이질적인 데이터이므로 그 원인에 대해서 심도있게 고민해볼 필요가 존재한다. Raw Data 기준으로 새벽시간대에는 교통량이 적어 사망사고건수도 적은 편에 비해서 일과시간대와 퇴근시간대에는 사망사고 건수가 많이 나타나고 있고 일과시간대에 퇴근시간대에 비해서 더 많은 사망사고가 발생한다는 것을 확인할 수 있다. 반면 평활된 데이터의 경우 퇴근시간대에 더 많은 사망사고가 발생하는 것으로 보여주고 있는데 평활된 데이터를 해석할 떄는 Raw Data를 유의해서 해석하는 것이 필요하다는 것을 확인할 수 있다.\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2564438/ 한 보고서에 따르면 사고가 발생하는 원인을 세가지를 제시하였는데 A credible physiological explanation for the importance of good lighting for road safety is provided; low luminance, low contrast images are processed slowly by the visual apparatus, due mainly to the limited temporal characteristics of the rod photoreceptors.\nthe rod photoreceptors의 제한된 시간적 특성은 야간의 낮은 불빛, 낮은 이미지의 대비로 인해서 시신경의 피로를 불러 일으키고 그 결과 높은 사망사고율로 이어졌음을 제시하였다.\n전날 심야시간대에 사망사고 발생건수가 낮은 것에 비해 토요일 0시에 사망사고 발생건수가 높아 진 것은 주말에 앞서 사람들이 밤 늦게 까지 돌아다녀서 그런 것으로 추측되어지기도 하다. 시계열데이터의 왼쪽과 오른쪽이 대칭적이지 않은 이유는 금요일 0시와 토요일 0시(금요일 24시)가 사람들의 시각에서 질적으로 다르기 때문이라고 생각한다. 띠라서 데이터를 더 잘 이해하기 위해서는 목요일과 토요일의 시계열 자료를 확인하는 것이 필요할 것으로 보인다. (요일적 요인의 고려)\n#2번 R의 datasets에 있는 sunspot.year 자료를 평활하고 그 패턴을 기술하라. 주기가 있는지? 많아지는 “속도”와 줄어드는 “속도”도 주목하여라.\n\nsunspot.year=tibble::tribble(\n    ~time, ~value,\n    1700L,      5,\n    1701L,     11,\n    1702L,     16,\n    1703L,     23,\n    1704L,     36,\n    1705L,     58,\n    1706L,     29,\n    1707L,     20,\n    1708L,     10,\n    1709L,      8,\n    1710L,      3,\n    1711L,      0,\n    1712L,      0,\n    1713L,      2,\n    1714L,     11,\n    1715L,     27,\n    1716L,     47,\n    1717L,     63,\n    1718L,     60,\n    1719L,     39,\n    1720L,     28,\n    1721L,     26,\n    1722L,     22,\n    1723L,     11,\n    1724L,     21,\n    1725L,     40,\n    1726L,     78,\n    1727L,    122,\n    1728L,    103,\n    1729L,     73,\n    1730L,     47,\n    1731L,     35,\n    1732L,     11,\n    1733L,      5,\n    1734L,     16,\n    1735L,     34,\n    1736L,     70,\n    1737L,     81,\n    1738L,    111,\n    1739L,    101,\n    1740L,     73,\n    1741L,     40,\n    1742L,     20,\n    1743L,     16,\n    1744L,      5,\n    1745L,     11,\n    1746L,     22,\n    1747L,     40,\n    1748L,     60,\n    1749L,   80.9,\n    1750L,   83.4,\n    1751L,   47.7,\n    1752L,   47.8,\n    1753L,   30.7,\n    1754L,   12.2,\n    1755L,    9.6,\n    1756L,   10.2,\n    1757L,   32.4,\n    1758L,   47.6,\n    1759L,     54,\n    1760L,   62.9,\n    1761L,   85.9,\n    1762L,   61.2,\n    1763L,   45.1,\n    1764L,   36.4,\n    1765L,   20.9,\n    1766L,   11.4,\n    1767L,   37.8,\n    1768L,   69.8,\n    1769L,  106.1,\n    1770L,  100.8,\n    1771L,   81.6,\n    1772L,   66.5,\n    1773L,   34.8,\n    1774L,   30.6,\n    1775L,      7,\n    1776L,   19.8,\n    1777L,   92.5,\n    1778L,  154.4,\n    1779L,  125.9,\n    1780L,   84.8,\n    1781L,   68.1,\n    1782L,   38.5,\n    1783L,   22.8,\n    1784L,   10.2,\n    1785L,   24.1,\n    1786L,   82.9,\n    1787L,    132,\n    1788L,  130.9,\n    1789L,  118.1,\n    1790L,   89.9,\n    1791L,   66.6,\n    1792L,     60,\n    1793L,   46.9,\n    1794L,     41,\n    1795L,   21.3,\n    1796L,     16,\n    1797L,    6.4,\n    1798L,    4.1,\n    1799L,    6.8,\n    1800L,   14.5,\n    1801L,     34,\n    1802L,     45,\n    1803L,   43.1,\n    1804L,   47.5,\n    1805L,   42.2,\n    1806L,   28.1,\n    1807L,   10.1,\n    1808L,    8.1,\n    1809L,    2.5,\n    1810L,      0,\n    1811L,    1.4,\n    1812L,      5,\n    1813L,   12.2,\n    1814L,   13.9,\n    1815L,   35.4,\n    1816L,   45.8,\n    1817L,   41.1,\n    1818L,   30.1,\n    1819L,   23.9,\n    1820L,   15.6,\n    1821L,    6.6,\n    1822L,      4,\n    1823L,    1.8,\n    1824L,    8.5,\n    1825L,   16.6,\n    1826L,   36.3,\n    1827L,   49.6,\n    1828L,   64.2,\n    1829L,     67,\n    1830L,   70.9,\n    1831L,   47.8,\n    1832L,   27.5,\n    1833L,    8.5,\n    1834L,   13.2,\n    1835L,   56.9,\n    1836L,  121.5,\n    1837L,  138.3,\n    1838L,  103.2,\n    1839L,   85.7,\n    1840L,   64.6,\n    1841L,   36.7,\n    1842L,   24.2,\n    1843L,   10.7,\n    1844L,     15,\n    1845L,   40.1,\n    1846L,   61.5,\n    1847L,   98.5,\n    1848L,  124.7,\n    1849L,   96.3,\n    1850L,   66.6,\n    1851L,   64.5,\n    1852L,   54.1,\n    1853L,     39,\n    1854L,   20.6,\n    1855L,    6.7,\n    1856L,    4.3,\n    1857L,   22.7,\n    1858L,   54.8,\n    1859L,   93.8,\n    1860L,   95.8,\n    1861L,   77.2,\n    1862L,   59.1,\n    1863L,     44,\n    1864L,     47,\n    1865L,   30.5,\n    1866L,   16.3,\n    1867L,    7.3,\n    1868L,   37.6,\n    1869L,     74,\n    1870L,    139,\n    1871L,  111.2,\n    1872L,  101.6,\n    1873L,   66.2,\n    1874L,   44.7,\n    1875L,     17,\n    1876L,   11.3,\n    1877L,   12.4,\n    1878L,    3.4,\n    1879L,      6,\n    1880L,   32.3,\n    1881L,   54.3,\n    1882L,   59.7,\n    1883L,   63.7,\n    1884L,   63.5,\n    1885L,   52.2,\n    1886L,   25.4,\n    1887L,   13.1,\n    1888L,    6.8,\n    1889L,    6.3,\n    1890L,    7.1,\n    1891L,   35.6,\n    1892L,     73,\n    1893L,   85.1,\n    1894L,     78,\n    1895L,     64,\n    1896L,   41.8,\n    1897L,   26.2,\n    1898L,   26.7,\n    1899L,   12.1,\n    1900L,    9.5,\n    1901L,    2.7,\n    1902L,      5,\n    1903L,   24.4,\n    1904L,     42,\n    1905L,   63.5,\n    1906L,   53.8,\n    1907L,     62,\n    1908L,   48.5,\n    1909L,   43.9,\n    1910L,   18.6,\n    1911L,    5.7,\n    1912L,    3.6,\n    1913L,    1.4,\n    1914L,    9.6,\n    1915L,   47.4,\n    1916L,   57.1,\n    1917L,  103.9,\n    1918L,   80.6,\n    1919L,   63.6,\n    1920L,   37.6,\n    1921L,   26.1,\n    1922L,   14.2,\n    1923L,    5.8,\n    1924L,   16.7,\n    1925L,   44.3,\n    1926L,   63.9,\n    1927L,     69,\n    1928L,   77.8,\n    1929L,   64.9,\n    1930L,   35.7,\n    1931L,   21.2,\n    1932L,   11.1,\n    1933L,    5.7,\n    1934L,    8.7,\n    1935L,   36.1,\n    1936L,   79.7,\n    1937L,  114.4,\n    1938L,  109.6,\n    1939L,   88.8,\n    1940L,   67.8,\n    1941L,   47.5,\n    1942L,   30.6,\n    1943L,   16.3,\n    1944L,    9.6,\n    1945L,   33.2,\n    1946L,   92.6,\n    1947L,  151.6,\n    1948L,  136.3,\n    1949L,  134.7,\n    1950L,   83.9,\n    1951L,   69.4,\n    1952L,   31.5,\n    1953L,   13.9,\n    1954L,    4.4,\n    1955L,     38,\n    1956L,  141.7,\n    1957L,  190.2,\n    1958L,  184.8,\n    1959L,    159,\n    1960L,  112.3,\n    1961L,   53.9,\n    1962L,   37.5,\n    1963L,   27.9,\n    1964L,   10.2,\n    1965L,   15.1,\n    1966L,     47,\n    1967L,   93.8,\n    1968L,  105.9,\n    1969L,  105.5,\n    1970L,  104.5,\n    1971L,   66.6,\n    1972L,   68.9,\n    1973L,     38,\n    1974L,   34.5,\n    1975L,   15.5,\n    1976L,   12.6,\n    1977L,   27.5,\n    1978L,   92.5,\n    1979L,  155.4,\n    1980L,  154.7,\n    1981L,  140.5,\n    1982L,  115.9,\n    1983L,   66.6,\n    1984L,   45.9,\n    1985L,   17.9,\n    1986L,   13.4,\n    1987L,   29.2,\n    1988L,  100.2\n    )\n\n\nnames(sunspot.year)=c(\"Time\", \"Value\")\nhead(sunspot.year,5)\n\n# A tibble: 5 × 2\n   Time Value\n  <int> <dbl>\n1  1700     5\n2  1701    11\n3  1702    16\n4  1703    23\n5  1704    36\n\n\nBoxplot을 그려서 Outlier 존재 여부를 확인하고, 결측치의 존재여부를 확인한다.\n\nboxplot(sunspot.year$Value) # Outer Fence 밖의 값들이 다수 존재하는 것으로 보인다.\n\n\n\nsum(is.na(sunspot.year$Value)) # 0이라 결측치가 존재하지 않는다.\n\n[1] 0\n\n\n\n(boxplot(sunspot.year$Value))\n\n$stats\n      [,1]\n[1,]   0.0\n[2,]  15.6\n[3,]  39.0\n[4,]  68.9\n[5,] 141.7\n\n$n\n[1] 289\n\n$conf\n         [,1]\n[1,] 34.04624\n[2,] 43.95376\n\n$out\n[1] 154.4 151.6 190.2 184.8 159.0 155.4 154.7\n\n$group\n[1] 1 1 1 1 1 1 1\n\n$names\n[1] \"1\"\n\nlength((boxplot(sunspot.year$Value))$out) # 총 7개의 Outlier가 존재한다. \n\n\n\n\n[1] 7\n\n# upper outer fence의 값은 141.7이다.\nsubset(sunspot.year,sunspot.year$Value>141.7) \n\n# A tibble: 7 × 2\n   Time Value\n  <int> <dbl>\n1  1778  154.\n2  1947  152.\n3  1957  190.\n4  1958  185.\n5  1959  159 \n6  1979  155.\n7  1980  155.\n\n\n1778년, 1947년, 1957년, 1958년, 1959년, 1979년, 1980년 데이터가 outer fence 밖에서 발견되며 1950년대 후반에 3년 연속으로, 1979~1980년 2년 연속 outlier를 보였다는 점을 염두에 두고 데이터 분석을 진행해야 할 것으로 보인다.\n\nstem(sunspot.year$Value,1) \n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   0 | 000112233334444455555666666777777788999\n   1 | 000000000111111111222233334445556666666677789\n   2 | 000111112233344445666778888899\n   3 | 0111122234455556666667888888999\n   4 | 0000112223444555667777778888889\n   5 | 024444457789\n   6 | 0000122334444444555677777788999\n   7 | 00133347888\n   8 | 0111233455669\n   9 | 033344669\n  10 | 01123345666\n  11 | 0112468\n  12 | 2256\n  13 | 125689\n  14 | 12\n  15 | 24559\n  16 | \n  17 | \n  18 | 5\n  19 | 0\n\n\n앞에서 언급하엿었던 것 처럼 총 7개의 outlier를 확인할 수 있고, 전반적으로 skewed되어있는 데이터인 것으로 확인된다.\n\nskewness = function(x) {\n  hl=fivenum(x)[2]\n  median=fivenum(x)[3]\n  hu=fivenum(x)[4]\n  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))\n  return(skew)\n}\nskewness(sunspot.year$Value)\n\n[1] 0.1219512\n\n\n\nskewness(log(sunspot.year$Value))\n\n[1] -0.2337415\n\nskewness(sqrt(sunspot.year$Value))\n\n[1] -0.05509413\n\nskewness(-1/(sunspot.year$Value))\n\n[1] -0.5512195\n\nskewness(-1/sqrt(sunspot.year$Value))\n\n[1] -0.4023891\n\n# sqrt 변환을 skewness가 최소화 되는 것으로 보인다.\n\nskewness를 계산할 경우 자료가 정규분포에 비해 skewed to the right 라는 사실을 확인할 수 있다.\n\nstem(sqrt(sunspot.year$Value)) # 자료의 비대칭성이 다소 개선 된 것으로 보인다.\n\n\n  The decimal point is at the |\n\n   0 | 000\n   1 | 223466789\n   2 | 001122222444455666667788999\n   3 | 1111222223333333445555566777889999\n   4 | 0000001112345556666778889999\n   5 | 0111222233344555556778889999\n   6 | 0000001111122222333344555666777788899999999999\n   7 | 0023334445667777788999\n   8 | 0000000001122222223333444555688889\n   9 | 000011222334566677889\n  10 | 000112223335556789\n  11 | 002245678899\n  12 | 34456\n  13 | 68\n\n\n\n# letter value display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\n(lvd2=lsum(sunspot.year$Value,9)) # mid 값이 점차 커지는 중\n\n  letter depth lower    mid  upper spread\n1      M 145.0  39.0 39.000  39.00   0.00\n2      H  73.0  15.6 42.250  68.90  53.30\n3      E  37.0   8.5 54.350 100.20  91.70\n4      D  19.0   5.0 63.250 121.50 116.50\n5      C  10.0   3.0 71.000 139.00 136.00\n6      B   5.5   1.6 78.075 154.55 152.95\n7      A   3.0   0.0 79.500 159.00 159.00\n8      Z   2.0   0.0 92.400 184.80 184.80\n9      Y   1.5   0.0 93.750 187.50 187.50\n\n# Kurtosis (E-spread / H-spread )- 1.705\n(lvd2[3,5]-lvd2[3,3])/(lvd2[2,5]-lvd2[2,3])-1.705 # more peaked than normal\n\n[1] 0.01545028\n\n\n데이터의 Five Numbers와 요약 지표\n\nfivenum(sunspot.year$Value)\n\n[1]   0.0  15.6  39.0  68.9 190.2\n\nsummary(sunspot.year$Value)# Median, Mean이 큰 차이가 없다.\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   15.60   39.00   48.61   68.90  190.20 \n\nas.numeric(summary(sunspot.year$Value)[6]-summary(sunspot.year$Value)[1]) # 최고점과 최저점 사이의 차이는 190.20\n\n[1] 190.2\n\n\n데이터의 최고점과 최저점 사이의 차이는 190.2정도인 것으로 확인된다. Median과 Mean값 사이의 차이가 꽤 크다는 점에서 skewed to the right 되었다는 사실을 어느정도 확인할 수 있다.\n\nqqnorm(sunspot.year$Value, ylab=\"Sunspots quantiles\");qqline(sunspot.year$Value, col='red',lty=2)\n\n\n\nfiv=fivenum(sunspot.year$Value)\n(pseudosigma = (fiv[4]-fiv[2])/1.34)\n\n[1] 39.77612\n\nsd(sunspot.year$Value) \n\n[1] 39.4741\n\n\npsudosigma와 실제 표준편차 사이에 큰 차이가 없는 것이 확인된다. 다만 qqplot에서 데이터가 직선을 따르기 보다는 Convex한 곡선을 따르고 있는 것처럼 보인다. (정규분포는 아니다)\n\nqqnorm(sunspot.year$Value, ylab=\"Sunspots quantiles\");qqline(sunspot.year$Value, col='red',lty=2)\nabline(fiv[3],pseudosigma,col=\"blue\",lty=2)\ntitle(sub=\"intercept=39.00(median); slope=39.776 (pseudo-sigma) blue line\")\n\n\n\n\n\nqqnorm(sqrt(sunspot.year$Value), ylab=\"Sqrt Sunspots quantiles\");qqline(sqrt(sunspot.year$Value), col='red',lty=2)\n\n\n\n# 변환 이전의 데이터에 비해서 정규분포를 상대적으로 잘 따르고 있는 것으로 보인다. \n\n데이터가 감마분포를 따르는가?\n\nSunspot.sort <- sort(sunspot.year$Value)\n(n.Sunspot<- length(sunspot.year$Value))\n\n[1] 289\n\ni <- 1:n.Sunspot\n\n\n(mean.Sunspot <- mean(sunspot.year$Value))\n\n[1] 48.61349\n\n(var.Sunspot <- var(sunspot.year$Value))\n\n[1] 1558.205\n\n(shape.Sunspot <- mean.Sunspot^2/var.Sunspot)\n\n[1] 1.516663\n\n(scale.Sunspot <- mean.Sunspot/var.Sunspot)\n\n[1] 0.0311984\n\nq.gamma.Sunspot <- qgamma((i-0.5)/n.Sunspot, shape.Sunspot, scale.Sunspot)\nplot(q.gamma.Sunspot, Sunspot.sort, main=\"Gamma prob plot\")\nline(q.gamma.Sunspot, Sunspot.sort)[2]\n\n$coefficients\n[1] -4.149998  1.111070\n\nabline(a=-4.14999766208764,b=1.11107022976103,col=\"red\",lwd=2) # 위에서 7개의 Outlier들을 제외하면 대부분의 데이터들이 직선 위에 존재하고 있는 것으로 보인다.\n\n\n\n\n데이터들이 Outlier들을 제외하면 감마분포를 잘 따르고 있는 것으로 보인다. 다만 데이터들이 0 근처에 Dense하게 몰려있는 것으로 보아 3승근 변환을 통해서 그러한 경향성을 완화하고자 한다.\n\nplot(q.gamma.Sunspot^(1/3), Sunspot.sort^(1/3), main=\"Gamma prob plot\")\nline(q.gamma.Sunspot^(1/3), Sunspot.sort^(1/3))[2]\n\n$coefficients\n[1] -0.5476324  1.1517409\n\nabline(a=-0.547632367221911,b=1.15174089923436,col=\"red\",lwd=2)\n\n\n\n\n삼승근 변환 후의 데이터들이 직선위에 잘 모여있는 것으로 보아 시계열 데이터들이 감마분포를 잘 다르고 있다고 볼 수 있다."
  },
  {
    "objectID": "posts/EDA6/과제6.html#정리",
    "href": "posts/EDA6/과제6.html#정리",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "정리",
    "text": "정리\n데이터가 현재 정규분포가 아닌 감마분포를 따르고 있는 것으로 보인다. 데이터의 정규성을 확보하기 위해서, sqrt 변환이나 Box Cox Transformation을 활용해 분석을 추가로 진행하고자 한다.\n평활을 진행하기 전 데이터가 어떤 경향을 가지고 있는지 Raw data와 차분 데이터에 대한 Plot을 그려 확인한다.\n\n# install.packages(\"forecast\")\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nsunspot.year=ts(sunspot.year$Value,start=1700)\nts.plot(sunspot.year, main = \"Sunspot.year Time-Series Plot\",xlim=c(1700,2000))\n\n\n\n\n\nlambda <- forecast::BoxCox.lambda(sunspot.year+1e-07)\nsunspot.year_new <- forecast::BoxCox(sunspot.year, lambda)\n\n\nplot(sunspot.year_new, main = \"Box-Cox : Sunspot.year\",xlim=c(1700,2000))\n\n\n\nplot(diff(sunspot.year_new), main = \"Difference & Box-Cox : sunspot.year\",xlim=c(1700,2000))\n\n\n\n\nBox-Cox Transformation을 통해서 데이터를 변환하였다. 다만, 0인 값이 있어 매우 작은 수를 더해 경향성 만을 확인할 수 있도록 변환하였다. (변환을 적용하기 적합하지 않음)\n\nlambda <- forecast::BoxCox.lambda(sunspot.year+1e-07)\nsunspot.year_new <- forecast::BoxCox(sunspot.year, lambda)\n\n\nts.plot(sunspot.year_new, main = \"Box-Cox : Sunspot.year\",xlim=c(1700,2000))\n\n\n\n\n변환전 그래프와 변환 후 그래프를 비교할 경우 차분된 데이터를 확인하면 원자료에서는 분산이 증가하는 경향성이 있었는데 두 변환을 사용할 경우 그 경향성이 다소 완화된 것이 확인되어 진다. 순서대로 원자료에 대한 평활 / Box-Cox Transforamtion에 대한 평활 / Sqrt 변환에 대한 평활을 진행하고자 한다. (3RSSH Twice,4253H Twice)\n원자료 3RSSH Twice\n\nsmooth_3RSSH=function(data){\n    smooth3RSS=smooth(data, kind=\"3RSS\")\n    \n    n=length(data)\n    smooth3RSSH=smooth3RSS\n    \n    \n    for (i in 2:(n-1)) {smooth3RSSH[i] <- smooth3RSS[i-1]/4 + smooth3RSS[i]/2 + smooth3RSS[i+1]/4}\n    smooth3RSSH[1] <- smooth3RSS[1]; smooth3RSSH[n] <- smooth3RSS[n]\n    rough=data-smooth3RSSH\n    roughH=rough\n    \n    smooth3RSS2=smooth(rough,kind=\"3RSS\")\n    \n    for (i in 2:(n-1)) roughH[i] <- smooth3RSS2[i-1]/4 + smooth3RSS2[i]/2 + smooth3RSS2[i+1]/4\n    roughH[1] <- smooth3RSS2[1]; roughH[n] <- smooth3RSS2[n]\n    out=smooth3RSSH+roughH\n    out=as.vector(out)\n    return(out)\n  }\n\n\nts.plot(sunspot.year,main=\"Raw Data: Default, 3RSSH Twice\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(smooth_3RSSH(sunspot.year),start=1700,end=1988),col=\"red\")\nlegend(x = 1700, y = 190, c(\"Default\", \"3RSSH Twice\"), \n      lty=c(2,1),lwd=2,col = c(\"black\",\"red\"))\n\n\n\n\n원자료 4253H Twice\n\nts.plot(sunspot.year,main=\"Raw Data: Default, 4253H Twice\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(sleek(sunspot.year),start=1700,end=1988),col=\"blue\")\nlegend(x = 1700, y = 190, c(\"Default\", \"4253H Twice\"), \n      lty=c(2,1),lwd=2,col = c(\"black\",\"blue\"))\n\n\n\n\nBoxcox 3RSSH Twice\n\nts.plot(sunspot.year_new,main=\"Box-Cox Transformed Data: Default, 3RSSH Twice\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\n\nlines(ts(smooth_3RSSH(sunspot.year_new),start=1700,end=1988),col=\"red\")\n\nlegend(x = 1850, y = 0, c(\"Default\", \"3RSSH Twice\"), \n       lty=c(2,1),lwd=2,col = c(\"black\",\"red\"))\n\n\n\n\nBoxcox 4253H Twice\n\nts.plot(sunspot.year_new,main=\"Box-Cox Transformed Data: Default, 4253H Twice\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(sleek(sunspot.year_new),start=1700,end=1988),col=\"red\")\nlegend(x = 1850, y = 0, c(\"Default\", \"4253H Twice\"), \n       lty=c(2,1),lwd=2,col = c(\"black\",\"blue\"))\n\n\n\n\nSqrt 3RSSH Twice\n\nts.plot(sqrt(sunspot.year),main=\"Sqrt Transformed Data: Default, 3RSSH Twice\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(smooth_3RSSH(sqrt(sunspot.year)),start=1700,end=1988),col=\"red\")\nlegend(x = 1700, y = 14, c(\"Default\", \"3RSSH Twice\"), \n       lty=c(2,1),lwd=2,col = c(\"black\",\"red\"))\n\n\n\n\nSqrt 4253H Twice\n\nts.plot(sqrt(sunspot.year),main=\"Sqrt Transformed Data: Default, 4253H Twice\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(sleek(sqrt(sunspot.year)),start=1700,end=1988),col=\"blue\")\nlegend(x = 1700, y = 14, c(\"Default\", \"4253H Twice\"), \n       lty=c(2,1),lwd=2,col = c(\"black\",\"blue\"))\n\n\n\n\n평활된 자료를 비교하기\n\nts.plot(sunspot.year,main=\"Raw Data:All\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(smooth_3RSSH(sunspot.year),start=1700,end=1988),col=\"red\")\nlines(ts(sleek(sunspot.year),start=1700,end=1988),col=\"blue\")\nlines(ts(smooth(sunspot.year, kind=\"3RS3R\",twiceit=T),start=1700,end=1988),col=\"darkgreen\")\nlegend(x = 1700, y = 190, c(\"Default\", \"3RSSH Twice\",\"4253H Twice\",\"3RS3R Twice\"), \n      lty=c(2,1,1,1),lwd=2,col = c(\"black\",\"red\",\"blue\",\"darkgreen\"))\n\n\n\n\n\n총 289년간 26번의 Fluctuation이 있는 것으로 보아 대략 11년마다 주기가 반복되어지는 것이 확인된다.\n각 주기별 Fluctuation에서 늘어나는 속도와 줄어드는 속도는 대체로 대칭적인 것으로 확인되어 진다.\n3RSSH twice 기법에 비해 4253H twice에서의 시계열 데이터가 조금 더 이 Fluctuation의 크기가 줄어든 것이 확인되어 진다.\n3RSSH Twice,4253H Twice 두 기법 모두 Hanning을 사용하여 3RS3R Twice 기법에 비해 평평한 지접이 나타나지 않는다.\n3RS3R Twice –> 3RSSH Twice –> 4253H Twice 순으로 극단값에 민감 한 것으로 보인다. 다만, 앞에서 언급하였던 1778년, 1947년, 1957년, 1958년, 1959년, 1979년, 1980년 자료들의 값들을 4253H Twice 기법이 다른 Smoothing 기법에 비해서 많이 깎아냈기 때문에 해석에 유의가 필요할 것으로 보인다.\n평활법을 사용하면 양 끝값을 실제 데이터를 활용하지 않고 인접 데이터들을 활용해 계산하기 때문에 실제 데이터에 비해서 덜 늘어나는 경향이 있다. 1988년도 실제 데이터는 더 늘어나고 있다는 점을 유의하여 해석해야 할 것이다.\n1950년대후반~1960년대초반에 Fluctuaition의 크기가 가장 큰 것으로 확인되고 1800년대 초반의 Fluctuation의 크기가 상대적으로 작은 것으로 보인다.\n봉우리의 Cycle은 앞에서 언급하였던 것 처럼 11년 정도이고 이 봉우리의 크기 또한 Fluctuate하는데 약 50년~60년 정도 마다 늘어나고 줄어드는 속도 또한 늘어나고 줄어드는 것으로 보인다. (ex, 1800s년대 초반 3주기 까지는 봉우리의 크기가 작다가 1840년이후의 3~4주기 정도는 봉우리의 크기가 늘어나고 그 이후 3주기 정도는 다시 그 크기가 줄어들고 있다.) (각 봉우리들의 크기 또한 주기성에 따라 움직이고 있는 것이 확인되어 진다.)\n이 분석에서는 주기성에 의미를 부여하기 위해서 3RSSH Twice Smoothing 기법이 다른 기법들에 비해서 해석에 도움이 될 것으로 보인다. 4253H Twice 기법을 사용할 경우 봉우리들의 크기 변화를 포착하기 어렵기 때문이다.\n\n\nts.plot(sunspot.year_new,main=\"Box-Cox Transformed Data:All\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(smooth_3RSSH(sunspot.year_new),start=1700,end=1988),col=\"red\")\nlines(ts(sleek(sunspot.year_new),start=1700,end=1988),col=\"blue\")\nlegend(x=1880, y = 0, c(\"Default\", \"3RSSH Twice\",\"4253H Twice\"), \n       lty=c(2,1,1),lwd=2,col = c(\"black\",\"red\",\"blue\"))\n\n\n\n\n\nts.plot(sqrt(sunspot.year),main=\"Sqrt Transformed Data:All\",ylab=\"Spots\",lty=2,xlim=c(1700,2000))\nlines(ts(smooth_3RSSH(sqrt(sunspot.year)),start=1700,end=1988),col=\"red\")\nlines(ts(sleek(sqrt(sunspot.year)),start=1700,end=1988),col=\"blue\")\nlegend(x = 1700, y = 14, c(\"Default\", \"3RSSH Twice\",\"4253H Twice\"), \n       lty=c(2,1,1),lwd=2,col = c(\"black\",\"red\",\"blue\"))\n\n\n\n\n\n자료를 변환하고 Smoothing을 적용할 경우 각 봉우리들의 변화 크기가 정규화 되는 장점은 있으나 각 봉우리들의 크기 변화를 포착하기 어려워 지는 단점이 있다.\n흑점의 수가 0인 지접이 있는데, 0일 경우에 Box-Cox Transformation을 사용하는 것은 부적절한 변환 방법이다. 따라서 주기성에 집중하기 위해서는 다른 변환 방법을 사용해야 할 것이다.\n\n\naTSA::adf.test(sunspot.year, nlag = NULL, output = TRUE) # p-value<=0.01 귀무가설을 기각하여 정상시계열\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -3.07  0.0100\n[2,]   1 -6.03  0.0100\n[3,]   2 -4.82  0.0100\n[4,]   3 -3.41  0.0100\n[5,]   4 -2.80  0.0100\n[6,]   5 -1.92  0.0546\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0  -5.32    0.01\n[2,]   1 -11.57    0.01\n[3,]   2 -10.62    0.01\n[4,]   3  -8.40    0.01\n[5,]   4  -7.57    0.01\n[6,]   5  -5.61    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0  -5.44    0.01\n[2,]   1 -12.04    0.01\n[3,]   2 -11.22    0.01\n[4,]   3  -9.00    0.01\n[5,]   4  -8.25    0.01\n[6,]   5  -6.22    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\n\nforecast(arima(sunspot.year,order=c(9,0,0)),h=10)\n\n     Point Forecast      Lo 80     Hi 80      Lo 95     Hi 95\n1989      140.83009 121.740253 159.91992 111.634707 170.02546\n1990      155.71030 126.109440 185.31116 110.439691 200.98091\n1991      142.43541 107.370583 177.50023  88.808386 196.06243\n1992      113.77752  77.425429 150.12961  58.181795 169.37324\n1993       77.79994  41.309958 114.28993  21.993327 133.60656\n1994       44.01103   7.475744  80.54632 -11.864870  99.88694\n1995       20.34908 -16.382092  57.08025 -35.826399  76.52455\n1996       11.45779 -25.473329  48.38891 -45.023484  67.93907\n1997       27.81698  -9.226678  64.86064 -28.836406  84.47036\n1998       60.68808  23.577875  97.79829   3.932919 117.44325\n\nplot(forecast(arima(sunspot.year,order=c(9,0,0)),h=10)) # Arima를 통해 미래 데이터가 어떻게 변할지 예측\n\n\n\n\n\n# 실제 2014년까지의 흑점 데이터와 예측된 데이터의 추이를 비교할 경우 \nnew=ts(read.table(\"https://raw.githubusercontent.com/SangwonJu/data/main/solar.txt\")$V2,start=1988)\nplot(forecast(arima(sunspot.year,order=c(9,0,0)),h=26))\nlines(new,col=\"red\",lwd=2)\nlegend(x = 1700, y = 220, c(\"Forecasted Data\", \"Actual Data\"), \n       lty=c(1,1),lwd=2,col = c(\"#2297E6\",\"red\"),cex=0.7)\n\n\n\n\n2000년대 초반까지의 흑점수의 변화는 거의 정확하게 예측된 것으로 보인다. 그 이후의 데이터는 다소 차이를 보이나 전반적인 추세는 유사한 것으로 보인다.\n\nplot(forecast(auto.arima(sunspot.year),h=26))\nlines(new,col=\"red\",lwd=2)\nlegend(x = 1700, y = 220, c(\"Forecasted Data\", \"Actual Data\"), \n       lty=c(1,1),lwd=2,col = c(\"#2297E6\",\"red\"),cex=0.7)"
  },
  {
    "objectID": "posts/EDA6/과제6.html#정리-1",
    "href": "posts/EDA6/과제6.html#정리-1",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "정리",
    "text": "정리\n해당 데이터는 정규분포를 따르고 있지 않다. (정규성x)\n\nlibrary(forecast)\nts.plot(fdeaths_ts, main = \"Female Deaths from Lung Diseases Time-Series Plot\") #시계열 자료의 계절성을 확인할 수 있다.\n\n\n\n# 1976년을 제외하면 데이터의 Fluctuation이 일정하게 유지되고 있다.\nplot(diff(fdeaths_ts), main = \"Difference : Female Deaths from Lung Diseases\") # 차분데이터를 확인할 경우 동일하게 1976년 자료를 제외하면 분산의 증가 없이 일정하게 변동성이 유지 되는 것으로 볼 수 있다.\n\n\n\n\nBox-Cox Transformation을 통해서 데이터를 변환하였다. (계절변동을 확인하기 위해서 데이터의 정규성을 개선하기 위해)\n\nlambda <- forecast::BoxCox.lambda(fdeaths_ts)\nfdeaths_ts_new <- forecast::BoxCox(fdeaths_ts, lambda)\n\n\nplot(fdeaths_ts_new, main = \"Box-Cox : Female Deaths from lung diseases\") # 1976년의 데이터의 이질성이 다소 완화되었다.\n\n\n\nplot(diff(fdeaths_ts_new), main = \"Difference & Box-Cox : Female Deaths from lung diseases\")\n\n\n\n\nfdeaths 데이터 - 분해 시계열\n\n# 계절요인 분해시계열\n \nfdeaths.decompose <- decompose(fdeaths)       # 데이터에서 4가지 요인을 분해\nfdeaths.decompose$seasonal                     # 계절요인으로 분해된 부분이다.\n\n            Jan        Feb        Mar        Apr        May        Jun\n1974  253.30417  276.72083  200.18750   39.36250  -73.05417 -131.32917\n1975  253.30417  276.72083  200.18750   39.36250  -73.05417 -131.32917\n1976  253.30417  276.72083  200.18750   39.36250  -73.05417 -131.32917\n1977  253.30417  276.72083  200.18750   39.36250  -73.05417 -131.32917\n1978  253.30417  276.72083  200.18750   39.36250  -73.05417 -131.32917\n1979  253.30417  276.72083  200.18750   39.36250  -73.05417 -131.32917\n            Jul        Aug        Sep        Oct        Nov        Dec\n1974 -148.36250 -195.49583 -192.26250 -106.68750  -63.97917  141.59583\n1975 -148.36250 -195.49583 -192.26250 -106.68750  -63.97917  141.59583\n1976 -148.36250 -195.49583 -192.26250 -106.68750  -63.97917  141.59583\n1977 -148.36250 -195.49583 -192.26250 -106.68750  -63.97917  141.59583\n1978 -148.36250 -195.49583 -192.26250 -106.68750  -63.97917  141.59583\n1979 -148.36250 -195.49583 -192.26250 -106.68750  -63.97917  141.59583\n\nplot(fdeaths.decompose)\n\n\n\nts.plot(fdeaths.decompose$seasonal,main=\"The Plot of Seasoal Decomposition\")\n\n\n\n\n계절 요인을 제외한 나머지 변화 확인\n\n# 계절요인 제외시키기\nfdeaths.decompose_new <- fdeaths - fdeaths.decompose$seasonal\nts.plot(fdeaths.decompose_new, main=\"Time-Series without Seasonal Effect\")\n\n\n\n\n계절성을 제외한 나머지 요인들을 분석할 경우 1976년도의 Random한 요인에 의해서 데이터가 늘어난 것을 확인 할 수 있다. 1977년도 초반의 Random한 요인 또한 시계열에서 1~2월 데이터를 각 년도 동기에 비해서 다소 낮은 수준으로 유지하게 만들었다.\n\nfdeaths_1974=ts(fdeaths_ts[1:12],start=1)\nfdeaths_1975=ts(fdeaths_ts[13:24],start=1)\nfdeaths_1976=ts(fdeaths_ts[25:36],start=1)\nfdeaths_1977=ts(fdeaths_ts[37:48],start=1)  \nfdeaths_1978=ts(fdeaths_ts[49:60],start=1)\n\n\nfdeaths_1979=ts(fdeaths_ts[61:72],start=1)  \nyr=paste(\"fdeaths\",\"_\",1974:1979,sep=\"\")\n\nxat=seq(0,12,by=1)\npar(mfrow=c(2,3))\nfor (i in yr) {ts.plot(as.name(i),main=i,ylab=\"Female Deaths by lung cancer\")\n               axis(side=1,at=xat)}\n\n\n\n\n각 년도별 계절성을 비교하기 위해서 이런식으로 그래프를 연도별로 쪼개서 그렸다. 연도별로 그러한 경향성이 비슷하게 드러나고 있는 것으로 보인다.\n\n# install.packages(\"ghibli\")\nlibrary(\"ghibli\")\npal=ghibli_palette(\"PonyoLight\",n=6)\nas.character(pal)\n\n[1] \"#A6A0A0FF\" \"#ADB7C0FF\" \"#94C5CCFF\" \"#F4ADB3FF\" \"#EEBCB1FF\" \"#ECD89DFF\"\n\npar(mfrow=c(1,1))\n\nts.plot(fdeaths_1974,main=\"Female Deaths by lung diseases in UK\",xlab=\"Month\",ylab=\"# of Deaths\",col=\"#A6A0A0FF\",lwd=2,ylim=c(300,1200))\naxis(side=1,at=xat)\nlines(fdeaths_1975,lwd=2,col=\"#ADB7C0FF\")\nlines(fdeaths_1976,lwd=2,col=\"#94C5CCFF\")\nlines(fdeaths_1977,lwd=2,col=\"#F4ADB3FF\")\nlines(fdeaths_1978,lwd=2,col=\"#EEBCB1FF\")\nlines(fdeaths_1979,lwd=2,col=\"#ECD89DFF\")\n\nlegend(x = 10, y = 1200, c(1974:1979), \n          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n\n전체데이터와 월별데이터에 평활을 진행하고자 한다. 평활법은 교수님께서 추천하셨던 3RSSH Twice, 4253H Twice에 더해 Hanning이 이루어지기 전 평활법인 3RS3R Twice를 추가하고자 한다.\n\nlibrary(sleekts)\npal2=as.vector(ghibli_palette(\"MononokeMedium\")[c(1,3,5,7)])\nts.plot(fdeaths_ts,main=\"Raw Data:All\",ylab=\"# of Female deaths\",lty=2,col=pal2[1])\nlines(ts(smooth(fdeaths_ts, kind=\"3RS3R\",twiceit=T),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[2],lwd=2)\nlines(ts(smooth_3RSSH(fdeaths_ts),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[3],lwd=2)\nlines(ts(sleek(fdeaths_ts),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[4],lwd=2)\n\nlegend(x =1978.7, y = 1100, c(\"Default\", \"3RS3R Twice\", \"3RSSH Twice\",\"4253H Twice\"), \n       lty=c(2,1,1,1),lwd=2,col = pal2,cex=0.5)\n\n\n\n\n평활법을 적용하여 확인할 경우 원 자료에 비해 최댓값과 최솟값의 폭이 많이 줄어들었음을 확인할 수 있다. 작은 값에서는 크게 변화가 없지만 값이 큰 자료들의 경우 많이 깎여나갔다. 시계열을 확인하는데 있어서 그 계절성을 확인하기 좋은 형태로 평활이 된 것은 사실이지만, 위에서 언급한 1976년도의 특이값이 사라지게 되었고 그 특이값을 해석하는데 있어서 주의를 기울여야 할 것으로 보인다. 3RS3R Twice 기법의 경우 Hanning이 진행되지 않았기 때문에 다소 각진 부분이 남아있지만, 전반적으로 계절성이 나타나는 형태로 데이터를 변화시켰다. 나머지 Hanning을 사용한 2가지 평활법의 차이를 분석하면 4253H방법에서 큰 값들의 감소폭이 크게 나타나고 있다. 3가지 평활법의 양 끝자료의 경우 실제 존재하는 데이터를 가지고 만든 것이 아니기 때문에 그 추세를 해석하는데 있어서 용이하지만 실제 데이터와 차이가 있으므로 해석에 유의해야 할 것이다.\n앞에서 시계열 Decompose를 통해서 그렸던 계절성 그래프의 모양과 4253H Twice의 그래프가 상당히 유사한 것으로 보인다. 1년을 주기로 폐질환 사망자수가 Fluctuate 하고 있는데 겨울철에 전반적으로 증가하고 여름철에 감소하는 경향성을 가지는 것이 확인된다. 각 주기는 거의 대칭적으로 증감을 반복하고 있으며 1976, 1977년도를 제외하면 사망건수는 거의 비슷하게 유지 되는 것을 확인할 수 있다. 따라서 시계열 자료를 해석할 때 1976년도와 1977년도 자료는 유의해서 해석해야 할 것으로 보인다.\nhttps://premium.weatherweb.net/weather-in-history-1975-to-1999-ad/ For England and Wales (using the Met Office EWR/EWP series), it was one of the six DRIEST winters in the previous 100 years, and the third consecutive season with less rain than usual: summer and autumn 1975 were also dry. Winter 1975/76 had around 61% of average rainfall over England and Wales. It was this persistence of low precipitation, particularly throughout the winter ‘re-charge’ season, that led to the severe DROUGHT problems encountered in 1976 (q.v.) 1976년도의 특이 값에 대한 원인을 분석할 때 해당 년도의 겨울이 100년중 6번째로 건조한 겨울이었다는 점을 고려해야 할 것으로 보인다. 가뭄 문제를 걱정할 정도로 겨울이 건조하였다는 사실은 해당 년도에 사람들이 호흡기 질환을 걸릴 가능성이 상당히 높았음을 예측할 수 있다.\n##Raw Data\n\nts.plot(fdeaths_1974,main=\"Female Deaths by lung diseases in UK\",xlab=\"Month\",ylab=\"# of Deaths\",col=\"#A6A0A0FF\",lwd=2,ylim=c(300,1200))\naxis(side=1,at=xat)\nlines(fdeaths_1975,lwd=2,col=\"#ADB7C0FF\")\nlines(fdeaths_1976,lwd=2,col=\"#94C5CCFF\")\nlines(fdeaths_1977,lwd=2,col=\"#F4ADB3FF\")\nlines(fdeaths_1978,lwd=2,col=\"#EEBCB1FF\")\nlines(fdeaths_1979,lwd=2,col=\"#ECD89DFF\")\n\nlegend(x = 10, y = 1200, c(1974:1979), \n          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)"
  },
  {
    "objectID": "posts/EDA6/과제6.html#rs3r-twice",
    "href": "posts/EDA6/과제6.html#rs3r-twice",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "3RS3R Twice",
    "text": "3RS3R Twice\n\nts.plot(ts(smooth(fdeaths_1974, kind=\"3RS3R\",twiceit=T),start=1, end=12),main=\"3RS3R Twice: Female Deaths by lung diseases in UK\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(300,1200))\naxis(side=1,at=xat)\nlines(ts(smooth(fdeaths_1975, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[2])\nlines(ts(smooth(fdeaths_1976, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[3])\nlines(ts(smooth(fdeaths_1977, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[4])\nlines(ts(smooth(fdeaths_1978, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[5])\nlines(ts(smooth(fdeaths_1979, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[6])\nlegend(x = 10, y = 1200, c(1974:1979), \n          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)"
  },
  {
    "objectID": "posts/EDA6/과제6.html#rssh-twice",
    "href": "posts/EDA6/과제6.html#rssh-twice",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "3RSSH Twice",
    "text": "3RSSH Twice\n\nts.plot(ts(smooth_3RSSH(fdeaths_1974),start=1, end=12),main=\"3RSSH Twice: Female Deaths by lung diseases in UK\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(300,1200))\naxis(side=1,at=xat)\nlines(ts(smooth_3RSSH(fdeaths_1975),start=1, end=12),lwd=2,col=pal[2])\nlines(ts(smooth_3RSSH(fdeaths_1976),start=1, end=12),lwd=2,col=pal[3])\nlines(ts(smooth_3RSSH(fdeaths_1977),start=1, end=12),lwd=2,col=pal[4])\nlines(ts(smooth_3RSSH(fdeaths_1978),start=1, end=12),lwd=2,col=pal[5])\nlines(ts(smooth_3RSSH(fdeaths_1979),start=1, end=12),lwd=2,col=pal[6])\nlegend(x = 10, y = 1200, c(1974:1979), \n          lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)"
  },
  {
    "objectID": "posts/EDA6/과제6.html#h-twice",
    "href": "posts/EDA6/과제6.html#h-twice",
    "title": "EDA Assignment 6: Chapter 8",
    "section": "4253H Twice",
    "text": "4253H Twice\n\nts.plot(ts(sleek(fdeaths_1974),start=1, end=12),main=\"4253H Twice: Female Deaths by lung diseases in UK\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(300,1200))\naxis(side=1,at=xat)\nlines(ts(sleek(fdeaths_1975),start=1, end=12),lwd=2,col=pal[2])\nlines(ts(sleek(fdeaths_1976),start=1, end=12),lwd=2,col=pal[3])\nlines(ts(sleek(fdeaths_1977),start=1, end=12),lwd=2,col=pal[4])\nlines(ts(sleek(fdeaths_1978),start=1, end=12),lwd=2,col=pal[5])\nlines(ts(sleek(fdeaths_1979),start=1, end=12),lwd=2,col=pal[6])\nlegend(x = 10, y = 1200, c(1974:1979), \n       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n\n원자료 –> 3RS3R Twice –> 3RSSH Twice–> 4253H Twice순으로 시계열의 추세가 유사해지고 있는것을 확인할 수 있다. Hanning을 사용하여 3RSSH Twice, 4253H Twice는 평평한 구간 없이 부드럽게 넘어가고 있으며 1976년도 평활법을 통해서 최대값이 줄어들어 다른 년도와 유사한 계절성을 확인할 수 있게 되었다.\n\nlibrary(forecast)\naTSA::adf.test(fdeaths, nlag = NULL, output = TRUE) # p-value<=0.01 귀무가설을 기각하여 정상시계열\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -1.29   0.214\n[2,]   1 -1.14   0.269\n[3,]   2 -1.42   0.169\n[4,]   3 -1.13   0.273\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -3.48  0.0131\n[2,]   1 -4.34  0.0100\n[3,]   2 -5.73  0.0100\n[4,]   3 -6.61  0.0100\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -3.40  0.0628\n[2,]   1 -4.36  0.0100\n[3,]   2 -5.69  0.0100\n[4,]   3 -6.68  0.0100\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\nfit <- auto.arima(fdeaths)\nplot(forecast(fit, level=c(80, 95), h=12)) \n\n\n\n# 다음 1년간의 변화를 예측할 경우 이전의 자료들과 비슷한 추세를 가지고 있는 것이 확인되어 진다."
  },
  {
    "objectID": "posts/EDA7/index.html",
    "href": "posts/EDA7/index.html",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "",
    "text": "9장. 로버스트 선형회귀"
  },
  {
    "objectID": "posts/EDA7/index.html#가",
    "href": "posts/EDA7/index.html#가",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(가)",
    "text": "(가)\nVolume을 Y축에, Height를 X축에 그리고 또한 Volume을 Y축에, Girth를 X축에 그린 후 두 그래프에서 자료들판단)이 직선으로 모형을 세우기 적합한 지, 선형성이 어떤 것이 더 강한지 비교하여라. (눈으로 판단)\n\npacman::p_load(\"skimr\",\"tidyverse\")\ntrees %>% glimpse()\n\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, …\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74,…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.…\n\nskimr::skim(trees)\n\n\nData summary\n\n\nName\ntrees\n\n\nNumber of rows\n31\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nGirth\n0\n1\n13.25\n3.14\n8.3\n11.05\n12.9\n15.25\n20.6\n▃▇▃▅▁\n\n\nHeight\n0\n1\n76.00\n6.37\n63.0\n72.00\n76.0\n80.00\n87.0\n▃▃▆▇▃\n\n\nVolume\n0\n1\n30.17\n16.44\n10.2\n19.40\n24.2\n37.30\n77.0\n▇▅▁▂▁\n\n\n\n\nsum(is.na(trees)) # 결측치 없음\n\n[1] 0\n\nplot(y=trees$Volume,x=trees$Height,main=\"Linearity of volume and height\",xlab=\"height\",ylab=\"volume\")\n\n\n\nplot(y=trees$Volume,x=trees$Girth,main=\"Linearity of volume and girth\",xlab=\"girth\",ylab=\"volume\")\n\n\n\n\n첫번째 자료의 경우 경향성이 따로 나타나지 않는 것으로 보인다. 두 Plot을 비교할 경우 두번째 자료(volume,girth)가 직선으로 모형으로 세우기 적합하다고 본다. X축과 Y축의 선형성 또한 두번째 자료가 더 강하게 나타나는 것으로 보인다."
  },
  {
    "objectID": "posts/EDA7/index.html#나",
    "href": "posts/EDA7/index.html#나",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(나)",
    "text": "(나)\n각각 r-line을 적합 시킨 후 그래프 위에 그려라. 각기 잔차를 구하여 y-hat vs. residual plot을 그리고 적합 타당성을 판단하고 비교하여라.\n\n# 첫번째 플롯에 대해서\nplot(y=trees$Volume,x=trees$Height,main=\"Linearity of volume and height\",xlab=\"height\",ylab=\"volume\")\n(z1 <- line(x=trees$Height,y=trees$Volume))\n\n\nCall:\nline(x = trees$Height, y = trees$Volume)\n\nCoefficients:\n[1]  -115.118     1.918\n\nabline(coef(z1))\nz1.ls <- lm(trees$Volume ~ trees$Height)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 63, y = 72, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# Tukey-Tree Plot :\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline:Volume~Height\")\nabline(0,0)       \n\n\n\nboxplot(residuals(z1)) # outlier 없음\n\n\n\n# 두번째 플롯에 대해서\nplot(y=trees$Volume,x=trees$Girth,main=\"Linearity of volume and girth\",xlab=\"girth\",ylab=\"volume\")\n(z2 <- line(x=trees$Girth,y=trees$Volume))\n\n\nCall:\nline(x = trees$Girth, y = trees$Volume)\n\nCoefficients:\n[1]  -36.585    5.046\n\nabline(coef(z2))\nz2.ls <- lm(trees$Volume ~ trees$Girth)\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 8.5, y = 71, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# Tukey-Tree Plot :\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline: Volume~Height\")\nabline(0,0)    \n\n# 이차함수를 따를지도 모른다.\nk=seq(-100,100,0.1)\na<-function(x) {((1/50)*(x-35)^2)-8}\nlines(a(k)~k)\n\n\n\nboxplot(residuals(z2)) # outlier 없음\n\n\n\n\n첫번째 Plot의 경우 이 분산성이 발생하는 것으로 보인다. Y_hat이 작을 때에는 잔차의 폭이 좁았는데 점점 커질수록 잔차의 폭이 넓어지고 있는 모습을 보이고 있다는 점에서 그러하다. Ideal한 Residual Plot은 띠 모양으로 Plot의 점들이 x축에 평행한 직선을 이루어야 하는데 Plot에서의 잔차는 점차 늘어나고 있다는 점에서 Transformation을 통해 잔차를 조정해야 한다.\n두번째 Plot의 경우에도 점들이 x축에 평행하게 모여있지는 않고 일종의 Qudratic Form (2차함수) 곡선을 이루고 있는 것으로 보인다.Ideal한 Residual Plot은 띠 모양으로 Plot의 점들이 x축에 평행 직선을 이루어야 하는데 실제 Plot의 모양은 곡선을 이루고 있는 것으로 보아 변수 들을 변환한다면 현재의 잔차들이 가지고 있는 경향성을 완화할 수 있을 것이라고 생각한다."
  },
  {
    "objectID": "posts/EDA7/index.html#다",
    "href": "posts/EDA7/index.html#다",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(다)",
    "text": "(다)\nVolume ~ Height plot에서 직선화하기 위한 재표현을 시행착오 방법으로 찾아라. Volume만 변환시켜 보아라. Height만 변환시켜 보아라. 시행착오 과정을 모두 수록하여라.\n\n# 단순화\nattach(trees)\nmedHeight <- as.vector(3)\nmedVolume <- as.vector(3)\nmedHeight[1] <- median(Height[1:10]); medVolume[1] <- median(Volume[1:10])\nmedHeight[2] <- median(Height[11:21]); medVolume[2] <- median(Volume[11:21])\nmedHeight[3] <- median(Height[22:31]); medVolume[3] <- median(Volume[22:31])\nplot(medVolume~medHeight, type=\"b\",main=\"MedVolume~MedHeight\")   # \"b\"= both points and line\n\n\n\ndetach(trees)\n\n# 시행착오\n# 먼저 Y변수에 대해서 큰부분을 작게 작은 부분을 크게 만드는 변환\nplot(y=sqrt(trees$Volume),x=trees$Height,main=\"Linearity of sqrt(volume) and height\",xlab=\"height\",ylab=\"sqrt volume\")\n\n\n\nplot(y=log(trees$Volume),x=trees$Height,main=\"Linearity of log(volume) and height\",xlab=\"height\",ylab=\"log volume\")\n\n\n\n# X변수에 대해서 변환을 진행\nplot(y=trees$Volume,x=(trees$Height)^2,main=\"Linearity of volume and height squares\",xlab=\"height squares\",ylab=\"volume\")\n\n\n\n# 책에서 제시한 변환법\nvh=trees$Volume/trees$Height\nplot(y=vh,x=trees$Height,main=\"Linearity of volume/height and height\",xlab=\"height\",ylab=\"volume/height\")\n\n\n\n# Volume만 변화시키기\n# 1) y에 Sqrt 씌우기\n\nplot(y=sqrt(trees$Volume),x=trees$Height,main=\"Linearity of sqrt(volume) and height\",xlab=\"height\",ylab=\"sqrt volume\")\n(z3 <- line(x=trees$Height,y=sqrt(trees$Volume)))\n\n\nCall:\nline(x = trees$Height, y = sqrt(trees$Volume))\n\nCoefficients:\n[1]  -7.7958   0.1693\n\nabline(coef(z3))\nz3.ls <- lm(sqrt(trees$Volume) ~ trees$Height)\nabline(z3.ls$coef, lty=2,col=\"red\")\nlegend(x = 63, y = 8.5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# 2) y에 Log 씌우기\n\nplot(y=log(trees$Volume),x=trees$Height,main=\"Linearity of log(volume) and height\",xlab=\"height\",ylab=\"log volume\")\n(z4 <- line(x=trees$Height,y=log(trees$Volume)))\n\n\nCall:\nline(x = trees$Height, y = log(trees$Volume))\n\nCoefficients:\n[1]  -1.39776   0.06068\n\nabline(coef(z4))\nz4.ls <- lm(log(trees$Volume) ~ trees$Height)\nabline(z4.ls$coef, lty=2,col=\"red\")\nlegend(x = 63, y = 4.2, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# 3) x에 제곱 씌우기\nplot(y=trees$Volume,x=(trees$Height)^2,main=\"Linearity of volume and height squares\",xlab=\"height squares\",ylab=\"volume\")\n(z5 <- line(x=(trees$Height)^2,y=trees$Volume))\n\n\nCall:\nline(x = (trees$Height)^2, y = trees$Volume)\n\nCoefficients:\n[1]  -43.3710    0.0127\n\nabline(coef(z5))\nk=(trees$Height)^2\nz5.ls <- lm(trees$Volume ~k )\nabline(z5.ls$coef, lty=2,col=\"red\")\nlegend(x = 4000, y = 72, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# 4) Huh\nvh=trees$Volume/trees$Height\nplot(y=vh,x=trees$Height,main=\"Linearity of volume/height and height\",xlab=\"height\",ylab=\"volume/height\")\n(z6 <- line(x=trees$Height,y=vh))\n\n\nCall:\nline(x = trees$Height, y = vh)\n\nCoefficients:\n[1]  -1.12720   0.01978\n\nabline(coef(z6))\nz6.ls <- lm(vh~trees$Height)\nabline(z6.ls$coef, lty=2,col=\"red\")\nlegend(x = 64, y = 0.85, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n\n총 네가지 방법으로 변환을 진행해 보았는데 4가지 표현 중 x변수를 변환하는 방법의 경우 r-line 위쪽 부분의 점들이 어느정도 r-line에 평행하게 선형성을 보이고 있으나 밑부분의 점들은 선형적이지 않아 선택하지 않았다. y변수에 대한 변환 중 log와 sqrt를 비교할 경우 log 변환을 한 Plot의 가장 큰 점인 (90,4.5) 부근에 한 점이 가장자리 점들을 기준으로 한 선위에서 약간 벗어난 것으로 보이기 때문에 y변수에 대한 sqrt 변환이 log변환에 비해서 적합하다고 판단하였다.\n\nplot(y=log(trees$Volume),x=log(trees$Height),main=\"Linearity of log(volume) and log(height)\",xlab=\"log height\",ylab=\"log volume\")\n(z7 <- line(x=log(trees$Height),y=log(trees$Volume)))\n\n\nCall:\nline(x = log(trees$Height), y = log(trees$Volume))\n\nCoefficients:\n[1]  -16.582    4.573\n\nabline(coef(z7))\nz7.ls <- lm(log(trees$Volume) ~ log(trees$Height))\nabline(z7.ls$coef, lty=2,col=\"red\")\nlegend(x = 4.15, y = 4.15, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(trees$Volume),x=sqrt(trees$Height),main=\"Linearity of sqrt(volume) and sqrt(height)\",xlab=\"sqrt height\",ylab=\"sqrt volume\")\n(z8 <- line(x=sqrt(trees$Height),y=sqrt(trees$Volume)))\n\n\nCall:\nline(x = sqrt(trees$Height), y = sqrt(trees$Volume))\n\nCoefficients:\n[1]  -20.50    2.94\n\nabline(coef(z8))\nz8.ls <- lm(sqrt(trees$Volume) ~ sqrt(trees$Height))\nabline(z8.ls$coef, lty=2,col=\"red\")\nlegend(x = 8, y = 8.5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n\n추가로 x축 y축 전부에 log와 sqrt 변환을 적용하였는데 위의 변환들 보다 잔차들이 안정되어진 것으로 추정된다."
  },
  {
    "objectID": "posts/EDA7/index.html#라",
    "href": "posts/EDA7/index.html#라",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(라)",
    "text": "(라)\n(다)에 찾은 변환으로 각기 r-line으로 적합하고 residual plot, residual의 boxplot, stem-and-leaf display 등으로 적합성를 비교하여라.\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by rline:sqrt(volume) ~ height\")\nabline(0,0)    \n\n\n\nplot(residuals(z4) ~ fitted(z4), main = \"Residual plot by rline:log(volume) ~ height\")\nabline(0,0) \n\n\n\nplot(residuals(z5) ~ fitted(z5), main = \"Residual plot by rline:volume ~ height^2\")\nabline(0,0)    \n\n\n\nplot(residuals(z6) ~ fitted(z6), main = \"Residual plot by rline:volume/height ~ height\")\nabline(0,0) \n\n\n\nplot(residuals(z7) ~ fitted(z7), main = \"Residual plot by rline:log volume ~ log height\")\nabline(0,0) \n\n\n\nplot(residuals(z8) ~ fitted(z8), main = \"Residual plot by rline:sqrt volume ~ sqrt height\")\nabline(0,0) \n\n\n\n\n그러나 Residual plot을 그려볼 경우 여러 변환들 중 적합해 보이는 것은 y변수에 대한 log 변환인 것으로 보인다. 나머지 변환의 경우 직선 밑 부분의 점들이 점점 줄어들고 있는 경향성이 나타나고 있는데 그럴경우 적합하지 않은 변환이기 때문이다\ny변수에만 log변환을 한 경우를 다른 Residual plot을 비교할 경우 미묘한 차이지만 맨위에 있는 4개의 점들이 줄어드는 경향성이 있고 마지막점의 이상성이 다소 있다는 점에서 단점이 있지만 전반적인 Residual Plot을 비교하면 log가 가장 적정해 보인다.\n\nboxplot(residuals(z3),main=\"Residual boxplot of sqrt(volume) ~ height\")\n\n\n\nboxplot(residuals(z4),main=\"Residual boxplot of log(volume) ~ height\")\n\n\n\nboxplot(residuals(z5),main=\"Residual boxplot of volume ~ height^2\")\n\n\n\nboxplot(residuals(z6),main=\"Residual boxplot of volume/height ~ height\")\n\n\n\nboxplot(residuals(z7),main=\"Residual boxplot of xy log\")\n\n\n\nboxplot(residuals(z8),main=\"Residual boxplot of xy sqrt\")\n\n\n\n\n첫번째, 두번째 boxplot의 경우 median이 lower hinge쪽에 더 가까운 것으로 보인다. 또 Whisker의 길이도 lower whisker쪽이 더 길게 나타나고 있다. Whisker의 길이와 Box내의 대칭성의 경우 3번째 Boxplot이 가장 대칭적인 것으로 보인다. 그럼에도 불구하고 log변환에서 대칭성이 크게 감소하지는 않았다는 점에서 사용 가능한 변환으로 보인다.\nx변수와 y변수에 동시에 log변환을 취한 boxplot이 y변수에만 log변환을 취한 boxplot에 비해서 whisker의 길이가 대칭적이 된 것으로 보인다.\n\nstem(residuals(z3))\n\n\n  The decimal point is at the |\n\n  -1 | 865\n  -1 | 0\n  -0 | 887655\n  -0 | 443100\n   0 | 3\n   0 | 5678\n   1 | 33444\n   1 | 58899\n\nstem(residuals(z4))\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -6 | 6\n  -4 | 821\n  -2 | 454110\n  -0 | 77650\n   0 | 014\n   2 | 174\n   4 | 4688900\n   6 | 173\n\nstem(residuals(z5))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -2 | 431\n  -1 | 55\n  -1 | 20\n  -0 | 9999866\n  -0 | 40\n   0 | 1344\n   0 | 5\n   1 | 01344\n   1 | 566\n   2 | 04\n\nstem(residuals(z6))\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -2 | 864\n  -1 | 7631100\n  -0 | 99764\n   0 | 034678\n   1 | 56889\n   2 | 14579\n\nstem(residuals(z7))\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -6 | 4\n  -4 | 818\n  -2 | 46111\n  -0 | 887764\n   0 | 007\n   2 | 084\n   4 | 5789900\n   6 | 178\n\nstem(residuals(z8))\n\n\n  The decimal point is at the |\n\n  -1 | 965\n  -1 | 0\n  -0 | 98776655\n  -0 | 421\n   0 | 044\n   0 | 678\n   1 | 22334\n   1 | 5788\n   2 | 0\n\nstem(residuals(z3),0.5)\n\n\n  The decimal point is at the |\n\n  -1 | 8650\n  -0 | 887655443100\n   0 | 35678\n   1 | 3344458899\n\nstem(residuals(z4),0.5)\n\n\n  The decimal point is at the |\n\n  -0 | 7655\n  -0 | 33222222221\n   0 | 0012334\n   0 | 555555677\n\nstem(residuals(z5),0.5)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -2 | 431\n  -1 | 5520\n  -0 | 999986640\n   0 | 13445\n   1 | 01344566\n   2 | 04\n\nstem(residuals(z6),0.5)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -2 | 864\n  -0 | 763110099764\n   0 | 03467856889\n   2 | 14579\n\nstem(residuals(z7),0.5)\n\n\n  The decimal point is at the |\n\n  -0 | 6655\n  -0 | 33222222220\n   0 | 002233\n   0 | 5555555678\n\nstem(residuals(z8),0.5)\n\n\n  The decimal point is at the |\n\n  -1 | 9650\n  -0 | 98776655421\n   0 | 044678\n   1 | 223345788\n   2 | 0\n\n\n전반적으로 가장 큰 Cluster를 기준으로 대칭적인 종모양을 띄고 있지는 않은 것으로 보인다. y변수에 대한 sqrt변환의 경우와 log변환의 경우 그리고 x변수에 대한 제곱 변환의 경우 한개의 Cluster가 아니라 여러개의 Cluster가 있는 것으로 보인다. 교재에서 제시한 변환 방식이 Skewed to the right 되어 있지만 그나마 한개의 종모양의 형태를 띄고 있는 것으로 보인다.\nxy변수에 대한 log변환을 취한 경우 Stem 4를 제외하면 전반적으로 종모양이 잘 이루어지고 있는 것으로 보인다. stem 4가 이상치가 아니라면 2개의 cluster에 의한 쌍봉분포일 것이고, stem 4가 이상치라면 정규분포를 잘 이루고 있는 것으로 볼 수 있다."
  },
  {
    "objectID": "posts/EDA7/index.html#가-1",
    "href": "posts/EDA7/index.html#가-1",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(가)",
    "text": "(가)\nOzone을 Y변수로 하고, Solar.R, Wind, Temp 각각의 세 X 변수에 대하여 산점도를 그리고 변환이 필요하면 변환하여 r-line으로 적합한 후 residual 분석 등으로 최적 X 변수의 순위를 선정하여라.\n\n결측치 확인하기\n\ndata(\"airquality\")\n\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\ndim(airquality)\n\n[1] 153   6\n\nround(colSums(is.na(airquality))/dim(airquality)[1]*100,2)\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n  24.18    4.58    0.00    0.00    0.00    0.00 \n\n\nOzone 자료의 결측치 비율이 24.18%로 거의 1/4에 해당하는 자료에 대한 Ozone 데이터가 나타나지 않은 것으로 보인다. 따라서 분석을 진행 할때 있어 이부분을 유의해야 할 것으로 보인다. Solar.R자료는 약 4.58%에 해당하는 자료가 현재 결측되어진 것으로 보인다.\n\n\nOzone~Solar.R\n\nattach(airquality)\nplot(Ozone~Solar.R,main=\"linearity of Ozone ~ Solar\")\n\n\n\nlength(Ozone) # 51개씩 쪼개기\n\n[1] 153\n\nairquality2=airquality[order(airquality$Solar.R),]\n\nmedOzone <- as.vector(3)\nmedSolar <- as.vector(3)\nmedOzone[1] <- median(airquality2$Ozone[1:51],na.rm=T); medSolar[1] <- median(airquality2$Solar.R[1:51],na.rm=T)\nmedOzone[2] <- median(airquality2$Ozone[52:102],na.rm=T); medSolar[2] <- median(airquality2$Solar.R[52:102],na.rm=T)\nmedOzone[3] <- median(airquality2$Ozone[103:153],na.rm=T); medSolar[3] <- median(airquality2$Solar.R[103:153],na.rm=T)\nplot(medOzone ~ medSolar, type=\"b\") \n\n\n\n# 선형관계보다는 기울기가 음수인 이차함수의 관계인 것으로 보인다. y변수 변환을 통해 선형성을 확보 \n\nplot(y=log(Ozone),x=Solar.R,main=\"Linearity of log(Ozone) and Solar.R\",xlab=\"Solar.R\",ylab=\"log(Ozone)\")\n(z1 <- line(y=log(Ozone),x=Solar.R))\n\n\nCall:\nline(y = log(Ozone), x = Solar.R)\n\nCoefficients:\n[1]  2.74304  0.00341\n\nabline(coef(z1))\nz1.ls <- lm(log(Ozone) ~ Solar.R)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 250, y = 1, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Ozone),x=Solar.R,main=\"Linearity of sqrt(Ozone) and Solar.R\",xlab=\"Solar.R\",ylab=\"sqrt(Ozone)\")\n(z2 <- line(y=sqrt(Ozone),x=Solar.R))\n\n\nCall:\nline(y = sqrt(Ozone), x = Solar.R)\n\nCoefficients:\n[1]  3.808932  0.008582\n\nabline(coef(z2))\nz2.ls <- lm(sqrt(Ozone) ~ Solar.R)\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 10, y = 12, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\ns2=Solar.R^2\nplot(y=Ozone,x=s2,main=\"Linearity of Ozone and Solar.R^2\",xlab=\"Solar.R^2\",ylab=\"Ozone\")\n(z3 <- line(y=Ozone,x=s2))\n\n\nCall:\nline(y = Ozone, x = s2)\n\nCoefficients:\n[1]  1.935e+01  2.498e-04\n\nabline(coef(z3))\nz3.ls <- lm(Ozone ~ s2)\nabline(z3.ls$coef, lty=2,col=\"red\")\nlegend(x = 10, y =150, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline:log y ~ x\")\nabline(0,0) \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline:sqrt y ~ x\")\nabline(0,0) \n\n\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by rline: y~ x^2\")\nabline(0,0) \n\n\n\n# log 변환 \nboxplot(residuals(z1)) # outlier 1개\n\n\n\nboxplot(residuals(z2)) # outlier 없음\n\n\n\nboxplot(residuals(z3)) # outlier 2개\n\n\n\n# stem and leaf\nstem(residuals(z1))\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -26 | 7\n  -24 | \n  -22 | \n  -20 | \n  -18 | \n  -16 | \n  -14 | 44\n  -12 | 4342\n  -10 | 4\n   -8 | 96624\n   -6 | 763776533321\n   -4 | 88610854\n   -2 | 865407632\n   -0 | 8744221088742\n    0 | 001139\n    2 | 0344802445799\n    4 | 0114735\n    6 | 44568\n    8 | 1280002234469\n   10 | 602689\n   12 | 44556\n   14 | 7\n\nstem(residuals(z2))\n\n\n  The decimal point is at the |\n\n  -3 | 20\n  -2 | 999432000\n  -1 | 7666655544443221111000\n  -0 | 9999766543333222221110\n   0 | 0013357899999\n   1 | 001111222677\n   2 | 0122667799\n   3 | 223344577789\n   4 | 1679\n   5 | 0015\n   6 | \n   7 | 1\n\nstem(residuals(z3))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -3 | 43\n  -2 | 976400\n  -1 | 986555554433332221111000\n  -0 | 99998776655543333222211\n   0 | 00011446\n   1 | 011122233344568\n   2 | 1459\n   3 | 01279999\n   4 | 2569\n   5 | 1117789\n   6 | 03\n   7 | 06\n   8 | 0366\n   9 | 8\n  10 | \n  11 | \n  12 | \n  13 | 5\n\ncor.test(Ozone,Solar.R,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and Solar.R\nt = 3.8798, df = 109, p-value = 0.0001793\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.173194 0.502132\nsample estimates:\n      cor \n0.3483417 \n\ncor.test(log(Ozone),Solar.R,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Ozone) and Solar.R\nt = 5.3509, df = 109, p-value = 4.885e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2947757 0.5921585\nsample estimates:\n      cor \n0.4561082 \n\ncor.test(sqrt(Ozone),Solar.R,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(Ozone) and Solar.R\nt = 4.5948, df = 109, p-value = 1.17e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2339963 0.5480629\nsample estimates:\n      cor \n0.4028201 \n\ncor.test(Ozone,s2,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and s2\nt = 2.8565, df = 109, p-value = 0.00513\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.08152183 0.42918594\nsample estimates:\n      cor \n0.2639047 \n\n# y변수에 그래프의 잔차제곱 합\nsum(residuals(z1)^2)\n\n[1] 66.29339\n\nsum(residuals(z2)^2)\n\n[1] 596.0805\n\nsum(residuals(z3)^2)\n\n[1] 129877.1\n\ndetach(airquality)\n\n총 3가지 변환 중 y축에 대한 변환들이 잔차들이 안정되어진 것으로 보인다. 1번 변환 (y변수만 log 변환)의 경우 자료가 잘 퍼져있다는 장점이 있으나 잔차들의 곡선의 경향성이 완전히 사라지지는 않은 것으로 보인다. 잔차의 크기가 작은 편이다. 2번 변환 (y변수만 sqrt 변환)의 경우 자료에서의 곡선성이 거의 사라졌다는 장점이 있으나 잔차의 y축으로 퍼진 범위가 넓어졌다.\n변환한 그래프들을 기준으로 전반적으로 Ozone량과 Solar.R의 값은 양의 상관관계를 가지고 있는 것으로 보인다.\n\n\nOzone~Wind\n\nattach(airquality)\nattach(airquality)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\nplot(Ozone~Wind,main=\"linearity of Ozone ~ Wind\")\n(z <- line(x=Wind,y=Ozone))\n\n\nCall:\nline(x = Wind, y = Ozone)\n\nCoefficients:\n[1]  113.175   -7.417\n\nabline(coef(z))\nz.ls <- lm(Ozone~Wind)\nabline(z.ls$coef, lty=2,col=\"red\")\nlegend(x = 16, y = 150, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nairquality2=airquality[order(airquality$Wind),]\n\nmedOzone <- as.vector(3)\nmedWind <- as.vector(3)\nmedOzone[1] <- median(airquality2$Ozone[1:51],na.rm=T); medWind[1] <- median(airquality2$Wind[1:51],na.rm=T)\nmedOzone[2] <- median(airquality2$Ozone[52:102],na.rm=T); medWind[2] <- median(airquality2$Wind[52:102],na.rm=T)\nmedOzone[3] <- median(airquality2$Ozone[103:153],na.rm=T); medWind[3] <- median(airquality2$Wind[103:153],na.rm=T)\nplot(medOzone ~ medWind, type=\"b\") \n\n\n\n# 감소하는 convex 함수의 관계인 것으로 보인다. y변수 변환을 통해 선형성을 확보\nplot(y=log(Ozone),x=Wind,main=\"Linearity of log(Ozone) and Wind\")\n(z1 <- line(y=log(Ozone),x=Wind))\n\n\nCall:\nline(y = log(Ozone), x = Wind)\n\nCoefficients:\n[1]   5.3333  -0.1924\n\nabline(coef(z1))\nz1.ls <- lm(log(Ozone) ~ Wind)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 2, y = 1, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Ozone),x=Wind,main=\"Linearity of sqrt(Ozone) and Wind\")\n(z2 <- line(y=sqrt(Ozone),x=Wind))\n\n\nCall:\nline(y = sqrt(Ozone), x = Wind)\n\nCoefficients:\n[1]  11.8507  -0.5891\n\nabline(coef(z2))\nz2.ls <- lm(sqrt(Ozone) ~ Wind)\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 16, y = 12, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nw2=Wind^2\nplot(y=Ozone,x=w2,main=\"Linearity of Ozone and Wind^2\")\n(z3 <- line(y=Ozone,x=w2))\n\n\nCall:\nline(y = Ozone, x = w2)\n\nCoefficients:\n[1]  79.8590  -0.3743\n\nabline(coef(z3))\nz3.ls <- lm(Ozone ~ w2)\nabline(z3.ls$coef, lty=2,col=\"red\")\nlegend(x = 300, y =150, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nboxplot(residuals(z1)) # three outliers\n\n\n\nboxplot(residuals(z2)) # one outlier\n\n\n\nboxplot(residuals(z3)) # two outliers\n\n\n\nstem(residuals(z1))\n\n\n  The decimal point is at the |\n\n  -3 | 5\n  -3 | \n  -2 | \n  -2 | 11\n  -1 | \n  -1 | 4221100\n  -0 | 988887776666665555\n  -0 | 444433333333222222221111110000\n   0 | 000111122222223333333344444\n   0 | 555555555566677777899999\n   1 | 01133\n   1 | 6\n   2 | 3\n\nstem(residuals(z2))\n\n\n  The decimal point is at the |\n\n  -5 | 11\n  -4 | 1\n  -3 | 85111\n  -2 | 9887433322100\n  -1 | 7766655432111100000\n  -0 | 998888887665443310\n   0 | 0011222344455666999\n   1 | 00012234445666777999\n   2 | 01222246899\n   3 | 134667\n   4 | 7\n   5 | \n   6 | 4\n\nstem(residuals(z3))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -5 | 5\n  -4 | 64310\n  -3 | 877643322\n  -2 | 9987765432110\n  -1 | 8887766666544220\n  -0 | 98876654444320\n   0 | 00134455667788999\n   1 | 01146799\n   2 | 00346689\n   3 | 0111123447\n   4 | 0277889\n   5 | 234\n   6 | 12\n   7 | 9\n   8 | \n   9 | 2\n  10 | \n  11 | 8\n\n# y변수 그래프의 잔차제곱 합\nsum(residuals(z1)^2)\n\n[1] 66.85564\n\nsum(residuals(z2)^2)\n\n[1] 489.5301\n\nsum(residuals(z3)^2)\n\n[1] 113187.1\n\ncor.test(Ozone,Wind,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and Wind\nt = -8.0401, df = 114, p-value = 9.272e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7063918 -0.4708713\nsample estimates:\n       cor \n-0.6015465 \n\ncor.test(log(Ozone),Wind,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Ozone) and Wind\nt = -6.822, df = 114, p-value = 4.551e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6563096 -0.3948621\nsample estimates:\n       cor \n-0.5384181 \n\ncor.test(sqrt(Ozone),Wind,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(Ozone) and Wind\nt = -7.8938, df = 114, p-value = 1.983e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7008452 -0.4622775\nsample estimates:\n       cor \n-0.5944899 \n\ncor.test(Ozone,w2,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and w2\nt = -6.2579, df = 114, p-value = 7.055e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6298998 -0.3561715\nsample estimates:\n      cor \n-0.505653 \n\ndetach(airquality)\n\n3가지 변환 중 Ozone에 sqrt를 취한 변환이 가장 적절해 보인다. Boxplot 기준으로 Residual에서 한개의 Outlier가 존재하고 median에서 hinge들 까지의 길이와 whisker의 길이들이 서로 각각 대칭적인 것으로 보인다. 줄기 잎 그림을 그렸더니 두 변환에서 모두 0을 중심으로 하는 종모양을 어느정도 이루고 있는 것으로 보인다. 변환 이전에 비해서 잔차들이 안정된 것으로 보이지만 완전하지 않다. 변환한 데이터들 기준으로 Ozone과 Wind는 전반적으로 음의 상관관계를 보이고 있는 것을 확인할 수 있다.\n\n### Ozone~Temp\nattach(airquality)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\nplot(Ozone~Temp,main=\"linearity of Ozone ~ Temp\")\n(z <- line(x=Temp,y=Ozone))\n\n\nCall:\nline(x = Temp, y = Ozone)\n\nCoefficients:\n[1]  -194.216     3.027\n\nabline(coef(z))\nz.ls <- lm(Ozone~Temp)\nabline(z.ls$coef, lty=2,col=\"red\")\nlegend(x = 58, y = 150, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# 직선에서 많이 떨어져있는 3~5개정도의 점을 제외하면 전반적으로 변수들이 양의 상관관계를 보이고 있는 것으로 보인다.\n\nairquality3=airquality[order(airquality$Temp),]\n\nmedOzone <- as.vector(3)\nmedTemp <- as.vector(3)\nmedOzone[1] <- median(airquality3$Ozone[1:51],na.rm=T); medTemp[1] <- median(airquality3$Temp[1:51],na.rm=T)\nmedOzone[2] <- median(airquality3$Ozone[52:102],na.rm=T); medTemp[2] <- median(airquality3$Temp[52:102],na.rm=T)\nmedOzone[3] <- median(airquality3$Ozone[103:153],na.rm=T); medTemp[3] <- median(airquality3$Temp[103:153],na.rm=T)\nplot(medOzone ~ medTemp, type=\"b\") \n\n\n\n# 증가하는 convex 함수의 관계인 것으로 보인다. y변수 변환을 통해 선형성을 확보 \n\nplot(y=log(Ozone),x=Temp,main=\"Linearity of log ozone and temp\",xlab=\"temp\",ylab=\"log Ozone\")\n(z1<- line(y=log(Ozone),x=(Temp)))\n\n\nCall:\nline(y = log(Ozone), x = (Temp))\n\nCoefficients:\n[1]  -2.8968   0.0813\n\nabline(coef(z1))\nz1.ls <- lm(log(Ozone) ~ Temp)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 58, y = 5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Ozone),x=Temp,main=\"Linearity of sqrt ozone and temp\",xlab=\"temp\",ylab=\"sqrt Ozone\")\n(z2<- line(y=sqrt(Ozone),x=Temp))\n\n\nCall:\nline(y = sqrt(Ozone), x = Temp)\n\nCoefficients:\n[1]  -12.7938    0.2424\n\nabline(coef(z2))\nz2.ls <- lm(sqrt(Ozone) ~ Temp)\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 58, y = 12, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nt2=Temp^2\nplot(Ozone~t2,main=\"linearity of Ozone ~ Temp^2\")\n(z3 <- line(x=t2,y=Ozone))\n\n\nCall:\nline(x = t2, y = Ozone)\n\nCoefficients:\n[1]  -79.53178    0.01972\n\nabline(coef(z3))\nz3.ls <- lm(Ozone~t2)\nabline(z3.ls$coef, lty=2,col=\"red\")\nlegend(x = 3000, y = 150, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nboxplot(residuals(z1)) # 5 outliers\n\n\n\nboxplot(residuals(z2)) # 2 outliers\n\n\n\nboxplot(residuals(z3)) # 3 outliers\n\n\n\nstem(residuals(z1))\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -18 | 0\n  -16 | 6\n  -14 | 9\n  -12 | \n  -10 | 700\n   -8 | \n   -6 | 7764228864331\n   -4 | 65187640\n   -2 | 9975322111074321\n   -0 | 866655420732111\n    0 | 112356679002345889\n    2 | 05666889113344689\n    4 | 175579\n    6 | 11357557\n    8 | 67\n   10 | 4676\n   12 | 2\n   14 | 04\n\nstem(residuals(z2))\n\n\n  The decimal point is at the |\n\n  -4 | 0\n  -3 | 811\n  -2 | 65311000\n  -1 | 9987777666444443333322221000\n  -0 | 888888876665522210\n   0 | 0011112233344455555677788888999\n   1 | 012333334455588\n   2 | 013668\n   3 | 007\n   4 | 04\n   5 | \n   6 | 1\n\nstem(residuals(z3))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -4 | 10\n  -3 | 773100\n  -2 | 88754332111\n  -1 | 888877666666655444443211100\n  -0 | 999977654220\n   0 | 011223344567888899999\n   1 | 0002222233344445556889\n   2 | 12238\n   3 | 00128\n   4 | 55\n   5 | \n   6 | \n   7 | 15\n   8 | \n   9 | \n  10 | \n  11 | 8\n\n# y변수 그래프의 잔차제곱 합\nsum(residuals(z1)^2)\n\n[1] 40.98835\n\nsum(residuals(z2)^2)\n\n[1] 332.1941\n\nsum(residuals(z3)^2)\n\n[1] 64694.05\n\ncor.test(Ozone,Temp)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and Temp\nt = 10.418, df = 114, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5913340 0.7812111\nsample estimates:\n      cor \n0.6983603 \n\ncor.test(sqrt(Ozone),(Temp))\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(Ozone) and (Temp)\nt = 11.869, df = 114, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6491335 0.8152728\nsample estimates:\n      cor \n0.7434629 \n\ncor.test(log(Ozone),(Temp))\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Ozone) and (Temp)\nt = 11.741, df = 114, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6444258 0.8125409\nsample estimates:\n      cor \n0.7398211 \n\ncor.test((Ozone),t2)\n\n\n    Pearson's product-moment correlation\n\ndata:  (Ozone) and t2\nt = 10.859, df = 114, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6100642 0.7923742\nsample estimates:\n      cor \n0.7130697 \n\ndetach(airquality)\n\n3가지 변환 중 sqrt 변환이 가장 적절해 보인다. boxplot 기준으로 median에서 hinge들까지의 거리가 적절해보이고 (다소 위쪽으로 몰려있는 것 처럼 보이지만), whisker의 길이도 대칭적인 것으로 보인다. 다만 outlier의 수는 2개이다. Stem and Leaf Plot에서 변환 이후 데이터에서 종 모양이 대칭적으로 나타나는 것 처럼 보인다.\n변환한 데이터들 기준으로 Ozone과 Temp는 전반적으로 양의 상관관계를 보이고 있는 것을 확인할 수 있다.\n변환된 데이터들의 Residual plot들을 기준으로 볼 때 예측력이 가장 높은 지표는 Wind변수이고, 다음으로는 Temp변수이고, 다음으로는 Solar.R변수 인 것으로 추측된다. Residual들이 가장 넓게 퍼져있는 지표는 Soalr.R변수이고 나머지 두변수들은 Residual이 원형으로 다소 뭉쳐있는데 상대적으로 Temp 변수들이 많이 퍼져있기 때문에 Wind 변수의 순위를 높게 잡았다.\nX~Y둘다 변환\n\nchoose(3,1)\n\n[1] 3\n\nattach(airquality)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n# Ozone ~ Solar.R\ncor.test(Ozone,Solar.R)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and Solar.R\nt = 3.8798, df = 109, p-value = 0.0001793\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.173194 0.502132\nsample estimates:\n      cor \n0.3483417 \n\nplot(y=log(Ozone),x=Solar.R,main=\"R-line\")\nabline(line(y=log(Ozone),x=Solar.R)$coeff)\n\n\n\nplot(y=log(Ozone),x=log(Solar.R),main=\"R-line: log\")\n(z1 <- line(y=log(Ozone),x=log(Solar.R)))\n\n\nCall:\nline(y = log(Ozone), x = log(Solar.R))\n\nCoefficients:\n[1]  0.8148  0.5269\n\nabline(coef(z1))\nz1.ls <- lm(log(Ozone) ~ log(Solar.R))\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 2, y = 4.5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Ozone),x=sqrt(Solar.R),main=\"R-line: sqrt\")\n(z2 <- line(y=sqrt(Ozone),x=sqrt(Solar.R)))\n\n\nCall:\nline(y = sqrt(Ozone), x = sqrt(Solar.R))\n\nCoefficients:\n[1]  2.5206  0.2168\n\nabline(coef(z2))\nz2.ls <- lm(sqrt(Ozone) ~ sqrt(Solar.R))\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 3, y = 12, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline: log\") #오른쪽으로 잔차들이 몰리게 되었다는 점에서 부적절한 변환으로 보임.\nabline(0,0)  \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline: sqrt\") #한개의 outlier만 제외 하면 전반적으로 변환 이전에 비해서 잔차들이 잘 퍼져있는 것으로 보임. \nabline(0,0)\n\n\n\nboxplot(residuals(z1)) # outlier 없음\n\n\n\nboxplot(residuals(z2)) # outlier 1개\n\n\n\nstem(residuals(z1)) \n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -18 | 1\n  -16 | \n  -14 | 650\n  -12 | 24\n  -10 | 332\n   -8 | 765421544\n   -6 | 84300874\n   -4 | 642219641\n   -2 | 7311999984320\n   -0 | 9887632\n    0 | 013893334777\n    2 | 005577825\n    4 | 369277\n    6 | 245023367\n    8 | 01112713337\n   10 | 25788049\n   12 | \n   14 | 3\n\nstem(residuals(z2)) \n\n\n  The decimal point is at the |\n\n  -3 | 1\n  -2 | 997431100\n  -1 | 876655554444432210\n  -0 | 999888666665443322221111000\n   0 | 033467899999\n   1 | 0011111155677\n   2 | 0222677788\n   3 | 113344667888\n   4 | 0568\n   5 | 0115\n   6 | \n   7 | 1\n\n# Y만 변환일 때에 비해 잔차들이 보이던 곡선의 경향성이 다소 완화되어진 것을 확인할 수 있다.\n\n\n# Ozone ~ Wind\ncor.test(Ozone,Wind)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and Wind\nt = -8.0401, df = 114, p-value = 9.272e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7063918 -0.4708713\nsample estimates:\n       cor \n-0.6015465 \n\nplot(y=log(Ozone),x=Wind,main=\"R-line\")\nabline(line(y=log(Ozone),x=Wind)$coeff)\n\n\n\nplot(line(y=log(Ozone),x=Wind)$residuals~line(y=log(Ozone),x=Wind)$fit)\nabline(0,0) # 다소 곡선의 형태\n\n\n\nplot(y=log(Ozone),x=log(Wind),main=\"R-line: log\")\n(z1 <- line(y=log(Ozone),x=log(Wind)))\n\n\nCall:\nline(y = log(Ozone), x = log(Wind))\n\nCoefficients:\n[1]   7.492  -1.845\n\nabline(coef(z1))\nz1.ls <- lm(log(Ozone) ~ log(Wind))\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 0.5, y = 4.5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Ozone),x=sqrt(Wind),main=\"R-line: sqrt\")\n(z2 <- line(y=sqrt(Ozone),x=sqrt(Wind)))\n\n\nCall:\nline(y = sqrt(Ozone), x = sqrt(Wind))\n\nCoefficients:\n[1]  17.078  -3.664\n\nabline(coef(z2))\nz2.ls <- lm(sqrt(Ozone) ~ sqrt(Wind))\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 1.5, y = 4, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline: log\") # Outlier 제외할 경우에도 약간의 곡선의 흔적이 남아있는 것으로 보임.\nabline(0,0)  \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline: sqrt\") # 여전히 완벽하게 잔차를 퍼트리지는 못한 것으로 보이고 곡선의 흔적이 보임\nabline(0,0)\n\n\n\nboxplot(residuals(z1)) # outlier 3개\n\n\n\nboxplot(residuals(z2)) # outlier 1개\n\n\n\nstem(residuals(z1)) \n\n\n  The decimal point is at the |\n\n  -3 | 3\n  -2 | \n  -2 | 0\n  -1 | 9\n  -1 | 322200\n  -0 | 99988766655555555\n  -0 | 44444333333322211111111111111000\n   0 | 0001111111222333333344444\n   0 | 5555566666666667788899\n   1 | 000002333\n   1 | 67\n\nstem(residuals(z2)) \n\n\n  The decimal point is at the |\n\n  -4 | 87\n  -3 | 751\n  -2 | 7766543300\n  -1 | 9977755222222100\n  -0 | 987777666555554443332221100\n   0 | 01345566678899\n   1 | 001233333344566678999\n   2 | 0001233444668\n   3 | 0137788\n   4 | 1\n   5 | 07\n\n# Y만 변환일 때에 비해 잔차들이 보이던 곡선의 경향성이 다소 완화되어진 것을 확인할 수 있다. 잔차들이 조금 더 y축을 기준으로 대칭적이 된 것을 확인할 수 있다.\n\n\n\n# Ozone ~ Temp\ncor.test(Ozone,Temp)\n\n\n    Pearson's product-moment correlation\n\ndata:  Ozone and Temp\nt = 10.418, df = 114, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5913340 0.7812111\nsample estimates:\n      cor \n0.6983603 \n\nplot(y=log(Ozone),x=Temp,main=\"R-line\")\nabline(line(y=log(Ozone),x=Temp)$coeff)\n\n\n\nplot(line(y=log(Ozone),x=Temp)$residuals~line(y=log(Ozone),x=Temp)$fitted)\nabline(0,0) # outleir을 제외하더라도 다소 대칭적이지는 않아보임 (곡선 관계가 남아있는 것 같아보이지만 확실하지는 않음)\n\n\n\nplot(y=log(Ozone),x=log(Temp),main=\"R-line: log\")\n(z1 <- line(y=log(Ozone),x=log(Temp)))\n\n\nCall:\nline(y = log(Ozone), x = log(Temp))\n\nCoefficients:\n[1]  -23.526    6.208\n\nabline(coef(z1))\nz1.ls <- lm(log(Ozone) ~ log(Temp))\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 4.05, y = 5., c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Ozone),x=sqrt(Temp),main=\"R-line: sqrt\")\n(z2 <- line(y=sqrt(Ozone),x=sqrt(Temp)))\n\n\nCall:\nline(y = sqrt(Ozone), x = sqrt(Temp))\n\nCoefficients:\n[1]  -31.28    4.24\n\nabline(coef(z2))\nz2.ls <- lm(sqrt(Ozone) ~ sqrt(Temp))\nabline(z2.ls$coef, lty=2,col=\"red\")\nlegend(x = 7.5, y = 12, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline: log\") # 곡선의 형태가 완화되고 잔차들이 y축을 기준으로 대칭되어 나타나는 것으로 보인다. \nabline(0,0)\n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline: sqrt\") # 여전히 완벽하게 잔차를 퍼트리지는 못한 것으로 보이고 곡선의 흔적이 보임\nabline(0,0)\n\n\n\nboxplot(residuals(z1)) # outlier 4개\n\n\n\nboxplot(residuals(z2)) # outlier 1개\n\n\n\nstem(residuals(z1))  # 0을 기준으로 종모양이 나타남\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -16 | 93\n  -14 | 6\n  -12 | 5\n  -10 | 66\n   -8 | 432100\n   -6 | 41108431\n   -4 | 54200776200\n   -2 | 987554229843322\n   -0 | 987651099931\n    0 | 133357788899013467\n    2 | 00235555679011224\n    4 | 468915669\n    6 | 03349\n    8 | 52\n   10 | 4345\n   12 | 17\n   14 | 7\n\nstem(residuals(z2)) \n\n\n  The decimal point is at the |\n\n  -4 | 0\n  -3 | 911\n  -2 | 6532110\n  -1 | 99888777776544444333222211100\n  -0 | 999888666665322110\n   0 | 0111222333455555566667788889\n   1 | 000122333444456789\n   2 | 11367\n   3 | 0128\n   4 | 03\n   5 | \n   6 | 1\n\n# 변환이후에는 변환 이전에 비해서 큰 차이가 없는 것 처럼 보이기는 한다. outlier들을 제외 하더라도 큰 차이는 없는 것으로 보인다.\n\n\nlmfit=lm(Ozone~Solar.R+Temp+Wind,data=airquality)\nsummary(lmfit)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Temp + Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nSolar.R       0.05982    0.02319   2.580  0.01124 *  \nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: < 2.2e-16\n\nlmfit2=lm(sqrt(Ozone)~(Solar.R)+(Temp)+(Wind),data=airquality)\nsummary(lmfit2)\n\n\nCall:\nlm(formula = sqrt(Ozone) ~ (Solar.R) + (Temp) + (Wind), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0994 -1.0902 -0.2060  0.8784  4.7782 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.284232   1.545516  -2.125 0.035888 *  \nSolar.R      0.005556   0.001554   3.574 0.000528 ***\nTemp         0.134491   0.016996   7.913 2.46e-12 ***\nWind        -0.220179   0.043869  -5.019 2.08e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.42 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6707,    Adjusted R-squared:  0.6614 \nF-statistic: 72.64 on 3 and 107 DF,  p-value: < 2.2e-16\n\nlmfit3=lm(log(Ozone)~(Solar.R)+(Temp)+(Wind),data=airquality)\nsummary(lmfit3)\n\n\nCall:\nlm(formula = log(Ozone) ~ (Solar.R) + (Temp) + (Wind), data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,    Adjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n\nlmfit4=lm(sqrt(Ozone)~sqrt(Solar.R)+sqrt(Temp)+sqrt(Wind),data=airquality)\nsummary(lmfit4)\n\n\nCall:\nlm(formula = sqrt(Ozone) ~ sqrt(Solar.R) + sqrt(Temp) + sqrt(Wind), \n    data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1313 -1.0142 -0.2502  0.9461  4.2580 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -9.59783    3.01971  -3.178 0.001937 ** \nsqrt(Solar.R)  0.12993    0.03477   3.737 0.000301 ***\nsqrt(Temp)     2.12867    0.29694   7.169 1.02e-10 ***\nsqrt(Wind)    -1.55165    0.26821  -5.785 7.28e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.392 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6837,    Adjusted R-squared:  0.6748 \nF-statistic: 77.09 on 3 and 107 DF,  p-value: < 2.2e-16\n\n\n변환전 데이터로 회귀분석 기준으로 변수들의 설명력을 볼 경우 Wind, Temp, Solar.R순서이다. 다만, 변환이 이루어지지 않았기 때문에 극단치에 영향을 많이 받았을 가능성이 있다. Sqrt나 Log변환을 진행 한 이후 회귀분석을 진행하면 영향력이 강한 독립변수는 Wind, Temp, Solar.R동일한 순서인 것으로 보인다.\n위에서 상관계수를 계산했을 때 확인했던 것처럼 Wind는 음의 회귀계수, 나머지 두 독립변수는 양의 회귀게수이다.\n(결론)\nOzone 변수에 Sqrt를 취한 값에 대한 적합값의 잔차들을 제곱해서 더할 경우 다음과 같다. Solar.R: 596.0805 Wind: 489.5301 Temp: 332.194\n잔차 제곱합의 기준에서 가장 작은 순서대로 fitting이 잘 되었다고 하였을 때, Temp, Wind, Solar.R 변수들의 순서로 Multiple r-line을 적합해보고자 한다."
  },
  {
    "objectID": "posts/EDA7/index.html#나-1",
    "href": "posts/EDA7/index.html#나-1",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(나)",
    "text": "(나)\n세 X 변수 사이의 종속성(dependency)을 X-Y 그래프로 직선 또는 곡선 관계의 경향이 있는 지 판단하여라. 세 변수가 한꺼번에 종속 관계가 있을 수 있으나 3차원 그래프를 이용해야 하므로 일단 세 변수 중 두 개씩 골라 X-Y 그래프를 그리고 두 변수 사이의 직선 또는 곡선 관계를 파악하여라. 필요한 경우에는 r-line으로 적합하여 판단하여라. (이러한 분석은 다중회귀식으로 모형을 확장할 때 필요한 작업이다)\n\nchoose(3,2)\n\n[1] 3\n\n\n\nWind~Solar.R\n\nattach(airquality)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n\nThe following objects are masked from airquality (pos = 4):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\nplot(Wind~Solar.R,main=\"linearity of Solar.R ~ Wind\")\n\n\n\ncor.test(Wind,Solar.R) # 상관관계가 없음\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and Solar.R\nt = -0.6826, df = 144, p-value = 0.496\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2172359  0.1066406\nsample estimates:\n        cor \n-0.05679167 \n\ncor.test(Wind,sqrt(Solar.R)) # 상관관계가 없음\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and sqrt(Solar.R)\nt = -1.1529, df = 144, p-value = 0.2509\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.25413132  0.06787189\nsample estimates:\n        cor \n-0.09563089 \n\ncor.test(Wind,log(Solar.R)) # 상관관계가 없음\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and log(Solar.R)\nt = -1.4876, df = 144, p-value = 0.139\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27988233  0.04022473\nsample estimates:\n       cor \n-0.1230276 \n\ncor.test(Wind,log(Solar.R)) # 상관관계가 없음\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and log(Solar.R)\nt = -1.4876, df = 144, p-value = 0.139\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27988233  0.04022473\nsample estimates:\n       cor \n-0.1230276 \n\ncor.test(Wind,sqrt(Solar.R)) # 상관관계가 없음\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and sqrt(Solar.R)\nt = -1.1529, df = 144, p-value = 0.2509\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.25413132  0.06787189\nsample estimates:\n        cor \n-0.09563089 \n\ndetach(airquality)\n\n태양 복사와 바람 변수는 서로 관계가 없는 것으로 보인다.\n\n\nWind~Temp\n\nattach(airquality)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n\nThe following objects are masked from airquality (pos = 4):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\nplot(Wind~Temp,main=\"linearity of Wind ~ Temp\")\n\n(z <- line(x=Temp,y=Wind))\n\n\nCall:\nline(x = Temp, y = Wind)\n\nCoefficients:\n[1]  24.7222  -0.1944\n\nabline(coef(z))\nz.ls <- lm(Wind~Temp)\nabline(z.ls$coef, lty=2,col=\"red\")\nlegend(x = 58, y = 5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\ncor.test(Wind,Temp,na.rm=T) #음의 상관관계\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and Temp\nt = -6.3308, df = 151, p-value = 2.642e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5748874 -0.3227660\nsample estimates:\n       cor \n-0.4579879 \n\nairquality4=airquality[order(airquality$Temp),]\n\nmedWind <- as.vector(3)\nmedTemp <- as.vector(3)\nmedWind[1] <- median(airquality4$Wind[1:51],na.rm=T); medTemp[1] <- median(airquality4$Temp[1:51],na.rm=T)\nmedWind[2] <- median(airquality4$Wind[52:102],na.rm=T); medTemp[2] <- median(airquality4$Temp[52:102],na.rm=T)\nmedWind[3] <- median(airquality4$Wind[103:153],na.rm=T); medTemp[3] <- median(airquality4$Temp[103:153],na.rm=T)\nplot(medWind ~ medTemp, type=\"b\") \n\n\n\n#감소하는 Concave 한 곡선형태\n\nplot(y=log(Wind),x=Temp,main=\"Linearity of log Wind and temp\")\n(z1<- line(y=log(Wind),x=(Temp)))\n\n\nCall:\nline(y = log(Wind), x = (Temp))\n\nCoefficients:\n[1]   3.81332  -0.02016\n\nabline(coef(z1))\nz1.ls <- lm(log(Wind) ~ Temp)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 58, y = 1, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Wind),x=Temp,main=\"Linearity of sqrt Wind and temp\")\n(z1<- line(y=sqrt(Wind),x=(Temp)))\n\n\nCall:\nline(y = sqrt(Wind), x = (Temp))\n\nCoefficients:\n[1]   5.51706  -0.03126\n\nabline(coef(z1))\nz1.ls <- lm(sqrt(Wind) ~ Temp)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 58, y = 2, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nt2=Temp^2\nplot(y=Wind,x=t2,main=\"Linearity of Wind and temp^2\")\n(z1<- line(y=(Wind),x=t2))\n\n\nCall:\nline(y = (Wind), x = t2)\n\nCoefficients:\n[1]  17.456566  -0.001263\n\nabline(coef(z1))\nz1.ls <- lm(Wind ~ t2)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 3300, y = 5, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline\") #가장 적절해 보인다\nabline(0,0) \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nboxplot(residuals(z1)) # 2 outliers\n\n\n\nboxplot(residuals(z2)) # 3 outliers\n\n\n\nboxplot(residuals(z3)) # 3 outliers\n\n\n\nstem(residuals(z1))\n\n\n  The decimal point is at the |\n\n  -8 | 5\n  -7 | \n  -6 | \n  -5 | 864\n  -4 | 44420\n  -3 | 9977766554331\n  -2 | 9999888886433220\n  -1 | 99877643221110000\n  -0 | 9888877554443321111100\n   0 | 0001112333345555557778889\n   1 | 11357899\n   2 | 0111244559\n   3 | 12224446699\n   4 | 1234467999\n   5 | 0357779\n   6 | 6\n   7 | 039\n   8 | \n   9 | 8\n\nstem(residuals(z2))\n\n\n  The decimal point is at the |\n\n  -4 | 0\n  -3 | 911\n  -2 | 6532110\n  -1 | 99888777776544444333222211100\n  -0 | 999888666665322110\n   0 | 0111222333455555566667788889\n   1 | 000122333444456789\n   2 | 11367\n   3 | 0128\n   4 | 03\n   5 | \n   6 | 1\n\nstem(residuals(z3))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -4 | 10\n  -3 | 773100\n  -2 | 88754332111\n  -1 | 888877666666655444443211100\n  -0 | 999977654220\n   0 | 011223344567888899999\n   1 | 0002222233344445556889\n   2 | 12238\n   3 | 00128\n   4 | 55\n   5 | \n   6 | \n   7 | 15\n   8 | \n   9 | \n  10 | \n  11 | 8\n\ncor.test(log(Wind),Temp,na.rm=T) #음의 상관관계\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Wind) and Temp\nt = -6.0416, df = 151, p-value = 1.141e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5606394 -0.3038034\nsample estimates:\n       cor \n-0.4412121 \n\ncor.test(sqrt(Wind),Temp,na.rm=T) #음의 상관관계\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(Wind) and Temp\nt = -6.3065, df = 151, p-value = 2.992e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5737080 -0.3211886\nsample estimates:\n      cor \n-0.456596 \n\ncor.test(Wind,t2,na.rm=T) #음의 상관관계\n\n\n    Pearson's product-moment correlation\n\ndata:  Wind and t2\nt = -6.3279, df = 151, p-value = 2.681e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5747457 -0.3225764\nsample estimates:\n       cor \n-0.4578206 \n\ndetach(airquality)\n\nTemp와 Wind변수는 서로 음의 상관관계가 나타나는 것으로 보인다. Temp와 log(Wind)변수의 관계는 선형성이 잘 드러나는 것으로 보이며 Boxplot을 통해 확인한 두개의 Outlier들을 제거하면 잔차들도 잘 퍼져있는 것을 확인할 수 있다.\n\n\nTemp~Solar.R\n\nattach(airquality)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n\nThe following objects are masked from airquality (pos = 4):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\nplot(Temp~Solar.R,main=\"linearity of Temp ~ Solar.R\")\n\n(z <- line(x=Solar.R,y=Temp))\n\n\nCall:\nline(x = Solar.R, y = Temp)\n\nCoefficients:\n[1]  75.42051   0.02051\n\nabline(coef(z))\nz.ls <- lm(Temp~Solar.R)\nabline(z.ls$coef, lty=2,col=\"red\")\nlegend(x = 10, y = 95, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\ncor.test(Temp,Solar.R,na.rm=T) #음의 상관관계\n\n\n    Pearson's product-moment correlation\n\ndata:  Temp and Solar.R\nt = 3.4437, df = 144, p-value = 0.0007518\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1187113 0.4194913\nsample estimates:\n      cor \n0.2758403 \n\nairquality5=airquality[order(airquality$Solar.R),]\n\nmedSolar <- as.vector(3)\nmedTemp <- as.vector(3)\nmedSolar[1] <- median(airquality5$Solar.R[1:51],na.rm=T); medTemp[1] <- median(airquality5$Temp[1:51],na.rm=T)\nmedSolar[2] <- median(airquality5$Solar.R[52:102],na.rm=T); medTemp[2] <- median(airquality5$Temp[52:102],na.rm=T)\nmedSolar[3] <- median(airquality5$Solar.R[103:153],na.rm=T); medTemp[3] <- median(airquality5$Temp[103:153],na.rm=T)\nplot(medTemp ~ medSolar, type=\"b\") \n\n\n\n# Concave 한곡선 형태\n\nplot(y=log(Temp),x=Solar.R,main=\"Linearity of log Temp and Solar.R\")\n(z1<- line(y=log(Temp),x=Solar.R))\n\n\nCall:\nline(y = log(Temp), x = Solar.R)\n\nCoefficients:\n[1]  4.323467  0.000263\n\nabline(coef(z1))\nz1.ls <- lm(log(Temp) ~ Solar.R)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 10, y =4.57, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\nplot(y=sqrt(Temp),x=Solar.R,main=\"Linearity of sqrt Temp and Solar.R\")\n(z1<- line(y=sqrt(Temp),x=(Solar.R)))\n\n\nCall:\nline(y = sqrt(Temp), x = (Solar.R))\n\nCoefficients:\n[1]  8.685351  0.001161\n\nabline(coef(z1))\nz1.ls <- lm(sqrt(Temp) ~ Solar.R)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 10, y = 9.8, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\ns2=Solar.R^2\nplot(y=Temp,x=s2,main=\"Linearity of Temp and Solar.R^2\")\n(z1<- line(y=Temp,x=s2))\n\n\nCall:\nline(y = Temp, x = s2)\n\nCoefficients:\n[1]  7.654e+01  5.844e-05\n\nabline(coef(z1))\nz1.ls <- lm(Temp ~ s2)\nabline(z1.ls$coef, lty=2,col=\"red\")\nlegend(x = 100, y =97, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.5)\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline\") # 곡선 모양인 것으로 보인다.\nabline(0,0) \n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by rline\")\nabline(0,0) \n\n\n\nboxplot(residuals(z1)) # 1 outliers\n\n\n\nboxplot(residuals(z2)) # 2 outliers\n\n\n\nboxplot(residuals(z3)) # 3 outliers\n\n\n\nstem(residuals(z1))\n\n\n  The decimal point is at the |\n\n  -22 | 7\n  -20 | 3\n  -18 | 98181\n  -16 | 5800\n  -14 | 866576\n  -12 | 6095\n  -10 | 86644\n   -8 | 75537\n   -6 | 76543\n   -4 | 6643166542\n   -2 | 87765308650\n   -0 | 8766543098766432\n    0 | 24455677711222\n    2 | 02377791233\n    4 | 12344457901134666\n    6 | 189003\n    8 | 44578159\n   10 | 1470138\n   12 | 5682\n   14 | 4554\n   16 | 2\n   18 | 1\n\nstem(residuals(z2))\n\n\n  The decimal point is at the |\n\n  -4 | 0\n  -3 | 911\n  -2 | 6532110\n  -1 | 99888777776544444333222211100\n  -0 | 999888666665322110\n   0 | 0111222333455555566667788889\n   1 | 000122333444456789\n   2 | 11367\n   3 | 0128\n   4 | 03\n   5 | \n   6 | 1\n\nstem(residuals(z3))\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -4 | 10\n  -3 | 773100\n  -2 | 88754332111\n  -1 | 888877666666655444443211100\n  -0 | 999977654220\n   0 | 011223344567888899999\n   1 | 0002222233344445556889\n   2 | 12238\n   3 | 00128\n   4 | 55\n   5 | \n   6 | \n   7 | 15\n   8 | \n   9 | \n  10 | \n  11 | 8\n\ncor.test(log(Temp),Solar.R,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Temp) and Solar.R\nt = 3.471, df = 144, p-value = 0.0006847\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1208668 0.4212917\nsample estimates:\n      cor \n0.2778595 \n\ncor.test(sqrt(Temp),Solar.R,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  sqrt(Temp) and Solar.R\nt = 3.4587, df = 144, p-value = 0.0007142\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1198963 0.4204814\nsample estimates:\n      cor \n0.2769505 \n\ncor.test(Temp,s2,na.rm=T)\n\n\n    Pearson's product-moment correlation\n\ndata:  Temp and s2\nt = 2.2271, df = 144, p-value = 0.0275\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.0206361 0.3349912\nsample estimates:\n      cor \n0.1824728 \n\ndetach(airquality)\n\nSolar.R변수와 Temp 변수는 서로 양의 상관관계를 가지고 있다. (다만 2차함수 곡선의 관계인 것으로 보인다.) 변환을 통해 그러한 경향성을 완화시키고자 하였는데 log변환의 경우 여전히 Residual Plot에서 곡선의 경향성이 남아있다 반면, Sqrt 변환은 위의 두개의 Outlier을 제외하면 곡선의 경향성이 다소 완화되어진 것으로 보인다.\n정리하면 세개의 변수들을 서로 상관성이 있는 것으로 보이며, multicollinearity issue가 발생할 수 있기 대문에 회귀분석을 진행할 때 유의해야 할 것으로 보인다. (wind <– Temp <– Solar.R, 한단계 거친 상관성)\n결측치 자료를 불완전 자료(incomplete data, missing data)라고 하고 불완전하지만 정보를 갖고 있으니 최대한 살려서 분석하려고 하는 시도는 당연한 것이다. missing 된 부분을 다른 자료들의 패턴을 이용하여 complete로 만들어 사용하려는 방법들이 Sampling (표본조사론) 분야에서 연구되고 있다. 당연히 Bayesian 방법도 가능하다. EDA 분야까지 응용되지는 않고 있다. (말 그대로 E=exploration, 있는 그대로 explore 해야지)"
  },
  {
    "objectID": "posts/EDA7/index.html#가-2",
    "href": "posts/EDA7/index.html#가-2",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(가)",
    "text": "(가)\ntrees 자료를 multiple r-line으로 적합하여라.\n\nattach(trees)\n\ncor.test(log(Height),log(Girth))\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Height) and log(Girth)\nt = 3.3675, df = 29, p-value = 0.002155\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2165351 0.7446400\nsample estimates:\n      cor \n0.5301949 \n\ncor.test(log(Height),log(Volume))\n\n\n    Pearson's product-moment correlation\n\ndata:  log(Height) and log(Volume)\nt = 4.5895, df = 29, p-value = 7.928e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3821245 0.8155363\nsample estimates:\n      cor \n0.6486377 \n\nsummary(lm(log(Height)~log(Girth)+log(Volume),data=trees))\n\n\nCall:\nlm(formula = log(Height) ~ log(Girth) + log(Volume), data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.08628 -0.03084 -0.00146  0.02622  0.13465 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.91689    0.22497  21.856  < 2e-16 ***\nlog(Girth)  -0.82177    0.19043  -4.315 0.000179 ***\nlog(Volume)  0.46196    0.08454   5.464 7.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05234 on 28 degrees of freedom\nMultiple R-squared:  0.6521,    Adjusted R-squared:  0.6273 \nF-statistic: 26.24 on 2 and 28 DF,  p-value: 3.804e-07\n\n# 1) Height ~ Volume --> Girth\nplot(log(Height)~log(Volume),main=\"Linearity of log Height ~ log Volume\")\n(z1 <- line(x=log(Volume),y=log(Height)))\n\n\nCall:\nline(x = log(Volume), y = log(Height))\n\nCoefficients:\n[1]  4.13492  0.06263\n\nabline(coef(z1),lty=2,col=\"red\")\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by multiple rline: log Height ~ log Volume\")\nabline(0,0)\n\n\n\nplot(residuals(z1)~log(Girth),main=\"Linearity of Residuals ~ log Girth\")\n(z2 <- line(x=log(Girth),y=residuals(z1)))\n\n\nCall:\nline(x = log(Girth), y = residuals(z1))\n\nCoefficients:\n[1]  -0.02973   0.01057\n\nabline(coef(z2),lty=2,col=\"red\")\n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by multiple rline: Residual ~ log Girth\")\nabline(0,0)\n\n\n\nstem(residuals(z2)) # 0을 기준으로 대칭적인 종모양\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -1 | 8\n  -1 | 310\n  -0 | 9975\n  -0 | 3332100\n   0 | 0001111234\n   0 | 6689\n   1 | 01\n\nboxplot(residuals(z2)) # outlier 1개\n\n\n\n# 끝에 있는 한 점만 제외하면 전반적으로 Residual들이 적합이 잘 된 것으로 보인다.\n\nsum(residuals(z1)^2)\n\n[1] 0.1480616\n\n# 2) Height ~  Girth --> Volume \n\nplot(log(Height)~log(Girth),main=\"Linearity of log Height ~ log Girth\")\n(z3 <- line(x=log(Girth),y=log(Height)))\n\n\nCall:\nline(x = log(Girth), y = log(Height))\n\nCoefficients:\n[1]  3.987  0.137\n\nabline(coef(z3),lty=2,col=\"red\")\n\n\n\nplot(residuals(z3) ~ fitted(z3), main = \"Residual plot by multiple rline: log Height ~ log Girth\")\nabline(0,0)\n\n\n\nplot(y=residuals(z3),x=log(Volume),main=\"Linearity of Residuals ~ log Volume\")\n(z4 <- line(y=residuals(z3),x=log(Volume)))\n\n\nCall:\nline(y = residuals(z3), x = log(Volume))\n\nCoefficients:\n[1]  -0.037395   0.009474\n\nabline(coef(z4),lty=2,col=\"red\")\n\n\n\nplot(residuals(z4) ~ fitted(z4), main = \"Residual plot by multiple rline: Residual ~ log Volume\")\nabline(0,0)\n\n\n\n# 끝에 있는 한 점만 제외하면 전반적으로 Residual들이 적합이 잘 된 것으로 보인다.\n\nstem(residuals(z4)) #0 을 기준으로 대칭적인 종모양\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  -1 | 8\n  -1 | 31\n  -0 | 99885\n  -0 | 2221000\n   0 | 011122234\n   0 | 6679\n   1 | 122\n\nboxplot(residuals(z4)) # outlier 1개\n\n\n\nsum(residuals(z3)^2)\n\n[1] 0.1667806\n\ndetach(trees)\n\n두가지 방식 중 첫번째 방식이 더 적합해 보이는 것으로 보인다. 잔차 제곱의 합의 값이 첫번째 방식이 더 작게 나타나고 있기 때문이다. . 또한 회귀분석을 같이 놓고 돌렸을 때 계수의 값이 더 크게 나타나는 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/EDA7/index.html#나-2",
    "href": "posts/EDA7/index.html#나-2",
    "title": "EDA Assignment 7: Chapter 9",
    "section": "(나)",
    "text": "(나)\nairquality 자료를 multiple r-line으로 적합하여라.\nOzone ~ Temp ==> Wind ==> Solar.R (위에서 이유는 이미 언급함)\n\ndata(airquality)\n\nair2=airquality[!is.na(airquality$Ozone),]\nair2\n\n    Ozone Solar.R Wind Temp Month Day\n1      41     190  7.4   67     5   1\n2      36     118  8.0   72     5   2\n3      12     149 12.6   74     5   3\n4      18     313 11.5   62     5   4\n6      28      NA 14.9   66     5   6\n7      23     299  8.6   65     5   7\n8      19      99 13.8   59     5   8\n9       8      19 20.1   61     5   9\n11      7      NA  6.9   74     5  11\n12     16     256  9.7   69     5  12\n13     11     290  9.2   66     5  13\n14     14     274 10.9   68     5  14\n15     18      65 13.2   58     5  15\n16     14     334 11.5   64     5  16\n17     34     307 12.0   66     5  17\n18      6      78 18.4   57     5  18\n19     30     322 11.5   68     5  19\n20     11      44  9.7   62     5  20\n21      1       8  9.7   59     5  21\n22     11     320 16.6   73     5  22\n23      4      25  9.7   61     5  23\n24     32      92 12.0   61     5  24\n28     23      13 12.0   67     5  28\n29     45     252 14.9   81     5  29\n30    115     223  5.7   79     5  30\n31     37     279  7.4   76     5  31\n38     29     127  9.7   82     6   7\n40     71     291 13.8   90     6   9\n41     39     323 11.5   87     6  10\n44     23     148  8.0   82     6  13\n47     21     191 14.9   77     6  16\n48     37     284 20.7   72     6  17\n49     20      37  9.2   65     6  18\n50     12     120 11.5   73     6  19\n51     13     137 10.3   76     6  20\n62    135     269  4.1   84     7   1\n63     49     248  9.2   85     7   2\n64     32     236  9.2   81     7   3\n66     64     175  4.6   83     7   5\n67     40     314 10.9   83     7   6\n68     77     276  5.1   88     7   7\n69     97     267  6.3   92     7   8\n70     97     272  5.7   92     7   9\n71     85     175  7.4   89     7  10\n73     10     264 14.3   73     7  12\n74     27     175 14.9   81     7  13\n76      7      48 14.3   80     7  15\n77     48     260  6.9   81     7  16\n78     35     274 10.3   82     7  17\n79     61     285  6.3   84     7  18\n80     79     187  5.1   87     7  19\n81     63     220 11.5   85     7  20\n82     16       7  6.9   74     7  21\n85     80     294  8.6   86     7  24\n86    108     223  8.0   85     7  25\n87     20      81  8.6   82     7  26\n88     52      82 12.0   86     7  27\n89     82     213  7.4   88     7  28\n90     50     275  7.4   86     7  29\n91     64     253  7.4   83     7  30\n92     59     254  9.2   81     7  31\n93     39      83  6.9   81     8   1\n94      9      24 13.8   81     8   2\n95     16      77  7.4   82     8   3\n96     78      NA  6.9   86     8   4\n97     35      NA  7.4   85     8   5\n98     66      NA  4.6   87     8   6\n99    122     255  4.0   89     8   7\n100    89     229 10.3   90     8   8\n101   110     207  8.0   90     8   9\n104    44     192 11.5   86     8  12\n105    28     273 11.5   82     8  13\n106    65     157  9.7   80     8  14\n108    22      71 10.3   77     8  16\n109    59      51  6.3   79     8  17\n110    23     115  7.4   76     8  18\n111    31     244 10.9   78     8  19\n112    44     190 10.3   78     8  20\n113    21     259 15.5   77     8  21\n114     9      36 14.3   72     8  22\n116    45     212  9.7   79     8  24\n117   168     238  3.4   81     8  25\n118    73     215  8.0   86     8  26\n120    76     203  9.7   97     8  28\n121   118     225  2.3   94     8  29\n122    84     237  6.3   96     8  30\n123    85     188  6.3   94     8  31\n124    96     167  6.9   91     9   1\n125    78     197  5.1   92     9   2\n126    73     183  2.8   93     9   3\n127    91     189  4.6   93     9   4\n128    47      95  7.4   87     9   5\n129    32      92 15.5   84     9   6\n130    20     252 10.9   80     9   7\n131    23     220 10.3   78     9   8\n132    21     230 10.9   75     9   9\n133    24     259  9.7   73     9  10\n134    44     236 14.9   81     9  11\n135    21     259 15.5   76     9  12\n136    28     238  6.3   77     9  13\n137     9      24 10.9   71     9  14\n138    13     112 11.5   71     9  15\n139    46     237  6.9   78     9  16\n140    18     224 13.8   67     9  17\n141    13      27 10.3   76     9  18\n142    24     238 10.3   68     9  19\n143    16     201  8.0   82     9  20\n144    13     238 12.6   64     9  21\n145    23      14  9.2   71     9  22\n146    36     139 10.3   81     9  23\n147     7      49 10.3   69     9  24\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\nair2=air2[!is.na(air2$Solar.R),]\n\ndim(air2)\n\n[1] 111   6\n\ncolSums(is.na(air2))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n      0       0       0       0       0       0 \n\nattach(air2)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n\nThe following objects are masked from airquality (pos = 4):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n# 결측치들을 Drop해서 중간에 Fit에서 Length가 안 맞아서 생기는 오류를 해결하고자 하였음.\n\nplot(y=sqrt(Ozone),x=Temp,main=\"Linearity of sqrt Ozone ~ Temp\")\n(z <- line(x=Temp,y=sqrt(Ozone)))\n\n\nCall:\nline(x = Temp, y = sqrt(Ozone))\n\nCoefficients:\n[1]  -12.9829    0.2456\n\nabline(coef(z),lty=2,col=\"red\")\n\n\n\nplot(residuals(z) ~ fitted(z), main = \"Residual plot by multiple rline: sqrt Ozone ~ Temp\")\nabline(0,0)\n\n\n\nplot(y=residuals(z),x= Wind, main=\"Linearity of residuals ~ Wind\")\n(z1 <- line(x=Wind,y=residuals(z)))\n\n\nCall:\nline(x = Wind, y = residuals(z))\n\nCoefficients:\n[1]   1.1115  -0.1428\n\nabline(coef(z1),lty=2,col=\"red\")\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by multiple rline: residuals ~ Wind\")\nabline(0,0)\n\n\n\nplot(y=residuals(z1),x=Solar.R,main=\"Linearity of residuals ~ Solar.R\")\n(z2 <- line(x=Solar.R,y=residuals(z1)))\n\n\nCall:\nline(x = Solar.R, y = residuals(z1))\n\nCoefficients:\n[1]  -1.128031   0.005224\n\nabline(coef(z2),lty=2,col=\"red\")\n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by multiple rline: residuals ~ Solar.R\")\nabline(0,0)\n\n\n\nsum(residuals(z2)^2) # 328.3632\n\n[1] 328.3632\n\ndetach(air2)\n\nOzone ~ Wind ==> Temp ==> Solar.R (회귀분석 회귀계수가 큰 순서대로 )\n\ndata(airquality)\n\nair2=airquality[!is.na(airquality$Ozone),]\nair2\n\n    Ozone Solar.R Wind Temp Month Day\n1      41     190  7.4   67     5   1\n2      36     118  8.0   72     5   2\n3      12     149 12.6   74     5   3\n4      18     313 11.5   62     5   4\n6      28      NA 14.9   66     5   6\n7      23     299  8.6   65     5   7\n8      19      99 13.8   59     5   8\n9       8      19 20.1   61     5   9\n11      7      NA  6.9   74     5  11\n12     16     256  9.7   69     5  12\n13     11     290  9.2   66     5  13\n14     14     274 10.9   68     5  14\n15     18      65 13.2   58     5  15\n16     14     334 11.5   64     5  16\n17     34     307 12.0   66     5  17\n18      6      78 18.4   57     5  18\n19     30     322 11.5   68     5  19\n20     11      44  9.7   62     5  20\n21      1       8  9.7   59     5  21\n22     11     320 16.6   73     5  22\n23      4      25  9.7   61     5  23\n24     32      92 12.0   61     5  24\n28     23      13 12.0   67     5  28\n29     45     252 14.9   81     5  29\n30    115     223  5.7   79     5  30\n31     37     279  7.4   76     5  31\n38     29     127  9.7   82     6   7\n40     71     291 13.8   90     6   9\n41     39     323 11.5   87     6  10\n44     23     148  8.0   82     6  13\n47     21     191 14.9   77     6  16\n48     37     284 20.7   72     6  17\n49     20      37  9.2   65     6  18\n50     12     120 11.5   73     6  19\n51     13     137 10.3   76     6  20\n62    135     269  4.1   84     7   1\n63     49     248  9.2   85     7   2\n64     32     236  9.2   81     7   3\n66     64     175  4.6   83     7   5\n67     40     314 10.9   83     7   6\n68     77     276  5.1   88     7   7\n69     97     267  6.3   92     7   8\n70     97     272  5.7   92     7   9\n71     85     175  7.4   89     7  10\n73     10     264 14.3   73     7  12\n74     27     175 14.9   81     7  13\n76      7      48 14.3   80     7  15\n77     48     260  6.9   81     7  16\n78     35     274 10.3   82     7  17\n79     61     285  6.3   84     7  18\n80     79     187  5.1   87     7  19\n81     63     220 11.5   85     7  20\n82     16       7  6.9   74     7  21\n85     80     294  8.6   86     7  24\n86    108     223  8.0   85     7  25\n87     20      81  8.6   82     7  26\n88     52      82 12.0   86     7  27\n89     82     213  7.4   88     7  28\n90     50     275  7.4   86     7  29\n91     64     253  7.4   83     7  30\n92     59     254  9.2   81     7  31\n93     39      83  6.9   81     8   1\n94      9      24 13.8   81     8   2\n95     16      77  7.4   82     8   3\n96     78      NA  6.9   86     8   4\n97     35      NA  7.4   85     8   5\n98     66      NA  4.6   87     8   6\n99    122     255  4.0   89     8   7\n100    89     229 10.3   90     8   8\n101   110     207  8.0   90     8   9\n104    44     192 11.5   86     8  12\n105    28     273 11.5   82     8  13\n106    65     157  9.7   80     8  14\n108    22      71 10.3   77     8  16\n109    59      51  6.3   79     8  17\n110    23     115  7.4   76     8  18\n111    31     244 10.9   78     8  19\n112    44     190 10.3   78     8  20\n113    21     259 15.5   77     8  21\n114     9      36 14.3   72     8  22\n116    45     212  9.7   79     8  24\n117   168     238  3.4   81     8  25\n118    73     215  8.0   86     8  26\n120    76     203  9.7   97     8  28\n121   118     225  2.3   94     8  29\n122    84     237  6.3   96     8  30\n123    85     188  6.3   94     8  31\n124    96     167  6.9   91     9   1\n125    78     197  5.1   92     9   2\n126    73     183  2.8   93     9   3\n127    91     189  4.6   93     9   4\n128    47      95  7.4   87     9   5\n129    32      92 15.5   84     9   6\n130    20     252 10.9   80     9   7\n131    23     220 10.3   78     9   8\n132    21     230 10.9   75     9   9\n133    24     259  9.7   73     9  10\n134    44     236 14.9   81     9  11\n135    21     259 15.5   76     9  12\n136    28     238  6.3   77     9  13\n137     9      24 10.9   71     9  14\n138    13     112 11.5   71     9  15\n139    46     237  6.9   78     9  16\n140    18     224 13.8   67     9  17\n141    13      27 10.3   76     9  18\n142    24     238 10.3   68     9  19\n143    16     201  8.0   82     9  20\n144    13     238 12.6   64     9  21\n145    23      14  9.2   71     9  22\n146    36     139 10.3   81     9  23\n147     7      49 10.3   69     9  24\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\nair2=air2[!is.na(air2$Solar.R),]\n\ndim(air2)\n\n[1] 111   6\n\ncolSums(is.na(air2))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n      0       0       0       0       0       0 \n\nattach(air2)\n\nThe following objects are masked from airquality (pos = 3):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n\nThe following objects are masked from airquality (pos = 4):\n\n    Day, Month, Ozone, Solar.R, Temp, Wind\n\n# 결측치들을 Drop해서 중간에 Fit에서 Length가 안 맞아서 생기는 오류를 해결하고자 하였음.\n\nplot(y=sqrt(Ozone),x=(Wind),main=\"Linearity of sqrt Ozone ~ Wind\")\n(z <- line(x=(Wind),y=sqrt(Ozone)))\n\n\nCall:\nline(x = (Wind), y = sqrt(Ozone))\n\nCoefficients:\n[1]  11.5705  -0.5507\n\nabline(coef(z),lty=2,col=\"red\")\n\n\n\nplot(residuals(z) ~ fitted(z), main = \"Residual plot by multiple rline: sqrt Ozone ~ Wind\")\nabline(0,0)\n\n\n\nplot(y=residuals(z),x=Temp,main=\"Linearity of residuals ~ Temp\")\n(z1 <- line(x=Temp,y=residuals(z)))\n\n\nCall:\nline(x = Temp, y = residuals(z))\n\nCoefficients:\n[1]  -9.2509   0.1155\n\nabline(coef(z1),lty=2,col=\"red\")\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by multiple rline: residuals ~ Temp\")\nabline(0,0)\n\n\n\nplot(y=residuals(z1),x=Solar.R,main=\"Linearity of residuals ~ Solar.R\")\n(z2 <- line(x=Solar.R,y=residuals(z1)))\n\n\nCall:\nline(x = Solar.R, y = residuals(z1))\n\nCoefficients:\n[1]  -1.212414   0.005828\n\nabline(coef(z2),lty=2,col=\"red\")\n\n\n\nplot(residuals(z2) ~ fitted(z2), main = \"Residual plot by multiple rline: residuals ~ Solar.R\")\nabline(0,0)\n\n\n\nsum(residuals(z2)^2) #360.3426\n\n[1] 360.3426\n\ndetach(air2)\n\n아래의 방식보다 위의 방식에서 잔차제곱의 합이 작게 나타난다. 따라서 위의 방식에서의 적합이 더 잘 이루어졌음을 확인해볼 수 있다.\n주의: multiple R-line은 sequential 방법이다. simultaneous 방법으로 하면 다른 결과 나올 수 있다."
  },
  {
    "objectID": "posts/EDA8/index.html",
    "href": "posts/EDA8/index.html",
    "title": "EDA Assignment 8: Final Exam",
    "section": "",
    "text": "기말고사\nEDA 방법 이외의 회귀분석, 다변량, 실험계획 등에서 배운 다른 방법도 사용할 수 있으나 EDA 방법이 기본이 되어야 한다. 같은 자료를 여러 가지 방법으로 분석할 수 있다. 한 가지 방법으로 끝내지 말고 다른 방법은 없는지 생각하여 보아라. 자료파일은 ASCII 형태로 되어 있다. [보조프로그램]의 [메모장]으로 파일을 열어 그 형식을 확인한 후 엑셀을 통하거나 R에서 직접 읽어 들여라. 아래한글에서도 열 수 있으나 <문자코드>를 잘 선택하여야 한다.\n작업한 부분은 기한 이내에 제출하여야 그 부분만이라도 감점없이 점수를 받을 수 있다.\n\n1.\n다음 자료는 사춘기 여성의 신경성 식욕 감퇴증에 대하여 일정 기간 동안 세 가지 방법으로 치료한 결과이다. 어느 방법이 가장 큰 효과를 보았나? [ANOREXIA.DAT]\nI Cognitive behavioral treatment\nII Control; standard treatment\nIII Family therapy\nWeights in Kg\n전처리 후 정리\n\n# weights \n\n# I Cognitive behavioral treatment \ncbt=read.delim(file=\"C://r/ANOREXIA.DAT\",header=F)[c(1,2)]\nnames(cbt)=c(\"before\",\"after\")\nhead(cbt,5)\n\n  before after\n1   80.5  82.2\n2   84.9  85.6\n3   81.5  81.4\n4   82.6  81.9\n5   79.9  76.4\n\ndim(cbt)\n\n[1] 29  2\n\n# II Control; standard treatment\ncontrol=read.delim(file=\"C://r/ANOREXIA.DAT\",header=F)[c(3,4)][c(1:26),]\nnames(control)=c(\"before\",\"after\")\nhead(control,5)\n\n  before after\n1   80.7  80.2\n2   89.4  80.1\n3   91.8  86.4\n4   74.0  86.3\n5   78.1  76.1\n\ndim(control)\n\n[1] 26  2\n\n# III Family therapy\nfamily=read.delim(file=\"C://r/ANOREXIA.DAT\",header=F)[c(5,6)][c(1:17),]\nnames(family)=c(\"before\",\"after\")\nhead(family,5)\n\n  before after\n1   83.8  95.2\n2   83.3  94.3\n3   86.0  91.5\n4   82.5  91.9\n5   86.7 100.3\n\ndim(family)\n\n[1] 17  2\n\n\n항목의 수가 Cognitive behavioral treatment: 29 Control group: 26 Family Therapy: 17로 서로 다르다. 따라서 결측치를 처리하여 각각을 나누어서 표현하였다.\nTreatment별로 기술통계량을 제시하고 시각화 한 후 자료의 특성을 확인하고 각 Sample에 대한 paired t-Test를 진행할 예정이다.\n먼저, Cognitive behavioral treatment의 분석이다.\n\nsummary(cbt)\n\n     before          after      \n Min.   :70.00   Min.   : 71.3  \n 1st Qu.:80.40   1st Qu.: 81.9  \n Median :82.60   Median : 83.9  \n Mean   :82.69   Mean   : 85.7  \n 3rd Qu.:85.00   3rd Qu.: 90.9  \n Max.   :94.90   Max.   :103.6  \n\n85.7-82.69\n\n[1] 3.01\n\n83.9-82.60\n\n[1] 1.3\n\n# Cognitive behavioral treatment를 시행하기 이전 집단과 이후집단의 평균은 약 3kg정도 차이나고, median값은 1.3kg정도 차이나는 것으로 보인다.\n# 성장기의 식용 감퇴증이 다소 완화되어진 것을 확인할 수 있다.\nfivenum(cbt$before)\n\n[1] 70.0 80.4 82.6 85.0 94.9\n\nfivenum(cbt$after)\n\n[1]  71.3  81.9  83.9  90.9 103.6\n\n# Stem and leaf Plot을 통해서 두 집단의 분포를 비교해 보았는데 treament 이전 집단의 경우 stem 8*을 기준으로 leaf들이 어느정도 종모양의 대칭적 분포를 이루고 있는데, 반면 treatment 이후 집단의 경우 stem 8*이 중심이 되어지는 것은 맞지만 다소 퍼지게 된 것을 확인할 수 있다. \nlibrary(aplpack)\nstem.leaf.backback(cbt$before,cbt$after,rule.line = \"Sturges\")\n\n_____________________________________________________\n  1 | 2: represents 12, leaf unit: 1 \n             cbt$before       cbt$after          \n_____________________________________________________\n    1                 0|  7* |123                3   \n    6             99666|  7. |56                 5   \n  (15)  444333211100000|  8* |11122222334      (11)  \n    8           9877765|  8. |55569             13   \n    1                 4|  9* |03                 8   \n                       |  9. |5668               6   \n                       | 10* |03                 2   \n_____________________________________________________\nn:                   29       29                 \n_____________________________________________________\n\n# Boxplot\nboxplot(cbt,main=\"Boxplot of before and after Cognitive behavioral treatment\")\n\n\n\nboxplot(cbt)$out #처리 이전 집단의 총 두개의 Outlier: lower fence인 76.3 밖의 값과 outer fence인 89.2 밖의 두개의 값\n\n\n\n\n[1] 94.9 70.0\n\nboxplot(cbt)$stats\n\n     [,1]  [,2]\n[1,] 76.3  71.3\n[2,] 80.4  81.9\n[3,] 82.6  83.9\n[4,] 85.0  90.9\n[5,] 89.2 103.6\n\n# Skewness\nskewness = function(x) {\n  hl=fivenum(x)[2]\n  median=fivenum(x)[3]\n  hu=fivenum(x)[4]\n  skew=((hu-median)-(median-hl))/((hu-median)+(median-hl))\n  return(skew)\n}\nskewness(cbt$before) # 거의 Skewed 되어지지 않음 (매우 미세하게 skewed to the right)\n\n[1] 0.04347826\n\nskewness(cbt$after) # Boxplot에서 확인하였듯이 Skewed to the right 되어 있음.\n\n[1] 0.5555556\n\n# Letter Value Display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\nlvd1=(lsum(cbt$before,6))\nlvd2=(lsum(cbt$after,6))\nlvd1\n\n  letter depth lower    mid upper spread\n1      M  15.0 82.60 82.600 82.60   0.00\n2      H   8.0 80.40 82.700 85.00   4.60\n3      E   4.5 78.10 82.925 87.75   9.65\n4      D   2.5 76.40 82.675 88.95  12.55\n5      C   1.5 73.15 82.600 92.05  18.90\n6      B   1.0 70.00 82.450 94.90  24.90\n\nlvd2\n\n  letter depth lower    mid  upper spread\n1      M  15.0 83.90 83.900  83.90   0.00\n2      H   8.0 81.90 86.400  90.90   9.00\n3      E   4.5 76.05 86.250  96.45  20.40\n4      D   2.5 72.95 86.175  99.40  26.45\n5      C   1.5 71.90 86.950 102.00  30.10\n6      B   1.0 71.30 87.450 103.60  32.30\n\n# H-Spread\nlsum(cbt$before)[2,5]-lsum(cbt$before)[2,3]\n\n[1] 4.6\n\nlsum(cbt$after)[2,5]-lsum(cbt$after)[2,3] #더 넓게 퍼져있는 것을 확인할 수 있음. \n\n[1] 9\n\n# Kurtosis (E-spread / H-spread - 1.705)\n(lvd1[3,5]-lvd1[3,3])/(lvd1[2,5]-lvd1[2,3])-1.705 # more peaked than normal\n\n[1] 0.3928261\n\n(lvd2[3,5]-lvd2[3,3])/(lvd2[2,5]-lvd2[2,3])-1.705 # more peaked than normal (Kurtosis값도 더 크게 나타남)\n\n[1] 0.5616667\n\n# 정규성\nqqnorm(cbt$before, ylab=\"before quantiles\",main=\"before\");qqline(cbt$before, col='red',lty=2,lwd=2) #위에서 언급했던 두개의 Outlier들만 제외하면 대부분 정규분포를 잘 따르고 있는 것이 확인되어지고 있다. \nfiv1=fivenum(cbt$before)\n(pseudosigma1 = (fiv1[4]-fiv1[2])/1.34)\n\n[1] 3.432836\n\nsd(cbt$before) # 약 1.4정도의 차이\n\n[1] 4.845495\n\nabline(fiv1[3],pseudosigma1,col=\"blue\",lty=2,lwd=2)\nlegend(x = -2, y = 92, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 거의 일치중\n\n\n\nqqnorm(cbt$after, ylab=\"after quantiles\", main=\"after\");qqline(cbt$after, col='red',lty=2,lwd=2) #정규분포를 따른다고 보기 어려운 이유가 값들이 직선을 따르기 보다는 곡선의 형태를 띄고 있음을 확인할 수 있다.  \nfiv2=fivenum(cbt$after)\n(pseudosigma2 = (fiv2[4]-fiv2[2])/1.34)\n\n[1] 6.716418\n\nsd(cbt$after) # 약 1.6정도의 차이\n\n[1] 8.351924\n\nabline(fiv2[3],pseudosigma2,col=\"blue\",lty=2,lwd=2)\nlegend(x = -2, y = 100, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 약간차이를 보임\n\n\n\n# Paired T-test\n# H0: cbt$after-cbt$before=0  H1: cbt$after-cbt$before>0 \ncbt_test<-t.test(cbt$after,cbt$before, paired=TRUE, alternative=\"greater\",conf.level=0.05)\ncbt_test # reject null hypothesis, the weight of after treatment is heavier\n\n\n    Paired t-test\n\ndata:  cbt$after and cbt$before\nt = 2.2156, df = 28, p-value = 0.01751\nalternative hypothesis: true mean difference is greater than 0\n5 percent confidence interval:\n 5.315595      Inf\nsample estimates:\nmean difference \n       3.006897 \n\n\n다음으로는 통제집단의 분석이다.\n\nsummary(control)\n\n     before          after      \n Min.   :70.50   Min.   :73.00  \n 1st Qu.:77.72   1st Qu.:77.58  \n Median :80.65   Median :80.70  \n Mean   :81.56   Mean   :81.11  \n 3rd Qu.:85.88   3rd Qu.:84.67  \n Max.   :91.80   Max.   :89.60  \n\nmean(control$after)-mean(control$before)\n\n[1] -0.45\n\nmedian(control$after)-median(control$before)\n\n[1] 0.05\n\n# 통제집단의 경우 실험 시행하기 이전 집단과 실험 이후 집단의 평균은 0.45kg정도 차이를 보이며 중위값은 큰 차이가 없다.\n# Cognitive behavioral treatment에서 보였던 차이보다 적은 차이를 보이고 있는 것을 확인할 수 있다.\n\nfivenum(control$before)\n\n[1] 70.50 77.60 80.65 86.00 91.80\n\nfivenum(control$after)\n\n[1] 73.0 77.4 80.7 84.7 89.6\n\n# Stem and leaf Plot을 통해서 두 집단의 분포를 비교해 보았는데 처치 이전 집단의 경우 stem 7. stem 8. 두개의 축을 기준으로 Cluster를 가지고 있는데 비해 처치 이후의 집단의 경우 stem 7., stem8*두개의 stem을 기준으로 어느정도 대칭적으로 종모양을 보이고 있는 것이 확인된다.\nstem.leaf.backback(control$before,control$after,rule.line = \"Sturge\")\n\n_______________________________________\n  1 | 2: represents 12, leaf unit: 1 \n    control$before       control$after\n_______________________________________\n   3         420|  7* |33          2   \n  12   998887775|  7. |556778899  11   \n  (5)      44100|  8* |001111444  (9)  \n   9    99887655|  8. |666889      6   \n   1           1|  9* |                \n                |  9. |                \n                | 10* |                \n_______________________________________\nn:            26       26          \n_______________________________________\n\n# Boxplot\nboxplot(control,main=\"Boxplot of before and after standard treatment\")\n\n\n\nboxplot(control)$out #처리 이전 이후 둘다 outlier는 없는 것으로 보인다. \n\n\n\n\nnumeric(0)\n\nboxplot(control)$stats\n\n      [,1] [,2]\n[1,] 70.50 73.0\n[2,] 77.60 77.4\n[3,] 80.65 80.7\n[4,] 86.00 84.7\n[5,] 91.80 89.6\n\n# Skewness\nskewness(control$before) # skewed to the right\n\n[1] 0.2738095\n\nskewness(control$after) # 거의 Skewed 되어지지 않음 (매우 미세하게 skewed to the right)\n\n[1] 0.09589041\n\n# Letter Value Display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\nlvd3=(lsum(control$before,6))\nlvd4=(lsum(control$after,6))\nlvd3\n\n  letter depth lower    mid upper spread\n1      M  13.5 80.65 80.650 80.65   0.00\n2      H   7.0 77.60 81.800 86.00   8.40\n3      E   4.0 75.10 81.900 88.70  13.60\n4      D   2.5 73.15 81.175 89.20  16.05\n5      C   1.5 71.40 81.000 90.60  19.20\n6      B   1.0 70.50 81.150 91.80  21.30\n\nlvd4\n\n  letter depth lower    mid upper spread\n1      M  13.5 80.70 80.700 80.70   0.00\n2      H   7.0 77.40 81.050 84.70   7.30\n3      E   4.0 75.40 81.050 86.70  11.30\n4      D   2.5 74.30 81.275 88.25  13.95\n5      C   1.5 73.25 81.100 88.95  15.70\n6      B   1.0 73.00 81.300 89.60  16.60\n\n# H-Spread\nlsum(control$before)[2,5]-lsum(control$before)[2,3]\n\n[1] 8.4\n\nlsum(control$after)[2,5]-lsum(control$after)[2,3] # 처치 이후 spread의 크기가 더 줄어들었음 \n\n[1] 7.3\n\n# Kurtosis (E-spread / H-spread - 1.705)\n(lvd3[3,5]-lvd3[3,3])/(lvd3[2,5]-lvd3[2,3])-1.705 # less peaked than normal\n\n[1] -0.08595238\n\n(lvd4[3,5]-lvd4[3,3])/(lvd4[2,5]-lvd4[2,3])-1.705 # less peaked than normal (Kurtosis값은 after집단에서 더 작게 나타남)\n\n[1] -0.1570548\n\n# 정규성\nqqnorm(control$before, ylab=\"before quantiles\",main=\"before\");qqline(control$before, col='red',lty=2) # 점들이 완벽히 qqline을 따르기 보다는 곡선의 형태를 이루고 있는 것을 확인할 수 있다.\nfiv3=fivenum(control$before)\n(pseudosigma3 = (fiv3[4]-fiv3[2])/1.34)\n\n[1] 6.268657\n\nsd(control$before) # 약 0.5정도의 차이\n\n[1] 5.70706\n\nabline(fiv3[3],pseudosigma3,col=\"blue\",lty=2)\nlegend(x = -2, y = 92, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 약간차이를 보임\n\n\n\nqqnorm(control$after, ylab=\"after quantiles\", main=\"after\");qqline(control$after, col='red',lty=2) # 처치 이전에 비해서 처치 이후의 데이터들이 직선을 따르기 때문에 정규분포를 조금 더 따르고 있다.  \nfiv4=fivenum(control$after)\n(pseudosigma4 = (fiv4[4]-fiv4[2])/1.34)\n\n[1] 5.447761\n\nsd(control$after) # 약 0.7정도의 차이\n\n[1] 4.744253\n\nabline(fiv4[3],pseudosigma4,col=\"blue\",lty=2)\nlegend(x = -2, y = 88, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 거의 차이가 없음\n\n\n\n# Paired T-test\n# H0: 처지 이전의 집단과 처치 이후의 집단의 몸무게 차이는 없다. H1: 처치 이후의 집단의 몸무게가 처지 이전의 집단보다 더 무겁다.\ncontrol_test<-t.test(control$after,control$before, paired=TRUE, alternative=\"greater\",conf.level=0.05)\ncontrol_test # do not reject null hypothesis.\n\n\n    Paired t-test\n\ndata:  control$after and control$before\nt = -0.28723, df = 25, p-value = 0.6118\nalternative hypothesis: true mean difference is greater than 0\n5 percent confidence interval:\n 2.226168      Inf\nsample estimates:\nmean difference \n          -0.45 \n\n\n마지막으로 Family therapy에 대한 분석이다.\n\nsummary(family)\n\n     before          after       \n Min.   :73.40   Min.   : 75.20  \n 1st Qu.:80.50   1st Qu.: 90.70  \n Median :83.30   Median : 92.50  \n Mean   :83.23   Mean   : 90.49  \n 3rd Qu.:86.00   3rd Qu.: 95.20  \n Max.   :94.20   Max.   :101.60  \n\nmean(family$after)-mean(family$before)\n\n[1] 7.264706\n\nmedian(family$after)-median(family$before)\n\n[1] 9.2\n\n# Family Therapy의 경우 실험 시행하기 이전 집단과 실험 이후 집단의 평균은 7.2kg정도 차이를 보이며 중위값은 9.2kg정도 차이가 나타난다.\n# Cognitive behavioral treatment에서 보였던 차이보다 적은 차이를 보이고 있는 것을 확인할 수 있다.\n\nfivenum(family$before)\n\n[1] 73.4 80.5 83.3 86.0 94.2\n\nfivenum(family$after)\n\n[1]  75.2  90.7  92.5  95.2 101.6\n\n# Stem and leaf Plot을 통해서 두 집단의 분포를 비교해 보았는데 처치 이전 집단의 경우  stem 8*을 기준으로 대칭적인 종모양을 보이는데 비해 처치 이후의 집단의 경우 stem 9*이 가장 많고 대칭적이지 않으며 중간에 빈 부분이 보인다.\nstem.leaf.backback(family$before,family$after,rule.line = \"Sturge\")\n\n_____________________________________\n  1 | 2: represents 12, leaf unit: 1 \n    family$before       family$after\n_____________________________________\n   1          3|  7* |               \n   4        976|  7. |5667       4   \n  (7)   3332210|  8* |               \n   6      97666|  8. |               \n   1          4|  9* |01112344  (8)  \n               |  9. |558        5   \n               | 10* |01         2   \n               | 10. |               \n               | 11* |               \n_____________________________________\nn:           17       17         \n_____________________________________\n\n# Boxplot\nboxplot(family,main=\"Boxplot of before and after family treatment\")\n\n\n\nboxplot(family)$out #처리 이후의 집단에서 4개의 outlier 확인됨. (아까 Stem and leaf에서 확인 한 부분 재확인)\n\n\n\n\n[1] 76.7 76.8 75.2 77.8\n\nboxplot(family)$stats # lower fence=lower hinge\n\n     [,1]  [,2]\n[1,] 73.4  90.7\n[2,] 80.5  90.7\n[3,] 83.3  92.5\n[4,] 86.0  95.2\n[5,] 94.2 101.6\n\n# Skewness\nskewness(family$before) # skewed to the left (but 매우 미세함)\n\n[1] -0.01818182\n\nskewness(family$after) # skewed to the right\n\n[1] 0.2\n\n# Letter Value Display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\nlvd5=(lsum(family$before,6))\nlvd6=(lsum(family$after,6))\nlvd5\n\n  letter depth lower   mid upper spread\n1      M   9.0 83.30 83.30 83.30    0.0\n2      H   5.0 80.50 83.25 86.00    5.5\n3      E   3.0 77.60 82.45 87.30    9.7\n4      D   2.0 76.90 83.40 89.90   13.0\n5      C   1.5 75.15 83.60 92.05   16.9\n6      B   1.0 73.40 83.80 94.20   20.8\n\nlvd6\n\n  letter depth lower   mid  upper spread\n1      M   9.0 92.50 92.50  92.50    0.0\n2      H   5.0 90.70 92.95  95.20    4.5\n3      E   3.0 76.80 87.40  98.00   21.2\n4      D   2.0 76.70 88.50 100.30   23.6\n5      C   1.5 75.95 88.45 100.95   25.0\n6      B   1.0 75.20 88.40 101.60   26.4\n\n# H-Spread\nlvd5[2,5]-lvd5[2,3]\n\n[1] 5.5\n\nlvd6[2,5]-lvd6[2,3] # 처치 이전이후의 차이가 1정도로 크지 않음.\n\n[1] 4.5\n\n# Kurtosis (E-spread / H-spread - 1.705)\n(lvd5[3,5]-lvd5[3,3])/(lvd5[2,5]-lvd5[2,3])-1.705 # 정규분포와 거의 차이가 없음\n\n[1] 0.05863636\n\n(lvd6[3,5]-lvd6[3,3])/(lvd6[2,5]-lvd6[2,3])-1.705 # more peaked than normal (Kurtosis값은 after집단에서 더 작게 나타남) \n\n[1] 3.006111\n\n# 다만 after집단의 분포가 17개의 자료중 4개의 outlier로 인해서 왜곡되었을 가능성이 있다.\n\n# 정규성\nqqnorm(family$before, ylab=\"before quantiles\",main=\"before\");qqline(family$before, col='red',lty=2) # 점들이 직선을 잘 따르고 있는 것으로 보인다. \nfiv5=fivenum(family$before)\n(pseudosigma5 = (fiv5[4]-fiv5[2])/1.34)\n\n[1] 4.104478\n\nsd(family$before) # 약 0.9정도의 차이\n\n[1] 5.016693\n\nabline(fiv5[3],pseudosigma5,col=\"blue\",lty=2)\nlegend(x = -1.8, y = 92, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 거의 차이가 없음\n\n\n\nqqnorm(family$after, ylab=\"after quantiles\", main=\"after\");qqline(family$after, col='red',lty=2) # 이상치 4개를 제외하면 나머지들이 직선을 잘 따르는 것으로 보이나 표본이 작아 신뢰성이 낮다.   \nfiv6=fivenum(family$after)\n(pseudosigma6 = (fiv6[4]-fiv6[2])/1.34)\n\n[1] 3.358209\n\nsd(family$after) # 약 5 정도의 차이 - 매우 큼\n\n[1] 8.475072\n\nabline(fiv6[3],pseudosigma6,col=\"blue\",lty=2)\nlegend(x = -1.8, y = 99, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 거의 차이가 없음 \n\n\n\n# Paired T-test\n# H0: family$after-family$before=0 H1:family$after-family$before>0\nfamily_test<-t.test(family$after,family$before, paired=TRUE, alternative=\"greater\",conf.level=0.05)\nfamily_test # Reject Null Hypothesis. The weight of treated group is larger than the group without treatment.\n\n\n    Paired t-test\n\ndata:  family$after and family$before\nt = 4.1849, df = 16, p-value = 0.0003501\nalternative hypothesis: true mean difference is greater than 0\n5 percent confidence interval:\n 10.29544      Inf\nsample estimates:\nmean difference \n       7.264706 \n\n\n세집단의 차이에 대해 일원분산분석을 진행하고자 한다.\n\ncbt_diff=cbt$after-cbt$before\ncon_diff=control$after-control$before\nfam_diff=family$after-family$before\n\nboxplot(cbt_diff,con_diff,fam_diff,names=c(\"Cognitive\",\"Control\",\"Family\"))$out #Cognitive Behavior Treament 7개의 outlier\n\n[1] 14.9 17.1 11.7 20.9 -9.1 12.6 15.4\n\nnaming=c(rep(\"cbt\",length(cbt_diff)),rep(\"control\",length(con_diff)),rep(\"family\",length(fam_diff)))\nvalue=c(cbt_diff,con_diff,fam_diff)\ndiffs=data.frame(naming,value)\n\naov1=aov(value~naming,data=diffs)\nsummary(aov1)\n\n            Df Sum Sq Mean Sq F value Pr(>F)   \nnaming       2    615  307.32   5.422 0.0065 **\nResiduals   69   3911   56.68                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#H0: 각 집단간 평균은 차이가 없다. #H1: 각 집단간 평균은 차이가 없지 않다.\n# Reject Null Hypothesis. 신뢰수준 95% 수준에서 사춘기 여성의 신경성 식욕 감퇴증의 각 치료 방법(Cognitive behavior, control, family treatment)을 적용할 경우 각 집단간 몸무게에서 차이를 보인다. \n\nbartlett.test(value~naming, data = diffs) #H0: 오차의 등분산성, 유의수준 10%에서 Select Null Hypothesis. (오차의 등분산성 만족)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  value by naming\nBartlett's K-squared = 0.30428, df = 2, p-value = 0.8589\n\ntapply(value,naming,mean)  \n\n      cbt   control    family \n 3.006897 -0.450000  7.264706 \n\nTukeyHSD(aov1) # Tukey multiple comparisons of means 95% family-wise\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = value ~ naming, data = diffs)\n\n$naming\n                    diff       lwr       upr     p adj\ncontrol-cbt    -3.456897 -8.327276  1.413483 0.2124428\nfamily-cbt      4.257809 -1.250554  9.766173 0.1607461\nfamily-control  7.714706  2.090124 13.339288 0.0045127\n\n# install.packages(\"agricolae\")\nlibrary(agricolae)\n\n\nAttaching package: 'agricolae'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    skewness\n\n\n\n\nHSD.test(aov1, \"naming\", group=TRUE,console=TRUE)\n\n\nStudy: aov1 ~ \"naming\"\n\nHSD Test for value \n\nMean Square Error:  56.67743 \n\nnaming,  means\n\n            value      std  r   Min  Max\ncbt      3.006897 7.308504 29  -9.1 20.9\ncontrol -0.450000 7.988705 26 -12.2 15.9\nfamily   7.264706 7.157421 17  -5.3 21.5\n\nAlpha: 0.05 ; DF Error: 69 \nCritical Value of Studentized Range: 3.387483 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n            value groups\nfamily   7.264706      a\ncbt      3.006897     ab\ncontrol -0.450000      b\n\nLSD.test(aov1, \"naming\", group=TRUE,console=TRUE)\n\n\nStudy: aov1 ~ \"naming\"\n\nLSD t Test for value \n\nMean Square Error:  56.67743 \n\nnaming,  means and individual ( 95 %) CI\n\n            value      std  r       LCL       UCL   Min  Max\ncbt      3.006897 7.308504 29  0.217970  5.795823  -9.1 20.9\ncontrol -0.450000 7.988705 26 -3.395435  2.495435 -12.2 15.9\nfamily   7.264706 7.157421 17  3.622105 10.907307  -5.3 21.5\n\nAlpha: 0.05 ; DF Error: 69\nCritical Value of t: 1.994945 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n            value groups\nfamily   7.264706      a\ncbt      3.006897     ab\ncontrol -0.450000      b\n\n\n통제집단에 비해서 cognitive behavior treatemnt는 3.46kg정도 높음. cognitive behavior treatment 집단에 비해서 family treatemnt 집단의 경우 4.26kg 정도 높음. 따라서 가장 효과가 좋은 순서대로 family treatment, cognitive behavior treatment, control 순이다.\n그러나 cognitive behavior treatment집단이 다른 집단에 비해서 크거나 작다라고 보기에는 HSD, LSD test를 통해서 확인할 경우 신뢰구간이 겹쳐 다소 모호한 부분이 있다. 실제 Boxplot을 통해 확인할 경우 outlier들이 넓게 퍼져있어서 cognitive behavior treatment집단의 몸무게가 유의수준 5%수준에서 두 집단의 몸무게와 완전히 다르다고 보기 어렵다.\n\n# 이원분산분석 \n\nlibrary(reshape2)\n\ncontrol2=melt(control)\n\nNo id variables; using all as measure variables\n\ncontrol2=cbind(rep(\"control\",dim(control2)[1]),control2)\nnames(control2)=c(\"treatment\",\"ba\",\"value\")\n\ncbt2=melt(cbt)\n\nNo id variables; using all as measure variables\n\ncbt2=cbind(rep(\"cognitive\",dim(cbt2)[1]),cbt2)\nnames(cbt2)=c(\"treatment\",\"ba\",\"value\")\n\nfamily2=melt(family)\n\nNo id variables; using all as measure variables\n\nfamily2=cbind(rep(\"family\",dim(family2)[1]),family2)\nnames(family2)=c(\"treatment\",\"ba\",\"value\")\nfamily2\n\n   treatment     ba value\n1     family before  83.8\n2     family before  83.3\n3     family before  86.0\n4     family before  82.5\n5     family before  86.7\n6     family before  79.6\n7     family before  76.9\n8     family before  94.2\n9     family before  73.4\n10    family before  80.5\n11    family before  81.6\n12    family before  82.1\n13    family before  77.6\n14    family before  83.5\n15    family before  89.9\n16    family before  86.0\n17    family before  87.3\n18    family  after  95.2\n19    family  after  94.3\n20    family  after  91.5\n21    family  after  91.9\n22    family  after 100.3\n23    family  after  76.7\n24    family  after  76.8\n25    family  after 101.6\n26    family  after  94.9\n27    family  after  75.2\n28    family  after  77.8\n29    family  after  95.5\n30    family  after  90.7\n31    family  after  92.5\n32    family  after  93.8\n33    family  after  91.7\n34    family  after  98.0\n\naov2=rbind(control2,cbt2,family2)\naov3=aov(value~treatment+ba,data=aov2) \nsummary(aov3)\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ntreatment     2    644   322.1   7.713 0.000664 ***\nba            1    275   275.0   6.585 0.011335 *  \nResiduals   140   5847    41.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(doBy)\nsummaryBy(value ~ treatment, data=aov2, FUN = c(mean, sd, min, max))\n\n  treatment value.mean value.sd value.min value.max\n1 cognitive   84.19310 6.935337      70.0     103.6\n2   control   81.33269 5.201045      70.5      91.8\n3    family   86.86176 7.785963      73.4     101.6\n\nHSD.test(aov3, \"treatment\", group=TRUE,console=TRUE) # family cognitive standard 순이지만 cognitive behavior treatment는 a그룹 b그룹 모두에 속하므로 해석에 유의해야 한다. \n\n\nStudy: aov3 ~ \"treatment\"\n\nHSD Test for value \n\nMean Square Error:  41.76232 \n\ntreatment,  means\n\n             value      std  r  Min   Max\ncognitive 84.19310 6.935337 58 70.0 103.6\ncontrol   81.33269 5.201045 52 70.5  91.8\nfamily    86.86176 7.785963 34 73.4 101.6\n\nAlpha: 0.05 ; DF Error: 140 \nCritical Value of Studentized Range: 3.350136 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n             value groups\nfamily    86.86176      a\ncognitive 84.19310     ab\ncontrol   81.33269      b\n\nHSD.test(aov3, \"ba\", group=TRUE,console=TRUE) # 전반적으로 더 높다 treatment 취한 그룹에서 \n\n\nStudy: aov3 ~ \"ba\"\n\nHSD Test for value \n\nMean Square Error:  41.76232 \n\nba,  means\n\n          value      std  r  Min   Max\nafter  85.17222 8.035173 72 71.3 103.6\nbefore 82.40833 5.182466 72 70.0  94.9\n\nAlpha: 0.05 ; DF Error: 140 \nCritical Value of Studentized Range: 2.795976 \n\nMinimun Significant Difference: 2.129411 \n\nTreatments with the same letter are not significantly different.\n\n          value groups\nafter  85.17222      a\nbefore 82.40833      b\n\n# Interaction Plot\n\nid=(1:dim(aov2)[1])\naov2=cbind(id,aov2)\naov2\n\n     id treatment     ba value\n1     1   control before  80.7\n2     2   control before  89.4\n3     3   control before  91.8\n4     4   control before  74.0\n5     5   control before  78.1\n6     6   control before  88.3\n7     7   control before  87.3\n8     8   control before  75.1\n9     9   control before  80.6\n10   10   control before  78.4\n11   11   control before  77.6\n12   12   control before  88.7\n13   13   control before  81.3\n14   14   control before  78.1\n15   15   control before  70.5\n16   16   control before  77.3\n17   17   control before  85.2\n18   18   control before  86.0\n19   19   control before  84.1\n20   20   control before  79.7\n21   21   control before  85.5\n22   22   control before  84.4\n23   23   control before  79.6\n24   24   control before  77.5\n25   25   control before  72.3\n26   26   control before  89.0\n27   27   control  after  80.2\n28   28   control  after  80.1\n29   29   control  after  86.4\n30   30   control  after  86.3\n31   31   control  after  76.1\n32   32   control  after  78.1\n33   33   control  after  75.1\n34   34   control  after  86.7\n35   35   control  after  73.5\n36   36   control  after  84.6\n37   37   control  after  77.4\n38   38   control  after  79.5\n39   39   control  after  89.6\n40   40   control  after  81.4\n41   41   control  after  81.8\n42   42   control  after  77.3\n43   43   control  after  84.2\n44   44   control  after  75.4\n45   45   control  after  79.5\n46   46   control  after  73.0\n47   47   control  after  88.3\n48   48   control  after  84.7\n49   49   control  after  81.4\n50   50   control  after  81.2\n51   51   control  after  88.2\n52   52   control  after  78.8\n53   53 cognitive before  80.5\n54   54 cognitive before  84.9\n55   55 cognitive before  81.5\n56   56 cognitive before  82.6\n57   57 cognitive before  79.9\n58   58 cognitive before  88.7\n59   59 cognitive before  94.9\n60   60 cognitive before  76.3\n61   61 cognitive before  81.0\n62   62 cognitive before  80.5\n63   63 cognitive before  85.0\n64   64 cognitive before  89.2\n65   65 cognitive before  81.3\n66   66 cognitive before  76.5\n67   67 cognitive before  70.0\n68   68 cognitive before  80.4\n69   69 cognitive before  83.3\n70   70 cognitive before  83.0\n71   71 cognitive before  87.7\n72   72 cognitive before  84.2\n73   73 cognitive before  86.4\n74   74 cognitive before  76.5\n75   75 cognitive before  80.2\n76   76 cognitive before  87.8\n77   77 cognitive before  83.3\n78   78 cognitive before  79.7\n79   79 cognitive before  84.5\n80   80 cognitive before  80.8\n81   81 cognitive before  87.4\n82   82 cognitive  after  82.2\n83   83 cognitive  after  85.6\n84   84 cognitive  after  81.4\n85   85 cognitive  after  81.9\n86   86 cognitive  after  76.4\n87   87 cognitive  after 103.6\n88   88 cognitive  after  98.4\n89   89 cognitive  after  93.4\n90   90 cognitive  after  73.4\n91   91 cognitive  after  82.1\n92   92 cognitive  after  96.7\n93   93 cognitive  after  95.3\n94   94 cognitive  after  82.4\n95   95 cognitive  after  72.5\n96   96 cognitive  after  90.9\n97   97 cognitive  after  71.3\n98   98 cognitive  after  85.4\n99   99 cognitive  after  81.6\n100 100 cognitive  after  89.1\n101 101 cognitive  after  83.9\n102 102 cognitive  after  82.7\n103 103 cognitive  after  75.7\n104 104 cognitive  after  82.6\n105 105 cognitive  after 100.4\n106 106 cognitive  after  85.2\n107 107 cognitive  after  83.6\n108 108 cognitive  after  84.6\n109 109 cognitive  after  96.2\n110 110 cognitive  after  86.7\n111 111    family before  83.8\n112 112    family before  83.3\n113 113    family before  86.0\n114 114    family before  82.5\n115 115    family before  86.7\n116 116    family before  79.6\n117 117    family before  76.9\n118 118    family before  94.2\n119 119    family before  73.4\n120 120    family before  80.5\n121 121    family before  81.6\n122 122    family before  82.1\n123 123    family before  77.6\n124 124    family before  83.5\n125 125    family before  89.9\n126 126    family before  86.0\n127 127    family before  87.3\n128 128    family  after  95.2\n129 129    family  after  94.3\n130 130    family  after  91.5\n131 131    family  after  91.9\n132 132    family  after 100.3\n133 133    family  after  76.7\n134 134    family  after  76.8\n135 135    family  after 101.6\n136 136    family  after  94.9\n137 137    family  after  75.2\n138 138    family  after  77.8\n139 139    family  after  95.5\n140 140    family  after  90.7\n141 141    family  after  92.5\n142 142    family  after  93.8\n143 143    family  after  91.7\n144 144    family  after  98.0\n\naov2$treatment=as.factor(aov2$treatment)\naov2$ba=as.factor(aov2$ba)\naov2$id=as.factor(aov2$id)\nstr(aov2)\n\n'data.frame':   144 obs. of  4 variables:\n $ id       : Factor w/ 144 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ treatment: Factor w/ 3 levels \"cognitive\",\"control\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ ba       : Factor w/ 2 levels \"before\",\"after\": 1 1 1 1 1 1 1 1 1 1 ...\n $ value    : num  80.7 89.4 91.8 74 78.1 88.3 87.3 75.1 80.6 78.4 ...\n\naov4=aov(value~treatment*ba,data=aov2)\nsummary(aov4)\n\n              Df Sum Sq Mean Sq F value   Pr(>F)    \ntreatment      2    644   322.1   8.025 0.000505 ***\nba             1    275   275.0   6.851 0.009847 ** \ntreatment:ba   2    307   153.7   3.828 0.024097 *  \nResiduals    138   5539    40.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ninteraction.plot(aov2$ba, aov2$treatment, aov2$value,main=\"Interaction plot\")\n\n\n\nHSD.test(aov4, \"treatment\", group=T,console=TRUE)\n\n\nStudy: aov4 ~ \"treatment\"\n\nHSD Test for value \n\nMean Square Error:  40.1406 \n\ntreatment,  means\n\n             value      std  r  Min   Max\ncognitive 84.19310 6.935337 58 70.0 103.6\ncontrol   81.33269 5.201045 52 70.5  91.8\nfamily    86.86176 7.785963 34 73.4 101.6\n\nAlpha: 0.05 ; DF Error: 138 \nCritical Value of Studentized Range: 3.350657 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n             value groups\nfamily    86.86176      a\ncognitive 84.19310     ab\ncontrol   81.33269      b\n\nHSD.test(aov4, \"ba\", group=T,console=TRUE)\n\n\nStudy: aov4 ~ \"ba\"\n\nHSD Test for value \n\nMean Square Error:  40.1406 \n\nba,  means\n\n          value      std  r  Min   Max\nafter  85.17222 8.035173 72 71.3 103.6\nbefore 82.40833 5.182466 72 70.0  94.9\n\nAlpha: 0.05 ; DF Error: 138 \nCritical Value of Studentized Range: 2.796329 \n\nMinimun Significant Difference: 2.087921 \n\nTreatments with the same letter are not significantly different.\n\n          value groups\nafter  85.17222      a\nbefore 82.40833      b\n\n\n\n\n2.\n근육 피로도의 지표로 쓰이는 혈중젖산농도의 참값과 계측기에서의 값의 자료이다. 계측기가 정확하다면 두 자료의 직선식은 원점을 지나는 45도 직선이어야 한다. 분석하여라.\n\ntrue_val=c(rep(1,4),rep(3,5),rep(5,3),rep(10,4),rep(15,4))\nest_val=c(1.1,0.7,1.8,0.4,3.0,1.4,4.9,4.4,4.5,7.3,8.2,6.2,12,13.1,12.6,13.2,18.7,19.7,17.4,17.1)\n\nlactic=data.frame(true_val,est_val)\nlactic\n\n   true_val est_val\n1         1     1.1\n2         1     0.7\n3         1     1.8\n4         1     0.4\n5         3     3.0\n6         3     1.4\n7         3     4.9\n8         3     4.4\n9         3     4.5\n10        5     7.3\n11        5     8.2\n12        5     6.2\n13       10    12.0\n14       10    13.1\n15       10    12.6\n16       10    13.2\n17       15    18.7\n18       15    19.7\n19       15    17.4\n20       15    17.1\n\ndim(lactic)\n\n[1] 20  2\n\nsummary(lactic) #평균 실제 젖산값은 6.7, 평균 측정 젖산값은 8.385로 실제 값에 비해서 높게 측정되었다.\n\n    true_val       est_val      \n Min.   : 1.0   Min.   : 0.400  \n 1st Qu.: 3.0   1st Qu.: 2.700  \n Median : 5.0   Median : 6.750  \n Mean   : 6.7   Mean   : 8.385  \n 3rd Qu.:10.0   3rd Qu.:13.125  \n Max.   :15.0   Max.   :19.700  \n\nmedtrue <- as.vector(3)\nmedest <- as.vector(3)\n\nmedtrue[1] <- median(lactic$true_val[1:6]); medest[1] <- median(lactic$est_val[1:6])\nmedtrue[2] <- median(lactic$true_val[7:14]); medest[2] <- median(lactic$est_val[7:14])\nmedtrue[3] <- median(lactic$true_val[15:20]); medest[3] <- median(lactic$est_val[15:20])\nplot(medest~medtrue, type=\"b\",main=\"True value와 Estimation Value 사이의 관계\")   # 다소 Concave 한 관계인 것으로 보인다.\n\n\n\nplot(lactic$est_val~lactic$true_val,main=\"relationship of truevalue and estimatedvalue\")\n(z <- line(x=lactic$true_val,y=lactic$est_val))\n\n\nCall:\nline(x = lactic$true_val, y = lactic$est_val)\n\nCoefficients:\n[1]  -0.9026   1.4053\n\nabline(coef(z))\nresiduals(z)\n\n [1]  0.59736842  0.19736842  1.29736842 -0.10263158 -0.31315789 -1.91315789\n [7]  1.58684211  1.08684211  1.18684211  1.17631579  2.07631579  0.07631579\n[13] -1.15000000 -0.05000000 -0.55000000  0.05000000 -1.47631579 -0.47631579\n[19] -2.77631579 -3.07631579\n\nz.ls <- lm(lactic$est_val ~ lactic$true_val)\nabline(z.ls$coef, lty=2,col=\"red\")\n\nlegend(x = 2, y = 18, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)\n\n\n\n# 잔차제곱합\nsum(residuals(z))\n\n[1] -2.552632\n\nsum(residuals(z)^2)\n\n[1] 37.86429\n\nattach(lactic)\n\nThe following objects are masked _by_ .GlobalEnv:\n\n    est_val, true_val\n\n# 두 자료가 같은 분포에서 온게 맞을까?\nqqplot(true_val,est_val,main=\"qqplot\")\nabline(line(qqplot(true_val,est_val,main=\"qqplot\"))$coef) # 전반적으로 Tukey's robust line estimation 직선위에 잘 존재하는 것으로 보인다.\n\n\n\n# 분포 예측하기\nlibrary(aplpack)\nstem.leaf.backback(true_val,est_val,rule.line=\"Sturges\") # 두 자료 모두 정규분포보다는 Skewed to the right 되어 있는 것을 어느정도 예측할 수 있다. \n\n______________________________________\n  1 | 2: represents 12, leaf unit: 1 \n        true_val      est_val     \n______________________________________\n   9   333331111| 0* |001113444   9   \n  (3)        555| 0. |678        (3)  \n   8        0000| 1* |2233        8   \n   4        5555| 1. |7789        4   \n                | 2* |                \n______________________________________\nn:            20      20          \n______________________________________\n\nskewness(true_val) # skewed to the right \n\n[1] 0.4285714\n\nskewness(est_val) # 실제 자료에 비해서 덜 skewed 되어있다 (skewed to the right)\n\n[1] 0.1906977\n\n# Letter Value Display\nlvd7=(lsum(true_val,6))\nlvd8=(lsum(est_val,6))\nlvd7\n\n  letter depth lower mid upper spread\n1      M  10.5     5 5.0     5      0\n2      H   5.5     3 6.5    10      7\n3      E   3.0     1 8.0    15     14\n4      D   2.0     1 8.0    15     14\n5      C   1.5     1 8.0    15     14\n6      B   1.0     1 8.0    15     14\n\nlvd8\n\n  letter depth lower    mid upper spread\n1      M  10.5  6.75  6.750  6.75   0.00\n2      H   5.5  2.40  7.775 13.15  10.75\n3      E   3.0  1.10  9.250 17.40  16.30\n4      D   2.0  0.70  9.700 18.70  18.00\n5      C   1.5  0.55  9.875 19.20  18.65\n6      B   1.0  0.40 10.050 19.70  19.30\n\n# H-Spread\nlvd7[2,5]-lvd7[2,3]\n\n[1] 7\n\nlvd8[2,5]-lvd8[2,3] # H Spread의 경우 예측값에서 더 길게 나타나고 있다.\n\n[1] 10.75\n\n# Kurtosis (E-spread / H-spread - 1.705)\n(lvd7[3,5]-lvd7[3,3])/(lvd7[2,5]-lvd7[2,3])-1.705 # more peaked than normal\n\n[1] 0.295\n\n(lvd8[3,5]-lvd8[3,3])/(lvd8[2,5]-lvd8[2,3])-1.705 # less peaked than normal \n\n[1] -0.1887209\n\n# 두 데이터를 합치고 해당 데이터들이 동일한 분포를 따르고 있는지 확인하기 \nlactic2=c(true_val,est_val)\nlactic2\n\n [1]  1.0  1.0  1.0  1.0  3.0  3.0  3.0  3.0  3.0  5.0  5.0  5.0 10.0 10.0 10.0\n[16] 10.0 15.0 15.0 15.0 15.0  1.1  0.7  1.8  0.4  3.0  1.4  4.9  4.4  4.5  7.3\n[31]  8.2  6.2 12.0 13.1 12.6 13.2 18.7 19.7 17.4 17.1\n\nqqnorm(lactic2);qqline(lactic2,col=\"red\",lty=2) \n\n\n\n#점들이 Tukey's robust line estimation위를 따르기 보다는 역s자 형태의 곡선의 형태를 띄고 있는 것을 확인할 수 있다.\n#허명회교수님의 책 113pg에 보면 혼합정규분포에서의 모의생성자료가 역 S자 형태를 띄고 있는데 현재 Plot도 그러한 것으로 보아 실제 데이터와 측정치 데이터가 서로 다른 분포를 따르고 있지 않을까 추측할 수 있다.\n#또한, 꼬리가 짧은 분포일때의 형태와 비슷하게 왼쪽 끝 자료들이 직선을 벗어나있다. \n\n# 두 데이터는 서로 다른 분포에서 왔다고 추정할 수 있다. 그러나 눈대중으로 판단하는 것은 한계가 있기 때문에 이 판단을 재확인하기 위해서는 Tuckey의 Mean-Difference Plot을 활용해야한다. \nqqplot(x=true_val,y=est_val,xlim=c(min(true_val,est_val),max(true_val,est_val)),\n       ylim=c(min(true_val,est_val),max(true_val,est_val))\n       ,main=\"QQ_plot of true value and estimated value\")\nabline(0,1,lty=2,col=\"darkgreen\",lwd=2) #초반의 일부 데이터를 제외하면 대부분이 주대각선 위에 존재하고 있다. \n\n\n\n# 따라서 두 Estimation의 평균은 같다고 보기 힘들다.\n\nqq.x <- qqplot(x=true_val,y=est_val)$x\nqq.y <- qqplot(x=true_val,y=est_val)$y\n\n\n\nplot((qq.x+qq.y)/2, qq.y-qq.x, main=\"Tukey mean difference plot\", \n     ylab=\"est_val - true_val\", xlab=\"mean\")\nabline(0,0)\n\n\n\n# mean difference plot을 통해서 확인하더라도 몇몇자료를 제외하면 대부분의 자료가 x축보다 위에 존재하는 것으로 보인다.\n\nqqplot을 통해서 처음 확인하였을 때 자료들이 전부 직선위에 존재하는 것 처럼 보였으나 실제로 자료들을 합쳐서 qqplot을 그릴경우 역s자 형태로 나타나서 동일한 분포에서 나온 데이터가 아닐 수도 있음을 확인할 수 있었고 mean-difference plot을 통해서 이를 재 확인할 수 있었다. 따라서 어떤 자료를 측정하는 경우 계측기가 정확하다면 두 자료는 동일한 분포를 따르는 것이 맞겠지만, 계측기가 정확하지 않기 때문에 두 자료들이 서로 다른 분포를 따르고 있다는 것을 확인할 수 있었다. 시각적으로 예측해보는 것은 항상 정확하지 않기 때문에 지금처럼 검증이 필요하다고 생각한다.\n\nattach(lactic)\n\nThe following objects are masked _by_ .GlobalEnv:\n\n    est_val, true_val\n\n\nThe following objects are masked from lactic (pos = 3):\n\n    est_val, true_val\n\ncor.test(true_val,est_val) #매우 높은 상관관계가 존재함.\n\n\n    Pearson's product-moment correlation\n\ndata:  true_val and est_val\nt = 26.107, df = 18, p-value = 9.278e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9668337 0.9949756\nsample estimates:\n      cor \n0.9870507 \n\nplot(est_val~true_val)\n(z <- line(x=true_val,y=est_val))\n\n\nCall:\nline(x = true_val, y = est_val)\n\nCoefficients:\n[1]  -0.9026   1.4053\n\nabline(coef(z),lty=2,col=\"red\")\n\n\n\nplot(residuals(z) ~ fitted(z), main = \"Residual plot by rline\")\nabline(0,0) # 잔차들이 우하향하는 듯하게 보인다.\n\n\n\nplot(sqrt(est_val)~(true_val))\n(z1 <- line(x=(true_val),y=sqrt(est_val)))\n\n\nCall:\nline(x = (true_val), y = sqrt(est_val))\n\nCoefficients:\n[1]  0.9012  0.2676\n\nabline(coef(z1),lty=2,col=\"red\")\n\n\n\nplot(residuals(z1) ~ fitted(z1), main = \"Residual plot by rline\")\nabline(0,0) #다소 완화되었다 그러나 자료들이 곡선의 경향성을 보여준다는 점에서 변환은 부적절하다. \n\n\n\ndetach(lactic)\n\n상관관계가 매우 높게 나타나서, r-line을 활용해 두 자료를 적합시키려고 하였는데 변환을 진행하기전 자료가 더 적합해보인다.\n\n\n3.\n취학아동의 정신 장애 상태와 부모의 사회경제적 지위(1=낮음, 6=높음)에 대한 표이다. 분석하여라.\n\nordered(1:6)\n\n[1] 1 2 3 4 5 6\nLevels: 1 < 2 < 3 < 4 < 5 < 6\n\nsoseco=as.data.frame(matrix(c(64,57,57,72,36,21,94,94,105,141,97,71,58,54,65,77,54,54,46,40,60,94,78,71),nrow=4,ncol=6,byrow=T))\ncolnames(soseco)=ordered(1:6)\nrownames(soseco)=c(\"Well\",\"Mild\",\"Moderate\",\"Impaired\")\n\n# median polish\nsoseco_polished=medpolish(soseco)\n\n1: 219\n2: 202.75\n3: 200\nFinal: 200\n\n# maxiter를 설정하지 않고 분석한 결과 maxiter=3에서 결과 값이 나오는 것을 확인하였다.\nsoseco_polished\n\n\nMedian Polish Results (Dataset: \"soseco\")\n\nOverall: 56.375\n\nRow Effects:\n    Well     Mild Moderate Impaired \n -11.875   39.375   -0.625    0.625 \n\nColumn Effects:\n      1       2       3       4       5       6 \n  0.250  -1.750   9.250  32.250  -0.250 -12.625 \n\nResiduals:\n              1      2     3      4     5       6\nWell      19.25  14.25  3.25  -4.75 -8.25 -10.875\nMild      -2.00   0.00  0.00  13.00  1.50 -12.125\nModerate   2.00   0.00  0.00 -11.00 -1.50  10.875\nImpaired -11.25 -15.25 -6.25   4.75 21.25  26.625\n\nplot(soseco_polished)\n#x축과 y축위에 데이터가 많이 있는 것으로 보인다. outlier로 보이는 4개의 점을 제외하면 전반적으로 residual들이 안정적이다.\nplot(soseco_polished)\nabline(0,1) #나름 점들이 경향성을 가지고 있는 것으로 보인다.\n\nz7=lm(as.vector(soseco_polished$residuals) ~ \n     as.vector(outer(soseco_polished$row,soseco_polished$col, \"*\")/soseco_polished$overall))[1]\nabline(z7,col=\"red\")\n\n\n\n# 기울기가 0.5665495이므로 1과는 다르므로 log변환의 필요성이 떨어져 보인다.\n# 직선을 그리기 어렵고 의미있는 패턴이 보이지 않기 때문에 변환이 요구되어보이지는 않는다. \n\nboxplot(soseco_polished$residuals) # Boxplot그릴경우 Outlier가 확인되어지지는 않는다.\n\n\n\n# 행 효과 크기 순으로 재정렬한 잔차표\nround(soseco_polished$residuals[order(soseco_polished$row),],1)\n\n             1     2    3     4    5     6\nWell      19.2  14.2  3.2  -4.8 -8.2 -10.9\nModerate   2.0   0.0  0.0 -11.0 -1.5  10.9\nImpaired -11.2 -15.2 -6.2   4.8 21.2  26.6\nMild      -2.0   0.0  0.0  13.0  1.5 -12.1\n\n# Check Decomposition\ndecomposed=(soseco_polished$overall + outer(soseco_polished$row,soseco_polished$col, \"+\") + soseco_polished$residuals)\ndecomposed\n\n          1  2   3   4  5  6\nWell     64 57  57  72 36 21\nMild     94 94 105 141 97 71\nModerate 58 54  65  77 54 54\nImpaired 46 40  60  94 78 71\n\nall(decomposed==soseco) \n\n[1] TRUE\n\n# 가법성 모형이 확인되어지고 있다.\n\n# Comparison Values\nround(outer(soseco_polished$row,soseco_polished$col, \"*\")/soseco_polished$overall,2)\n\n             1     2     3     4     5     6\nWell     -0.05  0.37 -1.95 -6.79  0.05  2.66\nMild      0.17 -1.22  6.46 22.52 -0.17 -8.82\nModerate  0.00  0.02 -0.10 -0.36  0.00  0.14\nImpaired  0.00 -0.02  0.10  0.36  0.00 -0.14\n\n# Comparison value 기준으로 0값이 총 4개 확인됨. 그런데 더 많은 값들이 Additivity Plot의 x축과 y축 근처에 존재하는 것으로 보아 몇몇 피팅에서 벗어난 값을 제외하면 대부분 0근처에서 존재하고 있는 것이 확인된다. \n\n#stem and leaf plot\nstem(soseco_polished$residuals) # stem 0이 가장 높음 (낮은 잔차인 stem -1의 잔차들이 다소 많이 나타나고 있음)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -1 | 52111\n  -0 | 86522\n   0 | 00002235\n   1 | 1349\n   2 | 17\n\n# Row Effect \nbarplot(soseco_polished$row,ylim=c(-15,40), main=\"정신상태의 효과\")\n\n\n\nsoseco_polished$row\n\n    Well     Mild Moderate Impaired \n -11.875   39.375   -0.625    0.625 \n\n# Row Effect에 대해서 먼저 분석을 진행해 보았다. 정신상태가 Mild인 경우 부모님의 사회경제적 지위가 가장 높아졌고, Well인경우 부모님의 사회경제적 지위가 낮은 것으로 보인다. \n\n# Column Effect\nbarplot(soseco_polished$col, ylim=c(-20,40) , main=\"부모님 사회경제적 지위의 효과\")\n\n\n\nsoseco_polished$col\n\n      1       2       3       4       5       6 \n  0.250  -1.750   9.250  32.250  -0.250 -12.625 \n\n# Column Effect에 대해서 분석을 진행해 보았다. 부모님의 사회경제적 지위가 4인 경우에 정신상태가 나빠질 가능성이 많은 편이다. 부모님의 사회경제적 지위가 6인경우 정신상태가 다른 집단들에 비해서 긍정적으로 나타나는 것이 확인되어진다. \n\n\npar(mfrow=c(2,2))\nfor (i in 1:4) {barplot(soseco_polished$residuals[i,],main=rownames(soseco)[i])}\n\n\n\n# 정신상태가 Well과 Impaired 된 집단을 기준으로 볼 경우 부모님의 사회경제적 지위가 높을수록 정신상태가 나쁜 Case가 많고, 낮을수록 정신상태가 좋은 Case가 많다. \n\npar(mfrow=c(2,3))\nfor (i in 1:6) {barplot(soseco_polished$residuals[,i],main=names(soseco)[i])}\n\n\n\n# 부모님의 사회경제적 지위가 높을 수록 정신상태가 나쁜 경향성을 보이고, 사회 경제적 지위가 낮을 때 정신상태가 좋게 나타나고 있다.\n\npar(mfrow=c(1,1))\n\n# twoway plot\nlibrary(\"twoway\")\ntwoway(soseco, method=\"median\")\n\n\nMedian polish decomposition (Dataset: \"soseco\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n           1       2       3       4       5       6         roweff \n         + ------- ------- ------- ------- ------- ------- + -------\nWell     |  19.250  14.250   3.250  -4.750  -8.250 -10.875 : -11.875\nMild     |  -2.000   0.000   0.000  13.000   1.500 -12.125 :  39.375\nModerate |   2.000   0.000   0.000 -11.000  -1.500  10.875 :  -0.625\nImpaired | -11.250 -15.250  -6.250   4.750  21.250  26.625 :   0.625\n         + ....... ....... ....... ....... ....... ....... + .......\ncoleff   |   0.250  -1.750   9.250  32.250  -0.250 -12.625 :  56.375\n\nplot(twoway(soseco, method=\"median\"))\n\n\n\n# mosaic plot\nlibrary(ghibli)\npal=ghibli_palette(\"PonyoMedium\",n=6)\npal\n\n\n\nmosaicplot(soseco,color=pal,main=\"mosaic plot of sosioeconomic data\")\n\n\n\n#부모님의 사회 경제적 위치가 높을수록 정신 상태가 안 좋은 사람들의 비율이 올라가는 듯한 경향성이 보이는 듯 하다. \n\n# coded plot\nresid=soseco_polished$residuals\nstandard=as.vector(resid)\nfivenum(standard)\n\n[1] -15.2500  -7.2500   0.0000   7.8125  26.6250\n\nHspread=fivenum(standard)[4]-fivenum(standard)[2]\ninner=c(fivenum(standard)[4]+1.5*Hspread,fivenum(standard)[2]-1.5*Hspread)\nouter=c(fivenum(standard)[4]+3*Hspread,fivenum(standard)[2]-3*Hspread)\n\nresid=as.data.frame(resid)\n# 교과서 기준과 동일하게 \"M\" - Far outside low, \"=\"- below low inner fence(outise), \"-\" - Below lower hinge but within inner fence\n# \".\" - Between Hinges , \"+\"- Above upper hinge, \"#\" - Above High inner fence (outside), \"F\" - Far outside high\nresid_coded=ifelse(resid >= outer[1], \"F\",\n                        ifelse(resid>= inner[1], \"#\",\n                               ifelse(resid >= fivenum(standard)[4],\"+\",\n                                      ifelse(resid >=fivenum(standard)[2] ,\".\",\n                                             ifelse(resid>= inner[2],\"-\",\n                                                    ifelse(resid >= outer[2],\"=\",\"M\"))))))\nresid_coded=as.data.frame(resid_coded)\nresid_coded #위에서 확인하였던 경향성이 다시한번 확인된다 (부모의 사회경제적 위치가 올라갈수록 정신질환을 가지는 경우가 높아짐)\n\n         1 2 3 4 5 6\nWell     + + . . - -\nMild     . . . + . -\nModerate . . . - . +\nImpaired - - . . + +\n\n\n\n\n4.\n자료는 정상적인 쥐, 알록산(alloxan)에 의해 당뇨병이 유발된 쥐, 당뇨병이 유발된 후 인슐린으로 치료한 쥐에서의 알부민양이다. 세 자료의 분포가 같은 지 분석하여라. 자료를 재표현하여 대칭형으로 만들고 분포가 같은 지 다시 분석하여라. [DIABETIC.DAT]\n\ndiabetes=read.delim(\"C:/r/DIABETIC.DAT\",header=F)\nnames(diabetes)=c(\"normal\",\"alloxan\",\"insulin\")\n\ndim(diabetes)\n\n[1] 20  3\n\nsummary(diabetes)\n\n     normal         alloxan          insulin     \n Min.   : 14.0   Min.   : 13.00   Min.   : 18.0  \n 1st Qu.:104.0   1st Qu.: 76.25   1st Qu.: 45.0  \n Median :124.5   Median :139.50   Median : 82.0  \n Mean   :186.1   Mean   :181.83   Mean   :112.9  \n 3rd Qu.:260.2   3rd Qu.:251.00   3rd Qu.:132.0  \n Max.   :655.0   Max.   :499.00   Max.   :465.0  \n                 NA's   :2        NA's   :1      \n\n# 결측치의 확인 \ncolSums(is.na(diabetes)) # alloxan에서 2개 insulin에서 1개\n\n normal alloxan insulin \n      0       2       1 \n\n# Stem and leaf를 활용하여 각 집단의 분포를 비교\nattach(diabetes)\nlibrary(aplpack)\nstem.leaf.backback(normal,alloxan,rule.line=\"Sturges\") \n\n_____________________________________\n  1 | 2: represents 120, leaf unit: 10 \n          normal     alloxan     \n_____________________________________\n   5       86221| 0 |145678      6   \n  (9)  954222111| 1 |0234677    (7)  \n   6         985| 2 |77          5   \n   3           4| 3 |9           3   \n   2           5| 4 |69          2   \n                | 5 |                \n_____________________________________\nHI: 655                              \nn:            20     20          \nNAs:           0     2           \n_____________________________________\n\n# normal data와 alloxan data모두 skewed to the right 되어 있는 자료로 stem 1에서 가장 많은 데이터를 보인다.\nstem.leaf.backback(normal,insulin,rule.line=\"Sturges\")\n\n__________________________________\n  1 | 2: represents 120, leaf unit: 10 \n        normal      insulin   \n__________________________________\n   3       221| 0* |123444    6   \n   5        86| 0. |67789    (5)  \n  (7)  4222111| 1* |0033      8   \n   8        95| 1. |5         4   \n              | 2* |24        3   \n   6       985| 2. |              \n   3         4| 3* |              \n__________________________________\nHI: 455 655         HI: 465       \nn:          20      20        \nNAs:         0      1         \n__________________________________\n\n# normal data의 경우 stem 1*을 중심으로 하고 있으며 stem 2*의 경우 중간에 비어 있는 모습을 확인할 수 있다. (Outlier의 가능성)\n# insulin data의 경우 stem 0*에서 가장 많은 자료가 있고 해당 자료도 skewed to the right 되어 있는 것을 확인할 수 있다.\nstem.leaf.backback(alloxan,insulin,rule.line=\"Sturges\")\n\n________________________________\n  1 | 2: represents 120, leaf unit: 10 \n      alloxan      insulin  \n________________________________\n   2       41| 0* |123444   6   \n   6     8765| 0. |67789   (5)  \n  (4)    4320| 1* |0033     8   \n   8      776| 1. |5        4   \n             | 2* |24       3   \n   5       77| 2. |             \n             | 3* |             \n________________________________\nHI: 391 469        HI: 465      \n499                             \nn:         20      20       \nNAs:        2      1        \n________________________________\n\n# 첫번째 stem and leaf에서는 alloxan data가 종모양을 이루고 있지 않은 것으로 보였으나 조금더 확대하였더니 어느정도 종모양을 띄고 있는 것이 확인된다.\n\n\nfivenum(normal)\n\n[1]  14.0  98.0 124.5 267.5 655.0\n\nfivenum(alloxan)\n\n[1]  13.0  73.0 139.5 276.0 499.0\n\nfivenum(insulin)\n\n[1]  18  45  82 132 465\n\n# Boxplot\nboxplot(diabetes,main=\"Comparison of groups\")\n\n\n\nboxplot(diabetes)$out #2개의 outlier 확인됨. (아까 Stem and leaf에서 확인 한 부분 재확인)\n\n\n\n\n[1] 655 465\n\n# normal data에서 655의 outlier, insulin 투여 쥐에서 465의 outlier 확인\nboxplot(diabetes)$stats \n\n      [,1]  [,2] [,3]\n[1,]  14.0  13.0   18\n[2,]  98.0  73.0   45\n[3,] 124.5 139.5   82\n[4,] 267.5 276.0  132\n[5,] 455.0 499.0  243\n\n# Skewness\nskewness(normal) # skewed to the right\n\n[1] 0.6873156\n\nskewness(alloxan) # skewed to the right\n\n[1] 0.3448276\n\nskewness(insulin) # skewed to the right\n\n[1] 0.1494253\n\n# Letter Value Display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\nlvd9=(lsum(normal,6))\nlvd10=(lsum(alloxan,6))\nlvd11=(lsum(insulin,6))\nlvd9\n\n  letter depth lower    mid upper spread\n1      M  10.5 124.5 124.50 124.5    0.0\n2      H   5.5  98.0 182.75 267.5  169.5\n3      E   3.0  29.0 189.00 349.0  320.0\n4      D   2.0  26.0 240.50 455.0  429.0\n5      C   1.5  20.0 287.50 555.0  535.0\n6      B   1.0  14.0 334.50 655.0  641.0\n\nlvd10\n\n  letter depth lower    mid upper spread\n1      M   9.5 139.5 139.50 139.5    0.0\n2      H   5.0  73.0 174.50 276.0  203.0\n3      E   3.0  50.0 220.50 391.0  341.0\n4      D   2.0  46.0 257.50 469.0  423.0\n5      C   1.5  29.5 256.75 484.0  454.5\n6      B   1.0  13.0 256.00 499.0  486.0\n\nlvd11\n\n  letter depth lower   mid upper spread\n1      M  10.0    82  82.0    82      0\n2      H   5.5    45  88.5   132     87\n3      E   3.0    34 131.0   228    194\n4      D   2.0    20 131.5   243    223\n5      C   1.5    19 186.5   354    335\n6      B   1.0    18 241.5   465    447\n\n# H-Spread\nlvd9[2,5]-lvd9[2,3]\n\n[1] 169.5\n\nlvd10[2,5]-lvd10[2,3] \n\n[1] 203\n\nlvd11[2,5]-lvd11[2,3]\n\n[1] 87\n\n# alloxan 집단에서 spread가 가장 크게 나타나며, normal 집단이 그다음, insulin 투여 집단의 분포는 좁게 나타나고 있다.\n\n# Kurtosis (E-spread / H-spread - 1.705)\n(lvd9[3,5]-lvd9[3,3])/(lvd9[2,5]-lvd9[2,3])-1.705 # more peaked than normal\n\n[1] 0.1829056\n\n(lvd10[3,5]-lvd10[3,3])/(lvd10[2,5]-lvd10[2,3])-1.705 # less peaked than normal\n\n[1] -0.02519704\n\n(lvd11[3,5]-lvd11[3,3])/(lvd11[2,5]-lvd11[2,3])-1.705 # more peaked than normal\n\n[1] 0.5248851\n\n# 정규성\nqqnorm(normal, ylab=\"normal group quantiles\",main=\"normal group\");qqline(normal, col='red',lty=2)\n# 점들이 직선을 잘 따르고 있지 않다. 앞에서 언급했던 outlier을 재확인하였으며, 점들이 convex한 곡선을 띄고 있는 것을 확인할 있다. \nfiv9=fivenum(normal)\n(pseudosigma9 = (fiv9[4]-fiv9[2])/1.34)\n\n[1] 126.4925\n\nsd(normal) # 약 32정도의 차이\n\n[1] 158.8349\n\nabline(fiv9[3],pseudosigma9,col=\"blue\",lty=2)\nlegend(x = -1.8, y = 600, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 차이 존재\n\n\n\nqqnorm(alloxan, ylab=\"alloxan group quantiles\",main=\"alloxan group\");qqline(alloxan, col='red',lty=2)\n# 점들이 직선을 잘 따르고 있지 않다. 점들이 곡선을 띄고 있는 것을 확인할 있다. \nfiv10=fivenum(alloxan)\n(pseudosigma10 = (fiv10[4]-fiv10[2])/1.34)\n\n[1] 151.4925\n\nsd(alloxan,na.rm=T) # 약 7정도의 차이\n\n[1] 144.8493\n\nabline(fiv10[3],pseudosigma10,col=\"blue\",lty=2)\nlegend(x = -1.8, y = 500, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 차이 존재\n\n\n\nqqnorm(insulin, ylab=\"insulin group quantiles\",main=\"insulin group\");qqline(insulin, col='red',lty=2)\n# oulier와 몇몇 2~3개의 점을 제외하면 점들이 직선을 잘 따르고 있는 것으로 보인다. \nfiv11=fivenum(insulin)\n(pseudosigma11 = (fiv11[4]-fiv11[2])/1.34)\n\n[1] 64.92537\n\nsd(insulin,na.rm=T) # 약 41정도의 차이\n\n[1] 105.7896\n\nabline(fiv11[3],pseudosigma11,col=\"blue\",lty=2)\nlegend(x = -1.8, y = 450, c(\"qqline\", \"robustline\"), \n       lty=2,lwd=2,col = c(\"red\",\"blue\")) # 거의 비슷함.\n\n\n\n#전반적으로 자료들이 skewed to the right되어있고 정규분포를 잘 따르지 않는 것으로 보인다. \n\n# 두 데이터를 합치고 해당 데이터들이 동일한 분포를 따르고 있는지 확인하기 \ndiabetes2=c(normal,alloxan,insulin)\ndiabetes2\n\n [1] 156 282 197 297 116 127 119  29 253 122 349 110 143  64  26  86 122 455 655\n[20]  14 391  46 469  86 174 133  13 499 168  62 127 276 176 146 108 276  50  73\n[39]  NA  NA  82 100  98 150 243  68 228 131  73  18  20 100  72 133 465  40  46\n[58]  34  44  NA\n\nqqnorm(diabetes2);qqline(diabetes2,col=\"red\",lty=2) \n\n\n\n#자료들이 Convex한 곡선의 형태를 띄고 있다 따라서 변환을 통해서 동일한 분포를 따르고 있는지 재확인해야 한다.\n#큰 값쪽으로 긴 꼬리를 뻗은 기울어진 분포\n\n# 자료의 재표현 - 일반적으로 많이 사용하는 재표현 방식의 사용\nboxplot(log(diabetes))\n\n\n\nboxplot(sqrt(diabetes))\n\n\n\nboxplot(-1/(diabetes))\n\n\n\nboxplot(-1/sqrt(diabetes))\n\n\n\n# Spread-versus-level Plot\ndiabetes_spread=c(lvd9[2,5]-lvd9[2,3],lvd10[2,5]-lvd10[2,3],lvd11[2,5]-lvd11[2,3])\ndiabetes_med=c(median(normal),median(alloxan,na.rm=T),median(insulin,na.rm=T))\n\nplot(log(diabetes_med), log(diabetes_spread), main=\"Spread vs. Level plot\")\n\nlogspread=log(diabetes_spread)\nlogmed=log(diabetes_med)\n(RegrLine <- lm(logspread~logmed))     \n\n\nCall:\nlm(formula = logspread ~ logmed)\n\nCoefficients:\n(Intercept)       logmed  \n     -2.564        1.595  \n\nabline(coef(RegrLine))\n\n\n\n1 - coef(RegrLine)[2] #-0.5853975 \n\n    logmed \n-0.5953067 \n\n# -0.5에 가까움\n\nboxplot(-(diabetes)^-0.5,main=\"-0.5 transformation\")\nboxplot(-(diabetes)^-0.5,main=\"-0.5 transformation\")$out # 총 4개의 outlier\n\n\n\n\n[1] -0.1856953 -0.1961161 -0.2672612 -0.2773501\n\nskewness_transformed1=c(skewness((-(diabetes)^-0.5)[,1]), skewness((-(diabetes)^-0.5)[,2]), skewness((-(diabetes)^-0.5)[,3]))\nskewness_transformed1\n\n[1]  0.4079370 -0.1365476 -0.2461638\n\nmean(skewness_transformed1) # skewness가 가장적음\n\n[1] 0.008408507\n\nboxplot(log(diabetes),main=\"log transformation\")\nboxplot(log(diabetes),main=\"log transformation\")$out # 총 3개의 outlier\n\n\n\n\n[1] 2.639057\n\nskewness_transformed2=c(skewness(log(diabetes)[,1]), skewness(log(diabetes)[,2]), skewness(log(diabetes)[,3]))\nskewness_transformed2     \n\n[1]  0.51161785  0.02774901 -0.11543537\n\nmean(skewness_transformed2)\n\n[1] 0.1413105\n\ndiabetes3=-(c(normal,alloxan,insulin))^-0.5\nqqnorm(diabetes3,main=\"minus sqrt inverse qqplot\");qqline(diabetes3,col=\"red\",lty=2) \n\n\n\ndiabetes4=log(c(normal,alloxan,insulin))\nqqnorm(diabetes4,main=\"log qqplot\");qqline(diabetes4,col=\"red\",lty=2) \n\n\n\ndiabetes5=sqrt(c(normal,alloxan,insulin))\nqqnorm(diabetes5,main=\"sqrt qqplot\");qqline(diabetes5,col=\"red\",lty=2) \n\n\n\ndiabetes6=-(c(normal,alloxan,insulin))^-1\nqqnorm(diabetes6,main=\"minus inverseqqplot\");qqline(diabetes6,col=\"red\",lty=2) \n\n\n\n# 전반적으로 log 나 sqrt변환 이후 자료들이 직선을 더 잘 따르고 있는 것을 확인할 수 있다. \n# QQplot 하에서 양 극단의 일부 데이터를 제외한 나머지 데이터들은 전부 qqline위에 있다는 점에서 변환이후 데이터들은 동일한 정규분포에서 나타난 것을 확인할 수 있다.\n\n# 가장 정규분포를 잘 따르고 있는 것으로 보이는 log변환을 활용해 Tuckey_Mean_Difference_Plot\n\nnor_log=log(diabetes)[,1];alo_log=log(diabetes)[,2];ins_log=log(diabetes)[,3]\nalo_log=alo_log[!is.na(alo_log)]\nins_log=ins_log[!is.na(ins_log)]\n\nqqplot(nor_log,alo_log,xlim=c(min(nor_log,alo_log),max(nor_log,alo_log)),\n       ylim=c(min(nor_log,alo_log),max(nor_log,alo_log))\n       ,main=\"QQ_plot of logged normal group and alloxan group\")\nabline(0,1,lty=2,col=\"darkgreen\",lwd=2) # 대부분의 자료들이 주대각선을 따르는 것으로 보인다.\n\n\n\nqqplot(nor_log,ins_log,xlim=c(min(nor_log,ins_log),max(nor_log,ins_log)),\n       ylim=c(min(nor_log,ins_log),max(nor_log,ins_log))\n       ,main=\"QQ_plot of logged normal group and insulin group\")\nabline(0,1,lty=2,col=\"darkgreen\",lwd=2) # 대부분의 자료들이 주대각선 밑에 있는 것으로 보인다. 따라서 같은 분포에서 나오지 않았을 가능성이 있다.\n\n\n\nqqplot(alo_log,ins_log,xlim=c(min(alo_log,ins_log),max(alo_log,ins_log)),\n       ylim=c(min(alo_log,ins_log),max(alo_log,ins_log))\n       ,main=\"QQ_plot of logged alloxan group and insulin group\")\nabline(0,1,lty=2,col=\"darkgreen\",lwd=2)  # 대부분의 자료들이 주대각선 밑에 있는 것으로 보인다. 따라서 같은 분포에서 나오지 않았을 가능성이 있다.\n\n\n\n#정리하면 Normal Group과 Alloxan Group의 경우는 같은 분포에서 나왔을 가능성이 높지만, insulin 투여 그룹은 다른 분포를 따르고 있을 가능성이 높다. \n\nqq.x1 <- qqplot(nor_log,alo_log)$x\nqq.y1 <- qqplot(nor_log,alo_log)$y\n\n\n\nplot((qq.x1+qq.y1)/2, qq.y1-qq.x1, main=\"Tukey mean difference plot\", \n     ylab=\"alo_log-nor_log\", xlab=\"mean\")\nabline(0,0)\n\n\n\nqq.x2 <- qqplot(nor_log,ins_log)$x\nqq.y2 <- qqplot(nor_log,ins_log)$y\n\n\n\nplot((qq.x2+qq.y2)/2, qq.y2-qq.x2, main=\"Tukey mean difference plot\", \n     ylab=\"ins_log-nor_log\", xlab=\"mean\")\nabline(0,0) # 대부분의 자료들이 X축의 밑에 있다.\n\n\n\nqq.x3 <- qqplot(alo_log,ins_log)$x\nqq.y3 <- qqplot(alo_log,ins_log)$y\n\n\n\nplot((qq.x3+qq.y3)/2, qq.y3-qq.x3, main=\"Tukey mean difference plot\", \n     ylab=\"ins_log-alo_log\", xlab=\"mean\")\nabline(0,0) # 대부분의 자료들이 X축의 밑에 있다.\n\n\n\n# 변환 이후 각 자료는 정규분포를 따르는가?\nqqnorm(nor_log);qqline(nor_log,col=\"red\") # outlier들을 제외하면 정규분포를 잘 따르는 것으로 보인다. (곡선의 흔적이 있으나 완화됨)\n\n\n\nqqnorm(alo_log);qqline(alo_log,col=\"red\") # 한두개의 점을 제외하면 정규분포를 잘 따르는 것으로 보인다.\n\n\n\nqqnorm(ins_log);qqline(ins_log,col=\"red\") # 가장 정규분포를 잘 따른다\n\n\n\n# 결론: 변환이후 각 자료들은 정규분포를 따르고 있는 것으로 보이나 그 각각의 평균은 다른 것으로 보이기에 서로 다른 분포를 따르고 있다고 보는 것이 타당하다. \n\n# 각 집단별 평균차이를 확인하기 위해서 일원분산분석을 진행하고자 한다.\nlibrary(reshape2)\ndia_melt=melt(diabetes)\n\nNo id variables; using all as measure variables\n\ndia_melt=dia_melt[!is.na(dia_melt$value),]\naov4=aov(value~variable,data=dia_melt)\nsummary(aov4) #H0: 각 집단간 평균차이는 없다. H1: 각 집단간 평균차이는 있다. \n\n            Df  Sum Sq Mean Sq F value Pr(>F)\nvariable     2   64357   32178   1.675  0.197\nResiduals   54 1037470   19212               \n\n# Do not reject null hypothesis\n\nlibrary(doBy)\nsummaryBy(value ~ variable, data=dia_melt, FUN = c(mean, sd, min, max))\n\n  variable value.mean value.sd value.min value.max\n1   normal   186.1000 158.8349        14       655\n2  alloxan   181.8333 144.8493        13       499\n3  insulin   112.8947 105.7896        18       465\n\nlibrary(agricolae)\nHSD.test(aov4, \"variable\", group=TRUE,console=TRUE) # 각집단간 차이가 명확하게 드러나지 않는다\n\n\nStudy: aov4 ~ \"variable\"\n\nHSD Test for value \n\nMean Square Error:  19212.41 \n\nvariable,  means\n\n           value      std  r Min Max\nalloxan 181.8333 144.8493 18  13 499\ninsulin 112.8947 105.7896 19  18 465\nnormal  186.1000 158.8349 20  14 655\n\nAlpha: 0.05 ; DF Error: 54 \nCritical Value of Studentized Range: 3.408232 \n\nGroups according to probability of means differences and alpha level( 0.05 )\n\nTreatments with the same letter are not significantly different.\n\n           value groups\nnormal  186.1000      a\nalloxan 181.8333      a\ninsulin 112.8947      a\n\ndia_melt2=dia_melt\ndia_melt2$value=log(dia_melt$value)\naov5=aov(value~variable,data=dia_melt2)\nsummary(aov5)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nvariable     2   2.75  1.3751   1.665  0.199\nResiduals   54  44.61  0.8261               \n\n\n\n\n5.\n1974년부터 1979년 까지 영국에서 기관지염, 폐기종, 천식의 폐 질환에 의한 월별 사망자 수이다. 분석하라. [MONTHLY.DAT]\n\nlung=as.matrix(read.delim(\"C:/r/MONTHLY.DAT\",header=F,)[,c(2:13)])\nlung\n\n       V2   V3   V4   V5   V6   V7   V8   V9  V10  V11  V12  V13\n[1,] 3035 2552 2704 2554 2014 1655 1721 1524 1596 2074 2199 2512\n[2,] 2933 2889 2938 2497 1870 1726 1607 1545 1396 1787 2076 2837\n[3,] 2787 3891 3179 2011 1636 1580 1489 1300 1356 1653 2013 2823\n[4,] 2996 2523 2540 2520 1994 1641 1691 1479 1596 1877 2032 2484\n[5,] 2899 2990 2890 2379 1933 1734 1617 1495 1440 1777 1970 2745\n[6,] 2841 3535 3010 2091 1667 1589 1518 1349 1392 1619 1954 2633\n\nlung2=as.vector(c(lung[1,],lung[2,],lung[3,],lung[4,],lung[5,],lung[6,]))\nlung_ts=ts(lung2,frequency=12,start = c(1974,1), end=c(1979,12))\n\nsummary(lung_ts) #최소값은 1300, 최대값은 3891, 평균은 2002이다.\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1300    1618    2002    2144    2651    3891 \n\n# 결측치와 outlier의 존재여부를 체크하고자 한다.\nboxplot(lung_ts, main=\"Lung Disease Deaths\")  # Outlier는 없다. Upper Whisker의 길이가 및의 Whisker에 비해서 길게 나타나진다. \n\n\n\nsum(is.na(lung_ts)) # 결측치 없음.\n\n[1] 0\n\n# Steam and leaf plot을 활용해서 자료의 분포를 예측해보고자 했다.\nstem(lung_ts,1)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  1 | 334444\n  1 | 555555666666666777777788999\n  2 | 000000011124\n  2 | 55555566677888899999\n  3 | 00002\n  3 | 59\n\n# 쌍봉분포의 경향. 두개의 Cluster가 존재하는 것 처럼 보인다. \nstem(lung_ts,2)\n\n\n  The decimal point is 2 digit(s) to the right of the |\n\n  12 | 0569\n  14 | 0489022589\n  16 | 0012244567923389\n  18 | 783579\n  20 | 1113789\n  22 | 08\n  24 | 80122455\n  26 | 3059\n  28 | 244990349\n  30 | 0148\n  32 | \n  34 | 4\n  36 | \n  38 | 9\n\n#자료들이 Skewed to the right 되어 있는 것으로 보인다. Stem 16에서 가장 높게 나타나고 있으며 대칭적으로 보이지는 않는다. \n#중간에 빈칸이 있어서 두개의 Outlier가 있을 것으로 예상되었지만 Boxplot을 통해 보았듯이 따로 Outlier는 나타나지 않는 것으로 보인다.\n\n#skewness\nskewness(lung_ts) #Skewed to the right되어 있음.\n\n[1] 0.2679676\n\nskewness(log(lung_ts))\n\n[1] 0.1476176\n\nboxplot(log(lung_ts)) # log 변환을 하면 다소 완화됨\n\n\n\n# letter value display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\n(lvd12=lsum(lung_ts,9)) # mid 값이 점차 커지는 중\n\n  letter depth  lower     mid  upper spread\n1      M  36.5 2002.5 2002.50 2002.5    0.0\n2      H  18.5 1618.0 2143.25 2668.5 1050.5\n3      E   9.5 1506.5 2211.25 2916.0 1409.5\n4      D   5.0 1396.0 2203.00 3010.0 1614.0\n5      C   3.0 1356.0 2267.50 3179.0 1823.0\n6      B   2.0 1349.0 2442.00 3535.0 2186.0\n7      A   1.5 1324.5 2518.75 3713.0 2388.5\n8      Z   1.0 1300.0 2595.50 3891.0 2591.0\n\n# Kurtosis (E-spread) / (H-spread) - 1.705\n(lvd12[3,5]-lvd12[3,3])/(lvd12[2,5]-lvd12[2,3])-1.705 # less peaked than normal\n\n[1] -0.363258\n\n# H-Spread\nHspread=fivenum(lung_ts)[4]-fivenum(lung_ts)[2]\nHspread\n\n[1] 1050.5\n\n# 자료들이 정규성 따르냐?\nqqnorm(lung_ts, ylab=\"Lung Disease Deaths Quantiles\");qqline(lung_ts, col='red',lty=2) #역 S자 모양\n\n\n\nqqnorm(log(lung_ts), ylab=\"Logged Lung Disease Deaths Quantiles\");qqline(log(lung_ts), col='red',lty=2) #역 S자 모양\n\n\n\n# 데이터들이 혼합 분포에서 나오지 않았을까하고 추측되어진다. (단일 정규분포를 따르지 않으므로 정규성은 X)\n\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nts.plot(lung_ts, main = \"Time-Series Plot: Lung Diseases Deaths\") #1976년과 1979년의 봉우리에 두번의 특이값이 확인됨\n\n\n\nts.plot(diff(lung_ts), main = \"Time-Series Plot: Lung Diseases Deaths\") #차분데이터에 대해서 시계열자료는 다음과 같다. \n\n\n\n# 6년동안 6개의 봉우리가 있으므로 매년 이러한 계절성이 존재한다는 것을 확인할 수 있다. \n\n# Box-Cox Transformation을 통해서 데이터를 변환하였다. (계절변동을 확인하기 위해서, 데이터의 정규성을 개선하기 위해)\nlambda <- forecast::BoxCox.lambda(lung_ts)\nlung_ts_new <- forecast::BoxCox(lung_ts, lambda)\n\n# 데이터가 0인 경우가 없기 때문에 Boxcox transforamtion을 진행해도 문제 없다.\nplot(lung_ts_new, main = \"Box-Cox : Lung Diseases Deaths\")\n\n\n\nplot(diff(lung_ts_new), main = \"Difference & Box-Cox : Lung Diseases Deaths\")\n\n\n\n# 계절요인 분해시계열\nlung_ts.decompose <- decompose(lung_ts)       # 데이터에서 4가지 요인을 분해\nlung_ts.decompose$seasonal                     # 계절요인으로 분해된 부분이다.\n\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n1974  732.1972 1009.7472  758.7056  152.3972 -321.3694 -486.3361 -530.4278\n1975  732.1972 1009.7472  758.7056  152.3972 -321.3694 -486.3361 -530.4278\n1976  732.1972 1009.7472  758.7056  152.3972 -321.3694 -486.3361 -530.4278\n1977  732.1972 1009.7472  758.7056  152.3972 -321.3694 -486.3361 -530.4278\n1978  732.1972 1009.7472  758.7056  152.3972 -321.3694 -486.3361 -530.4278\n1979  732.1972 1009.7472  758.7056  152.3972 -321.3694 -486.3361 -530.4278\n           Aug       Sep       Oct       Nov       Dec\n1974 -693.4028 -695.9444 -337.8361 -106.6861  518.9556\n1975 -693.4028 -695.9444 -337.8361 -106.6861  518.9556\n1976 -693.4028 -695.9444 -337.8361 -106.6861  518.9556\n1977 -693.4028 -695.9444 -337.8361 -106.6861  518.9556\n1978 -693.4028 -695.9444 -337.8361 -106.6861  518.9556\n1979 -693.4028 -695.9444 -337.8361 -106.6861  518.9556\n\nplot(lung_ts.decompose)\n\n\n\nts.plot(lung_ts.decompose$seasonal,main=\"The Plot of Seasoal Decomposition\")\n\n\n\n# 계절요인 제외시키기\nlung_ts.decompose_new <- lung_ts - lung_ts.decompose$seasonal\nts.plot(lung_ts.decompose_new, main=\"Time-Series without Seasonal Effect\")\n\n\n\n# 계절성을 제외한 나머지 요인들을 분석할 경우 1976,1979년도의 Random한 요인에 의해서 데이터가 늘어난 것을 확인 할 수 있다.\n# 또한 1974년 1977년 두번의 Random한 요인에 의해서 데이터가 줄어듦\n\nlung_1974=ts(lung_ts[1:12],start=1)\nlung_1975=ts(lung_ts[13:24],start=1)\nlung_1976=ts(lung_ts[25:36],start=1)\nlung_1977=ts(lung_ts[37:48],start=1)  \nlung_1978=ts(lung_ts[49:60],start=1)\nlung_1979=ts(lung_ts[61:72],start=1)\nyr=paste(\"lung\",\"_\",1974:1979,sep=\"\")\nxat=seq(0,12,by=1)\n\npar(mfrow=c(2,3))\n\nfor (i in yr) {ts.plot(as.name(i),main=i,ylab=\"Deaths by lung diseases\")\n  axis(side=1,at=xat)}\n\n\n\n#각 년도별 계절성을 비교하기 위해서 이런식으로 그래프를 연도별로 쪼개서 그렸다. 연도별로 그러한 경향성이 비슷하게 드러나고 있는 것으로 보인다.\n\nlibrary(\"ghibli\")\npal=ghibli_palette(\"PonyoMedium\",n=6)\nas.character(pal)\n\n[1] \"#4C413FFF\" \"#5A6F80FF\" \"#278B9AFF\" \"#E75B64FF\" \"#DE7862FF\" \"#D8AF39FF\"\n\npar(mfrow=c(1,1))\n\n\nts.plot(lung_1974,main=\"Deaths by lung diseases\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(1000,4000))\naxis(side=1,at=xat)\nlines(lung_1975,lwd=2,col=pal[2])\nlines(lung_1976,lwd=2,col=pal[3])\nlines(lung_1977,lwd=2,col=pal[4])\nlines(lung_1978,lwd=2,col=pal[5])\nlines(lung_1979,lwd=2,col=pal[6])\n\nlegend(x = 10, y = 4000, c(1974:1979), \n       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n# 전체데이터와 월별데이터에 평활을 진행하고자 한다. 평활법은 교수님께서 추천하셨던 3RSSH Twice, 4253H Twice에 더해 Hanning이 이루어지기 전 평활법인 3RS3R Twice를 추가하고자 한다.\n\n# 3RSSH Twice\nsmooth_3RSSH=function(data){\n  smooth3RSS=smooth(data, kind=\"3RSS\")\n  \n  n=length(data)\n  smooth3RSSH=smooth3RSS\n  \n  \n  for (i in 2:(n-1)) {smooth3RSSH[i] <- smooth3RSS[i-1]/4 + smooth3RSS[i]/2 + smooth3RSS[i+1]/4}\n  smooth3RSSH[1] <- smooth3RSS[1]; smooth3RSSH[n] <- smooth3RSS[n]\n  rough=data-smooth3RSSH\n  roughH=rough\n  \n  smooth3RSS2=smooth(rough,kind=\"3RSS\")\n  \n  for (i in 2:(n-1)) roughH[i] <- smooth3RSS2[i-1]/4 + smooth3RSS2[i]/2 + smooth3RSS2[i+1]/4\n  roughH[1] <- smooth3RSS2[1]; roughH[n] <- smooth3RSS2[n]\n  out=smooth3RSSH+roughH\n  out=as.vector(out)\n  return(out)\n}\n\nlibrary(sleekts)\npal2=as.vector(ghibli_palette(\"MononokeMedium\")[c(1,3,5,6)])\nts.plot(lung_ts,main=\"Raw Data:All\",ylab=\"# of deaths\",lty=2,col=pal2[1])\nlines(ts(smooth(lung_ts, kind=\"3RS3R\",twiceit=T),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[2],lwd=2)\nlines(ts(smooth_3RSSH(lung_ts),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[3],lwd=2)\nlines(ts(sleek(lung_ts),frequency = 12,start=c(1974,1),end=c(1979,12)),col=pal2[4],lwd=2)\n\nlegend(x =1974, y = 3900, c(\"Default\", \"3RS3R Twice\", \"3RSSH Twice\",\"4253H Twice\"), \n       lty=c(2,1,1,1),lwd=2,col = pal2,cex=0.7)\n\n\n\n#평활법을 적용하여 확인할 경우 원 자료에 비해 최댓값과 최솟값의 폭이 많이 줄어들었음을 확인할 수 있다. 작은 값에서는 크게 변화가 없지만 값이 큰 자료들의 경우 많이 깎여나갔다.\n#시계열을 확인하는데 있어서 그 계절성을 확인하기 좋은 형태로 평활이 된 것은 사실이지만, 위에서 언급한 1976, 1979년도의 특이값이 사라지게 되었고 그 특이값을 해석하는데 있어서 주의를 기울여야 할 것으로 보인다. 3RS3R Twice 기법의 경우 Hanning이 진행되지 않았기 때문에 다소 각진 부분이 남아있지만, 전반적으로 계절성이 나타나는 형태로 데이터를 변화시켰다. 나머지 Hanning을 사용한 2가지 평활법의 차이를 분석하면 4253H방법에서 큰 값들의 감소폭이 크게 나타나고 있다. 3가지 평활법의 양 끝자료의 경우 실제 존재하는 데이터를 가지고 만든 것이 아니기 때문에 그 추세를 해석하는데 있어서 용이하지만 실제 데이터와 차이가 있으므로 해석에 유의해야 할 것이다.\n# 앞에서 시계열 Decompose를 통해서 그렸던 계절성 그래프의 모양과 4253H Twice의 그래프가 상당히 유사한 것으로 보인다. 1년을 주기로 폐질환 사망자수가 Fluctuate 하고 있는데 겨울철에 전반적으로 증가하고 여름철에 감소하는 경향성을 가지는 것이 확인된다. 각 주기는 거의 대칭적으로 증감을 반복하고 있으며 1976, 1977년도를 제외하면 사망건수는 거의 비슷하게 유지 되는 것을 확인할 수 있다. 따라서 시계열 자료를 해석할 때 1976년도와 1977년도 자료는 유의해서 해석해야 할 것으로 보인다.\n\n# Raw Data\nts.plot(lung_1974,main=\"Deaths by lung diseases\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(1000,4000))\naxis(side=1,at=xat)\nlines(lung_1975,lwd=2,col=pal[2])\nlines(lung_1976,lwd=2,col=pal[3])\nlines(lung_1977,lwd=2,col=pal[4])\nlines(lung_1978,lwd=2,col=pal[5])\nlines(lung_1979,lwd=2,col=pal[6])\nlegend(x = 10, y = 4000, c(1974:1979), \n       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n### 3RS3R Twice\nts.plot(ts(smooth(lung_1974, kind=\"3RS3R\",twiceit=T),start=1, end=12),main=\"3RS3R TwicC: Deaths by lung diseases\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(1000,4000))\naxis(side=1,at=xat)\nlines(ts(smooth(lung_1975, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[2])\nlines(ts(smooth(lung_1976, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[3])\nlines(ts(smooth(lung_1977, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[4])\nlines(ts(smooth(lung_1978, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[5])\nlines(ts(smooth(lung_1979, kind=\"3RS3R\",twiceit=T),start=1, end=12),lwd=2,col=pal[6])\nlegend(x = 10, y = 4000, c(1974:1979), \n       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n### 3RSSH Twice\nts.plot(ts(smooth_3RSSH(lung_1974),start=1, end=12),main=\"3RSSH TwicC: Deaths by lung diseases\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(1000,4000))\naxis(side=1,at=xat)\nlines(ts(smooth_3RSSH(lung_1975),start=1, end=12),lwd=2,col=pal[2])\nlines(ts(smooth_3RSSH(lung_1976),start=1, end=12),lwd=2,col=pal[3])\nlines(ts(smooth_3RSSH(lung_1977),start=1, end=12),lwd=2,col=pal[4])\nlines(ts(smooth_3RSSH(lung_1978),start=1, end=12),lwd=2,col=pal[5])\nlines(ts(smooth_3RSSH(lung_1979),start=1, end=12),lwd=2,col=pal[6])\nlegend(x = 10, y = 4000, c(1974:1979), \n       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n### 4253H Twice\n\nts.plot(ts(sleek(lung_1974),start=1, end=12),main=\"4253H TwicC: Deaths by lung diseases\",xlab=\"Month\",ylab=\"# of Deaths\",col=pal[1],lwd=2,ylim=c(1000,4000))\naxis(side=1,at=xat)\nlines(ts(sleek(lung_1975),start=1, end=12),lwd=2,col=pal[2])\nlines(ts(sleek(lung_1976),start=1, end=12),lwd=2,col=pal[3])\nlines(ts(sleek(lung_1977),start=1, end=12),lwd=2,col=pal[4])\nlines(ts(sleek(lung_1978),start=1, end=12),lwd=2,col=pal[5])\nlines(ts(sleek(lung_1979),start=1, end=12),lwd=2,col=pal[6])\nlegend(x = 10, y = 4000, c(1974:1979), \n       lty=c(rep(1,6)),lwd=2,col = c(as.character(pal)),cex=0.6)\n\n\n\n# 원자료 --> 3RS3R Twice --> 3RSSH Twice--> 4253H Twice순으로 시계열의 추세가 유사해지고 있는것을 확인할 수 있다.\n# Hanning을 사용하여 3RSSH Twice, 4253H Twice는 평평한 구간 없이 부드럽게 넘어가고 있으며 1976,1979년도 평활법을 통해서 최대값이 줄어들어 다른 년도와 유사한 계절성을 확인할 수 있게 되었다.\n# 다만 4253H Twice 방법보다는 3RSSH Twice 방법이 더 적절한 것으로 보인다. 왜냐하면 11월~12월의 값이 서로 같게 나타나고 있기 때문이다. \n\nlibrary(forecast)\naTSA::adf.test(lung_ts, nlag = NULL, output = TRUE) # p-value<=0.01 귀무가설을 기각하여 정상시계열\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -0.922   0.347\n[2,]   1 -0.938   0.341\n[3,]   2 -1.067   0.295\n[4,]   3 -0.926   0.345\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -3.12  0.0328\n[2,]   1 -4.71  0.0100\n[3,]   2 -5.93  0.0100\n[4,]   3 -6.96  0.0100\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -3.02   0.159\n[2,]   1 -4.67   0.010\n[3,]   2 -5.85   0.010\n[4,]   3 -6.89   0.010\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\nfit <- auto.arima(lung_ts)\nplot(forecast(fit, level=c(75, 95), h=12),col='black') \n\n\n\n# 다음 1년간의 변화를 예측할 경우 이전의 자료들과 비슷한 추세를 가지고 있는 것이 확인되어 진다.\n\n\n\n6.\n헬멧의 충돌 실험에서 충돌 이후 머리의 가속(g)과 시간의 경과(milliseconds)에 대한 자료이다. 자료를 평활하고 특징을 설명하여라. [HELMETS.DAT]\n\nhelmet=(read.delim(\"C:/r/HELMETS.DAT\",header=F,))\nh1=helmet[c(1,2)]\nnames(h1)=c(\"imptime\",\"accel\")\nh2=helmet[c(3,4)]\nnames(h2)=c(\"imptime\",\"accel\")\nh3=helmet[c(5,6)]\nnames(h3)=c(\"imptime\",\"accel\")\n\nhelmet=rbind(h1,h2,h3)\ndim(helmet)\n\n[1] 135   2\n\nplot(helmet$accel~helmet$imptime,main=\"scatter plot of impact time and head\") # 두 좌표간의 관계를 보면 곡선의 관계를 띄고 있는 것을 확인할 수 있다.\n\n\n\n# 결측치와 outlier의 존재여부를 체크하고자 한다.\nboxplot(helmet, main=\"Helmet\")  # Outlier는 없다. 충돌 후 시간의 경우 median이 lower hinge쪽으로 기울어 있고, 머리의 가속의 경우 median이 upper hinge쪽으로 기울어 있다. \n\n\n\ncolSums(is.na(helmet)) # 결측치 각각 두개씩\n\nimptime   accel \n      2       2 \n\nhelmet=helmet[ifelse(!is.na(helmet)[,1]==F|!is.na(helmet)[,2]==F,F,T),]\nhead(helmet,5) # 결측치 제거 후 분석 시작\n\n  imptime accel\n1     2.4   0.0\n2     2.6  -1.3\n3     3.2  -2.7\n4     3.6   0.0\n5     4.0  -2.7\n\ndim(helmet)\n\n[1] 133   2\n\nattach(helmet)\nsummary(imptime) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.40   15.60   23.40   25.18   34.80   57.60 \n\nsummary(accel) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-134.00  -54.90  -13.30  -25.55    0.00   75.00 \n\n# Steam and leaf plot을 활용해서 자료의 분포를 예측해보고자 했다.\nstem(imptime,1) # stem 1을 기준으로 skewed 된 종모양\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 23344\n  0 | 6778899\n  1 | 000111344\n  1 | 5555555555566666666666777788888899999\n  2 | 000112233444\n  2 | 55555666667777888899\n  3 | 011223344\n  3 | 555566668899\n  4 | 0022233344\n  4 | 57889\n  5 | 123\n  5 | 5558\n\nstem(accel,0.5) # stem -0을 기준으로 skewed 종모양\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  -12 | 49733333\n  -10 | 83387422\n   -8 | 9516620\n   -6 | 8221642\n   -4 | 9855441116430\n   -2 | 882777433222222\n   -0 | 76665533311955555533333333333333311111\n    0 | 000000014580111111256\n    2 | 3912568\n    4 | 677785\n    6 | 055\n\n#skewness\nskewness(imptime) #Skewed to the right되어 있음.\n\n[1] 0.1875\n\nskewness(accel) #Skewed to the left되어 있음\n\n[1] -0.5154827\n\n# letter value display\nsource(\"http://mgimond.github.io/ES218/es218.R\")\n(lvd13=lsum(imptime,9)) # mid 값이 점차 커지는 중\n\n  letter depth lower  mid upper spread\n1      M  67.0  23.4 23.4  23.4    0.0\n2      H  34.0  15.6 25.2  34.8   19.2\n3      E  17.5  11.2 26.9  42.6   31.4\n4      D   9.0   7.8 27.8  47.8   40.0\n5      C   5.0   4.0 28.6  53.2   49.2\n6      B   3.0   3.2 29.1  55.0   51.8\n7      A   2.0   2.6 29.0  55.4   52.8\n8      Z   1.5   2.5 29.5  56.5   54.0\n9      Y   1.0   2.4 30.0  57.6   55.2\n\n(lvd14=lsum(accel,9))\n\n  letter depth   lower     mid  upper spread\n1      M  67.0  -13.30 -13.300 -13.30   0.00\n2      H  34.0  -54.90 -27.450   0.00  54.90\n3      E  17.5  -97.10 -40.875  15.35 112.45\n4      D   9.0 -117.90 -36.150  45.60 163.50\n5      C   5.0 -123.10 -37.450  48.20 171.30\n6      B   3.0 -127.20 -28.800  69.60 196.80\n7      A   2.0 -128.50 -26.750  75.00 203.50\n8      Z   1.5 -131.25 -28.125  75.00 206.25\n9      Y   1.0 -134.00 -29.500  75.00 209.00\n\n# Kurtosis (E-spread) / (H-spread) - 1.705\n(lvd13[3,5]-lvd13[3,3])/(lvd13[2,5]-lvd13[2,3])-1.705 # less peaked than normal\n\n[1] -0.06958333\n\n(lvd14[3,5]-lvd14[3,3])/(lvd14[2,5]-lvd14[2,3])-1.705 # more peaked than normal\n\n[1] 0.3432696\n\n# H-Spread\n(lvd13[2,5]-lvd13[2,3]) \n\n[1] 19.2\n\n(lvd14[2,5]-lvd14[2,3]) \n\n[1] 54.9\n\n# 자료들이 정규성 따르냐?\nqqnorm(imptime, ylab=\"Impact Time Quantiles\");qqline(imptime, col='red',lty=2) # 자료들이 정규분포를 잘 따르고 있는 것으로 보인다. (꼬리부분에서 다소 벗어나 보이나)\n\n\n\nqqnorm(accel, ylab=\"Head Acceleration Quantiles\");qqline(accel, col='red',lty=2) # 자료들이 qqline을 잘 따르기 보다는 역s자 곡선의 형태를 띄고 있는 것으로 보인다.\n\n\n\n# 시계열 분석이 아니라 산점도 평활법을 적용해 분석을 진행해보고자 한다. \nplot(helmet$accel~helmet$imptime,main=\"scatter plot of impact time and head acceleration\")\nlines(lowess(accel~imptime,f=2/3), col = \"Red\")\nlines(lowess(accel~imptime,f=1/2), col = \"blue\")\nlines(lowess(accel~imptime,f=1/3), col = \"darkgreen\")\nlegend(3, 75, c(paste(\"f = \", c(\"2/3\", \"1/2\",\"1/3\"))), lty = 1, col = c(\"red\",\"blue\",\"darkgreen\"), cex=0.7)\n\n\n\n# f=1/3일때 가장 이상적으로 보인다.\n\n# residuals with the default span f = 2/3\nresiduals <- helmet$accel - lowess(helmet)$y\nplot(residuals ~ helmet$imptime, main = \"Residuals with f = 2/3\")\nlines(lowess(helmet$imptime, residuals, f=0.3))\n\n\n\n#여전히 잔차들이 경향성을 가지는 것으로 보인다.\n\n\n# residuals with span f = 1/2\nresiduals <- helmet$accel - lowess(helmet,f=0.5)$y\nplot(residuals ~ helmet$imptime, main = \"Residuals with f = 1/2\")\nlines(lowess(helmet$imptime, residuals, f=0.3))\n\n\n\n#여전히 잔차들이 경향성을 가지는 것으로 보인다.\n\n# residuals with span f = 1/3\nresiduals <- helmet$accel - lowess(helmet,f=1/3)$y\nplot(residuals ~ helmet$imptime, main = \"Residuals with f = 1/3\")\nlines(lowess(helmet$imptime, residuals, f=1))\n\n\n\n# 잔차들의 경향성이 완화되었다.\n\nplot(helmet, main = \"loess(cars)\")\n\nhelmet.lo <- loess(accel ~ imptime, helmet)\nhelmet.lo.pred <- predict(helmet.lo, data.frame(imptime=seq(0, 60, length=133)), se=TRUE)\nlines(helmet.lo.pred$fit ~ seq(0, 60, length=133), col = 2)\n\n\n\n\n\n\n7.\n어린 아이들의 나이별 평균 사용 단어 수이다. 분석하여라.\n\nage=c(1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,6.0)\nwords=c(3,22,272,446,896,1222,1540,1870,2072,2562)\n\nlength(words) #총 10개의 자료\n\n[1] 10\n\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.125   3.250   3.300   4.375   6.000 \n\nsummary(words)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    3.0   315.5  1059.0  1090.5  1787.5  2562.0 \n\nboxplot(age,main=\"age\")\n\n\n\nboxplot(words,main=\"words\") #outlier는 없다.\n\n\n\nmedage <- as.vector(3)\nmedwords <- as.vector(3)\nmedage[1] <- median(age[1:3],na.rm=T); medwords[1] <- median(words[1:3],na.rm=T)\nmedage[2] <- median(age[4:7],na.rm=T); medwords[2] <- median(words[4:7],na.rm=T)\nmedage[3] <- median(age[8:10],na.rm=T); medwords[3] <- median(words[8:10],na.rm=T)\nplot(medwords ~ medage, type=\"b\") \n\n\n\n# 선형 관계로 나타나는 것을 확인할 수 있다. \n\ncor.test(words,age)\n\n\n    Pearson's product-moment correlation\n\ndata:  words and age\nt = 23.134, df = 8, p-value = 1.294e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9678822 0.9983153\nsample estimates:\n      cor \n0.9926087 \n\nlmfit=lm(words~age)\nlmfit2=lm(words~age+0)\nsummary(lmfit) #단순회귀에서는 상관계수/회귀값 모두 두 변수가 상관정도가 높게 나타나고 있다.\n\n\nCall:\nlm(formula = words ~ age)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-194.959  -54.200   -3.404   48.670  204.931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -763.86      88.25  -8.656 2.47e-05 ***\nage           561.93      24.29  23.134 1.29e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 116.7 on 8 degrees of freedom\nMultiple R-squared:  0.9853,    Adjusted R-squared:  0.9834 \nF-statistic: 535.2 on 1 and 8 DF,  p-value: 1.294e-08\n\n# 나이가 한살 많아질수록 알고 있는 단어의 수는 561.93개로 나타남. \n\nsummary(lmfit2) #모르는 단어가 음수일 수는 없기 때문에 절편을 원점으로 고정하고 회귀분석을 진행할 경우 나이 한살이 많아질 수록 370.96개의 단어를 추가적으로 알 것이다라고 추론 할 수 있다. \n\n\nCall:\nlm(formula = words ~ age + 0)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-534.4 -444.4 -146.6  164.5  336.2 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \nage   370.96      30.84   12.03 7.55e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 354.4 on 9 degrees of freedom\nMultiple R-squared:  0.9414,    Adjusted R-squared:  0.9349 \nF-statistic: 144.7 on 1 and 9 DF,  p-value: 7.551e-07\n\nplot(words~age,main=\"linearity of words ~ age\")\n(z5<-line(x=age,y=words))\n\n\nCall:\nline(x = age, y = words)\n\nCoefficients:\n[1]  -917   608\n\nabline(coef(z5))\nz5.ls <- lm(words~age)\nabline(z5.ls$coef, lty=2,col=\"red\")\nlegend(x = 1, y = 2300, c(\"R - Line\", \"Regression\"), \n       lty=c(1,2),lwd=2,col = c(\"black\",\"red\"),cex=0.7)       \n\n\n\nplot(residuals(z5) ~ fitted(z5), main = \"Residual plot by rline\") \nabline(0,0) \n\n\n\n# 1살에서의 단어의 수를 제외하면 대부분의 데이터들이 0 근처에 있는 것이 확인되어 진다.\n\nboxplot(residuals(z5))\nboxplot(residuals(z5))$out #lower fence 밖의 oulier -169, upper fence 밖의 outlier 312\n\n\n\n\n[1]  312 -169\n\nstem(residuals(z5)) # Outlier 재확인 가능\n\n\n  The decimal point is 2 digit(s) to the right of the |\n\n  -1 | 76\n  -0 | 531\n   0 | 1335\n   1 | \n   2 | \n   3 | 1\n\nsum(residuals(z5)^2)\n\n[1] 158081\n\n\n\n\n8.\n다음 자료는 권투선수 Mike Tyson과 Frank Bruno의 1989년 World Heavyweight Championship match의 자료이다. 타이슨이 이겼다. connected는 펀치를 정확히 맞힌 것이다. 누가 펀치를 많이 날렸는지? 누가 정확히 맞혔는지? 누가 잽(작은 펀치)을 많이 했는지? 누가 파워펀치를 많이 했는지? 등등을 분석하여 보아라.\n\ntyson=as.data.frame(matrix(c(43,39,28,37,55,13,14,13,15,34,6,9,5,13,8,0,1,1,3,3,37,30,23,24,47,13,13,12,12,31,1,0,0,0,0),nrow=7,byrow=T))\nnames(tyson)=1:5\nrownames(tyson)=c(\"Total punches\",\"Punches connected\",\"Jabs thrown\",\"Jabs conneted\",\"Power punches\",\"Power connected\",\"Knockdowns\")\ntyson\n\n                   1  2  3  4  5\nTotal punches     43 39 28 37 55\nPunches connected 13 14 13 15 34\nJabs thrown        6  9  5 13  8\nJabs conneted      0  1  1  3  3\nPower punches     37 30 23 24 47\nPower connected   13 13 12 12 31\nKnockdowns         1  0  0  0  0\n\nbruno=as.data.frame(matrix(c(55,42,35,18,20,14,8,5,3,7,14,24,16,8,6,2,1,0,0,1,41,18,19,10,14,12,7,5,3,6,0,0,0,0,0),nrow=7,byrow=T))           \nnames(bruno)=1:5              \nrownames(bruno)=c(\"Total punches\",\"Punches connected\",\"Jabs thrown\",\"Jabs conneted\",\"Power punches\",\"Power connected\",\"Knockdowns\")                   \nbruno   \n\n                   1  2  3  4  5\nTotal punches     55 42 35 18 20\nPunches connected 14  8  5  3  7\nJabs thrown       14 24 16  8  6\nJabs conneted      2  1  0  0  1\nPower punches     41 18 19 10 14\nPower connected   12  7  5  3  6\nKnockdowns         0  0  0  0  0\n\n\n\nlibrary(ghibli)\npal=ghibli_palette(\"PonyoMedium\",5)\nmosaicplot(tyson,col=pal,main=\"mosaic plot of tyson\")\n\n\n\nmosaicplot(bruno,col=pal,main=\"mosaic plot of bruno\")\n\n\n\n\n\nPunch\n\n\n# connected는 펀치를 정확히 맞힌 것이다. \n# 누가 펀치를 많이 했는지         \ntotalpunch=rbind(tyson[1,],bruno[1,])\nrownames(totalpunch)=c(\"tyson\",\"bruno\")\ntotalpunch\n\n       1  2  3  4  5\ntyson 43 39 28 37 55\nbruno 55 42 35 18 20\n\nchisq.test(totalpunch)\n\n\n    Pearson's Chi-squared test\n\ndata:  totalpunch\nX-squared = 22.67, df = 4, p-value = 0.0001473\n\n# Null hypothesis (H0): the row and the column variables of the contingency table are independent.\n# Alternative hypothesis (H1): row and column variables are dependent\n# 따라서, tyson의 round별 파워펀치 수와 bruno의 round별 파워펀치 수는 차이가 있다.\n\ntotalpunch_polished=medpolish(totalpunch)\n\n1: 80\n2: 73\nFinal: 73\n\ntotalpunch_polished\n\n\nMedian Polish Results (Dataset: \"totalpunch\")\n\nOverall: 37.5\n\nRow Effects:\ntyson bruno \n -1.5   1.5 \n\nColumn Effects:\n    1     2     3     4     5 \n 11.5   3.0  -6.0 -10.0   0.0 \n\nResiduals:\n         1 2  3   4   5\ntyson -4.5 0 -2  11  19\nbruno  4.5 0  2 -11 -19\n\nbarplot(totalpunch_polished$col) # 1라운드에서 펀치가 가장 많고 4라운드에서 펀치가 적다\n\n\n\nbarplot(totalpunch_polished$row) # Bruno가 Tyson에 비해 펀치를 많이 한 것으로 보인다.\n\n\n\nbarplot(totalpunch_polished$residuals)\n\n\n\nlibrary(reshape2)\ntotalpunch2=cbind(c(\"tyson\",\"bruno\"),totalpunch)\ntotalpunch2=melt(totalpunch2)\n\nUsing c(\"tyson\", \"bruno\") as id variables\n\nnames(totalpunch2)=c(\"player\",\"round\",\"value\")\ntotalpunch2\n\n   player round value\n1   tyson     1    43\n2   bruno     1    55\n3   tyson     2    39\n4   bruno     2    42\n5   tyson     3    28\n6   bruno     3    35\n7   tyson     4    37\n8   bruno     4    18\n9   tyson     5    55\n10  bruno     5    20\n\nlibrary(doBy)\nsummaryBy(value ~player, data=totalpunch2, FUN = c(mean,sum,min,max)) #평균적으로 bruno가 펀치의 횟수가 많았다.\n\n  player value.mean value.sum value.min value.max\n1  bruno       34.0       170        18        55\n2  tyson       40.4       202        28        55\n\nsummaryBy(value ~round, data=totalpunch2, FUN = c(mean,sum,min,max)) #라운드별로 보았을 때에는 1라운드에서 펀치횟수가 가장 많고 4라운드에서 펀치횟수가 가장 적다. \n\n  round value.mean value.sum value.min value.max\n1     1       49.0        98        43        55\n2     2       40.5        81        39        42\n3     3       31.5        63        28        35\n4     4       27.5        55        18        37\n5     5       37.5        75        20        55\n\nlibrary(ggplot2)\nggplot(totalpunch2,aes(x=as.factor(round),y=value))+\n  geom_bar(stat=\"identity\",position=\"dodge\",fill=rep(pal,2))+\n  facet_wrap(~ player, nrow=1)+\n  geom_text(aes(label=value), vjust=-0.4, size=3.5)+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Total Punch Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value\")\n\n\n\nggplot(totalpunch2, aes(round, value, fill=player))+\n  geom_bar(stat='identity', position = 'dodge')+\n  geom_text(aes(label= value),vjust=-0.4, size=3,position = position_dodge(width = 1))+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Total Punch Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value\")\n\n\n\naov10=aov(value~player+round,data=totalpunch2)\nsummary(aov10)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nplayer       1  102.4   102.4   0.517  0.512\nround        4  553.6   138.4   0.699  0.631\nResiduals    4  791.6   197.9               \n\nlibrary(agricolae)\nHSD.test(aov10, \"round\", group=T,console=TRUE)\n\n\nStudy: aov10 ~ \"round\"\n\nHSD Test for value \n\nMean Square Error:  197.9 \n\nround,  means\n\n  value       std r Min Max\n1  49.0  8.485281 2  43  55\n2  40.5  2.121320 2  39  42\n3  31.5  4.949747 2  28  35\n4  27.5 13.435029 2  18  37\n5  37.5 24.748737 2  20  55\n\nAlpha: 0.05 ; DF Error: 4 \nCritical Value of Studentized Range: 6.287027 \n\nMinimun Significant Difference: 62.53933 \n\nTreatments with the same letter are not significantly different.\n\n  value groups\n1  49.0      a\n2  40.5      a\n5  37.5      a\n3  31.5      a\n4  27.5      a\n\nHSD.test(aov10, \"player\", group=T,console=TRUE)\n\n\nStudy: aov10 ~ \"player\"\n\nHSD Test for value \n\nMean Square Error:  197.9 \n\nplayer,  means\n\n      value       std r Min Max\nbruno  34.0 15.475788 5  18  55\ntyson  40.4  9.838699 5  28  55\n\nAlpha: 0.05 ; DF Error: 4 \nCritical Value of Studentized Range: 3.926503 \n\nMinimun Significant Difference: 24.70267 \n\nTreatments with the same letter are not significantly different.\n\n      value groups\ntyson  40.4      a\nbruno  34.0      a\n\nlibrary(twoway)\ntwoway(totalpunch)\n\n\nMean decomposition (Dataset: \"totalpunch\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n         1     2     3     4     5       roweff\n       + ----- ----- ----- ----- ----- + ----- \ntyson  |  -9.2  -4.7  -6.7   6.3  14.3 :   3.2 \nbruno  |   9.2   4.7   6.7  -6.3 -14.3 :  -3.2 \n       + ..... ..... ..... ..... ..... + ..... \ncoleff |  11.8   3.3  -5.7  -9.7   0.3 :  37.2 \n\nplot(twoway(totalpunch))\n\n\n\n# 누가 정확히 맞혔는지? \npunch_accu=cbind((tyson[2,]/tyson[1,]),sum(tyson[2,])/sum(tyson[1,]))\nnames(punch_accu)=c(1:5,\"total\")\nrownames(punch_accu)=\"tyson_connect\"\npunch_accu=round(punch_accu*100,2)\n\npunch_accu1=cbind((bruno[2,]/bruno[1,]),sum(bruno[2,])/sum(bruno[1,]))\nnames(punch_accu1)=c(1:5,\"total\")\nrownames(punch_accu1)=\"bruno_connect\"\npunch_accu1=round(punch_accu1*100,2)\npunch_accu1\n\n                  1     2     3     4  5 total\nbruno_connect 25.45 19.05 14.29 16.67 35 21.76\n\nconnect_prob=rbind(punch_accu,punch_accu1)\nconnect_prob\n\n                  1     2     3     4     5 total\ntyson_connect 30.23 35.90 46.43 40.54 61.82 44.06\nbruno_connect 25.45 19.05 14.29 16.67 35.00 21.76\n\n# 전반적으로 정확히 맞춘 비율의 경우 Tyson이 Bruno에 비해서 높은 것으로 보인다.\n# Punch가 Connected될 비율은 모든 라운드에서 tyson이 bruno에 비해 높은 것으로 나타났으며 전체 비율의 경우에도 tyson이 bruno에 비해서 높게 나타난 것을 확인할 수 있다. \n\nconnect_prob=cbind(c(\"tyson_connect\",\"bruno_connect\"),connect_prob)\nconnect_prob2=melt(connect_prob)\n\nUsing c(\"tyson_connect\", \"bruno_connect\") as id variables\n\nnames(connect_prob2)=c(\"player\",\"round\",\"value\")\nggplot(connect_prob2, aes(round, value, fill=player))+\n  geom_bar(stat='identity', position = 'dodge')+\n  geom_text(aes(label= paste(sprintf(\"%2.1f\", value),\"%\",sep=\"\")),vjust=-0.4, size=3,position = position_dodge(width = 1))+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Punch Connect Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value(prop(%)\")\n\n\n\n# tyson이 bruno에 비해서 punch를 더 정확하게 하였다.\n\n\nJab\n\n\n# 누가 잽(작은 펀치)을 많이 했는지? \ntotaljab=rbind(tyson[3,],bruno[3,])\nrownames(totaljab)=c(\"tyson\",\"bruno\")\ntotaljab\n\n       1  2  3  4 5\ntyson  6  9  5 13 8\nbruno 14 24 16  8 6\n\nchisq.test(totaljab)\n\n\n    Pearson's Chi-squared test\n\ndata:  totaljab\nX-squared = 11.259, df = 4, p-value = 0.0238\n\n# Null hypothesis (H0): the row and the column variables of the contingency table are independent.\n# Alternative hypothesis (H1): row and column variables are dependent\n# 따라서, tyson의 round별 jab수와 bruno의 round별 jab수는 차이가 있다.\n\ntotaljab_polished=medpolish(totaljab)\n\n1: 35\n2: 33\nFinal: 33\n\ntotaljab_polished\n\n\nMedian Polish Results (Dataset: \"totaljab\")\n\nOverall: 10.5\n\nRow Effects:\ntyson bruno \n   -4     4 \n\nColumn Effects:\n   1    2    3    4    5 \n-0.5  6.0  0.0  0.0 -3.5 \n\nResiduals:\n      1    2    3    4  5\ntyson 0 -3.5 -1.5  6.5  5\nbruno 0  3.5  1.5 -6.5 -5\n\nbarplot(totaljab_polished$col) # 2라운드에서 Jab이 가장 많고 5라운드에서 Jab이 적게 나타난다. (5라운드에서 punch가 많았던 것에 비교하면 약간의 차이를 보인다) \n\n\n\nbarplot(totaljab_polished$row) # Bruno가 Tyson에 비해 Jab을 많이 한 것으로 보인다.\n\n\n\nbarplot(totaljab_polished$residuals)\n\n\n\ntotaljab2=cbind(c(\"tyson\",\"bruno\"),totaljab)\ntotaljab2=melt(totaljab2)\n\nUsing c(\"tyson\", \"bruno\") as id variables\n\nnames(totaljab2)=c(\"player\",\"round\",\"value\")\ntotaljab2\n\n   player round value\n1   tyson     1     6\n2   bruno     1    14\n3   tyson     2     9\n4   bruno     2    24\n5   tyson     3     5\n6   bruno     3    16\n7   tyson     4    13\n8   bruno     4     8\n9   tyson     5     8\n10  bruno     5     6\n\nlibrary(doBy)\nsummaryBy(value ~player, data=totaljab2, FUN = c(mean,sum,min,max)) #평균적으로 Bruno가 잽을 많이 했다.\n\n  player value.mean value.sum value.min value.max\n1  bruno       13.6        68         6        24\n2  tyson        8.2        41         5        13\n\nsummaryBy(value ~round, data=totaljab2, FUN = c(mean,sum,min,max)) #라운드별로 보았을 때에는 2라운드에서 Jab이 가장 많고 5라운드에서 Jab이 가장 적다. \n\n  round value.mean value.sum value.min value.max\n1     1       10.0        20         6        14\n2     2       16.5        33         9        24\n3     3       10.5        21         5        16\n4     4       10.5        21         8        13\n5     5        7.0        14         6         8\n\nggplot(totaljab2,aes(x=as.factor(round),y=value))+\n  geom_bar(stat=\"identity\",position=\"dodge\",fill=rep(pal,2))+\n  facet_wrap(~ player, nrow=1)+\n  geom_text(aes(label=value), vjust=-0.4, size=3.5)+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Total Jab Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value\")\n\n\n\nggplot(totaljab2, aes(round, value, fill=player))+\n  geom_bar(stat='identity', position = 'dodge')+\n  geom_text(aes(label= value),vjust=-0.4, size=3,position = position_dodge(width = 1))+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Total Jab Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value\")\n\n\n\naov11=aov(value~player+round,data=totaljab2)\nsummary(aov11)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nplayer       1   72.9   72.90   1.989  0.231\nround        4   95.4   23.85   0.651  0.656\nResiduals    4  146.6   36.65               \n\nHSD.test(aov11, \"round\", group=T,console=TRUE)\n\n\nStudy: aov11 ~ \"round\"\n\nHSD Test for value \n\nMean Square Error:  36.65 \n\nround,  means\n\n  value       std r Min Max\n1  10.0  5.656854 2   6  14\n2  16.5 10.606602 2   9  24\n3  10.5  7.778175 2   5  16\n4  10.5  3.535534 2   8  13\n5   7.0  1.414214 2   6   8\n\nAlpha: 0.05 ; DF Error: 4 \nCritical Value of Studentized Range: 6.287027 \n\nMinimun Significant Difference: 26.91332 \n\nTreatments with the same letter are not significantly different.\n\n  value groups\n2  16.5      a\n3  10.5      a\n4  10.5      a\n1  10.0      a\n5   7.0      a\n\nHSD.test(aov11, \"player\", group=T,console=TRUE)\n\n\nStudy: aov11 ~ \"player\"\n\nHSD Test for value \n\nMean Square Error:  36.65 \n\nplayer,  means\n\n      value      std r Min Max\nbruno  13.6 7.127412 5   6  24\ntyson   8.2 3.114482 5   5  13\n\nAlpha: 0.05 ; DF Error: 4 \nCritical Value of Studentized Range: 3.926503 \n\nMinimun Significant Difference: 10.6306 \n\nTreatments with the same letter are not significantly different.\n\n      value groups\nbruno  13.6      a\ntyson   8.2      a\n\nlibrary(twoway)\ntwoway(totaljab)\n\n\nMean decomposition (Dataset: \"totaljab\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n         1    2    3    4    5      roweff\n       + ---- ---- ---- ---- ---- + ----  \ntyson  | -1.3 -4.8 -2.8  5.2  3.7 : -2.7  \nbruno  |  1.3  4.8  2.8 -5.2 -3.7 :  2.7  \n       + .... .... .... .... .... + ....  \ncoleff | -0.9  5.6 -0.4 -0.4 -3.9 : 10.9  \n\nplot(twoway(totaljab))\n\n\n\n# 누가 정확히 맞혔는지? \njab_accu=cbind((tyson[4,]/tyson[3,]),sum(tyson[4,])/sum(tyson[3,]))\nnames(jab_accu)=c(1:5,\"total\")\nrownames(jab_accu)=\"tyson_connect\"\njab_accu=round(jab_accu*100,2)\n\njab_accu1=cbind((bruno[4,]/bruno[3,]),sum(bruno[4,])/sum(bruno[3,]))\nnames(jab_accu1)=c(1:5,\"total\")\nrownames(jab_accu1)=\"bruno_connect\"\njab_accu1=round(jab_accu1*100,2)\njab_accu1\n\n                  1    2 3 4     5 total\nbruno_connect 14.29 4.17 0 0 16.67  5.88\n\njconnect_prob=rbind(jab_accu,jab_accu1)\njconnect_prob\n\n                  1     2  3     4     5 total\ntyson_connect  0.00 11.11 20 23.08 37.50 19.51\nbruno_connect 14.29  4.17  0  0.00 16.67  5.88\n\n# 전반적으로 정확히 맞춘 비율의 경우 Tyson이 Bruno에 비해서 높은 것으로 보인다.\n# Punch의 Connected될 비율은 1라운드를 제외하면 tyson이 bruno에 비해 높은 것으로 나타났으며 전체 비율의 경우에도 tyson이 bruno에 비해서 높게 나타난 것을 확인할 수 있다. \n\njconnect_prob=cbind(c(\"tyson_connect\",\"bruno_connect\"),jconnect_prob)\njconnect_prob2=melt(jconnect_prob)\n\nUsing c(\"tyson_connect\", \"bruno_connect\") as id variables\n\nnames(jconnect_prob2)=c(\"player\",\"round\",\"value\")\nggplot(jconnect_prob2, aes(round, value, fill=player))+\n  geom_bar(stat='identity', position = 'dodge')+\n  geom_text(aes(label= paste(sprintf(\"%2.1f\", value),\"%\",sep=\"\")),vjust=-0.4, size=3,position = position_dodge(width = 1))+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Jab Punch Connect Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value(prop(%)\")\n\n\n\n# tyson이 bruno에 비해서 더 정확한 Jab 펀치를 하였다.\n\n\npower punch\n\n\n# 누가 파워펀치를 많이 했는지         \ntotalpower=rbind(tyson[5,],bruno[5,])\nrownames(totalpower)=c(\"tyson\",\"bruno\")\ntotalpower\n\n       1  2  3  4  5\ntyson 37 30 23 24 47\nbruno 41 18 19 10 14\n\nchisq.test(totalpower)\n\n\n    Pearson's Chi-squared test\n\ndata:  totalpower\nX-squared = 14.708, df = 4, p-value = 0.005348\n\n# Null hypothesis (H0): the row and the column variables of the contingency table are independent.\n# Alternative hypothesis (H1): row and column variables are dependent\n# 따라서, tyson의 round별 파워펀치 수와 bruno의 round별 파워펀치 수는 차이가 있다.\n\ntotalpower_polished=medpolish(totalpower)\n\n1: 47\nFinal: 47\n\ntotalpower_polished\n\n\nMedian Polish Results (Dataset: \"totalpower\")\n\nOverall: 24\n\nRow Effects:\ntyson bruno \n    6    -6 \n\nColumn Effects:\n   1    2    3    4    5 \n15.0  0.0 -3.0 -7.0  6.5 \n\nResiduals:\n       1 2  3  4     5\ntyson -8 0 -4  1  10.5\nbruno  8 0  4 -1 -10.5\n\nbarplot(totalpower_polished$col) # 1라운드에서 파워펀치가 가장 많고 점점 줄다가 5라운드에서 다시 늘어남 \n\n\n\nbarplot(totalpower_polished$row) # Tyson이 Bruno에 비해 파워펀치를 많이 한 것으로 보인다.\n\n\n\nbarplot(totalpower_polished$residuals)\n\n\n\ntotalpower2=cbind(c(\"tyson\",\"bruno\"),totalpower)\ntotalpower2=melt(totalpower2)\n\nUsing c(\"tyson\", \"bruno\") as id variables\n\nnames(totalpower2)=c(\"player\",\"round\",\"value\")\ntotalpower2\n\n   player round value\n1   tyson     1    37\n2   bruno     1    41\n3   tyson     2    30\n4   bruno     2    18\n5   tyson     3    23\n6   bruno     3    19\n7   tyson     4    24\n8   bruno     4    10\n9   tyson     5    47\n10  bruno     5    14\n\nlibrary(doBy)\nsummaryBy(value ~player, data=totalpower2, FUN = c(mean,sum,min,max)) #평균적으로 tyson이 파워펀치가 많았다.\n\n  player value.mean value.sum value.min value.max\n1  bruno       20.4       102        10        41\n2  tyson       32.2       161        23        47\n\nsummaryBy(value ~round, data=totalpower2, FUN = c(mean,sum,min,max)) #라운드별로 보았을 때에는 1라운드에서 파워펀치가 가장 많고 4라운드에서 파워펀치가 가장 적다. \n\n  round value.mean value.sum value.min value.max\n1     1       39.0        78        37        41\n2     2       24.0        48        18        30\n3     3       21.0        42        19        23\n4     4       17.0        34        10        24\n5     5       30.5        61        14        47\n\nggplot(totalpower2,aes(x=as.factor(round),y=value))+\n  geom_bar(stat=\"identity\",position=\"dodge\",fill=rep(pal,2))+\n  facet_wrap(~ player, nrow=1)+\n  geom_text(aes(label=value), vjust=-0.4, size=3.5)+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Total Power Punch Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value\")\n\n\n\nggplot(totalpower2, aes(round, value, fill=player))+\n  geom_bar(stat='identity', position = 'dodge')+\n  geom_text(aes(label= value),vjust=-0.4, size=3,position = position_dodge(width = 1))+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Total Power Punch Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value\")\n\n\n\naov12=aov(value~player+round,data=totalpower2)\nsummary(aov12)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nplayer       1  348.1   348.1   3.641  0.129\nround        4  597.6   149.4   1.563  0.338\nResiduals    4  382.4    95.6               \n\nHSD.test(aov12, \"round\", group=T,console=TRUE)\n\n\nStudy: aov12 ~ \"round\"\n\nHSD Test for value \n\nMean Square Error:  95.6 \n\nround,  means\n\n  value       std r Min Max\n1  39.0  2.828427 2  37  41\n2  24.0  8.485281 2  18  30\n3  21.0  2.828427 2  19  23\n4  17.0  9.899495 2  10  24\n5  30.5 23.334524 2  14  47\n\nAlpha: 0.05 ; DF Error: 4 \nCritical Value of Studentized Range: 6.287027 \n\nMinimun Significant Difference: 43.46696 \n\nTreatments with the same letter are not significantly different.\n\n  value groups\n1  39.0      a\n5  30.5      a\n2  24.0      a\n3  21.0      a\n4  17.0      a\n\nHSD.test(aov12, \"player\", group=T,console=TRUE)\n\n\nStudy: aov12 ~ \"player\"\n\nHSD Test for value \n\nMean Square Error:  95.6 \n\nplayer,  means\n\n      value       std r Min Max\nbruno  20.4 12.054045 5  10  41\ntyson  32.2  9.984989 5  23  47\n\nAlpha: 0.05 ; DF Error: 4 \nCritical Value of Studentized Range: 3.926503 \n\nMinimun Significant Difference: 17.16919 \n\nTreatments with the same letter are not significantly different.\n\n      value groups\ntyson  32.2      a\nbruno  20.4      a\n\nlibrary(twoway)\ntwoway(totalpower)\n\n\nMean decomposition (Dataset: \"totalpower\"; Response: Value)\nResiduals bordered by row effects, column effects, and overall\n\n         1     2     3     4     5       roweff\n       + ----- ----- ----- ----- ----- + ----- \ntyson  |  -7.9   0.1  -3.9   1.1  10.6 :   5.9 \nbruno  |   7.9  -0.1   3.9  -1.1 -10.6 :  -5.9 \n       + ..... ..... ..... ..... ..... + ..... \ncoleff |  12.7  -2.3  -5.3  -9.3   4.2 :  26.3 \n\nplot(twoway(totalpower))\n\n\n\n# 누가 정확히 맞혔는지? \npower_accu=cbind((tyson[6,]/tyson[5,]),sum(tyson[6,])/sum(tyson[5,]))\nnames(power_accu)=c(1:5,\"total\")\nrownames(power_accu)=\"tyson_connect\"\npower_accu=round(power_accu*100,2)\n\npower_accu1=cbind((bruno[6,]/bruno[5,]),sum(bruno[6,])/sum(bruno[5,]))\nnames(power_accu1)=c(1:5,\"total\")\nrownames(power_accu1)=\"bruno_connect\"\npower_accu1=round(power_accu1*100,2)\npower_accu1\n\n                  1     2     3  4     5 total\nbruno_connect 29.27 38.89 26.32 30 42.86 32.35\n\npconnect_prob=rbind(power_accu,power_accu1)\npconnect_prob\n\n                  1     2     3  4     5 total\ntyson_connect 35.14 43.33 52.17 50 65.96 50.31\nbruno_connect 29.27 38.89 26.32 30 42.86 32.35\n\n# 전반적으로 정확히 맞춘 비율의 경우 Tyson이 Bruno에 비해서 높은 것으로 보인다.\n# Punch의 Connected될 비율은 1라운드를 제외하면 tyson이 bruno에 비해 높은 것으로 나타났으며 전체 비율의 경우에도 tyson이 bruno에 비해서 높게 나타난 것을 확인할 수 있다. \n\npconnect_prob=cbind(c(\"tyson_connect\",\"bruno_connect\"),pconnect_prob)\npconnect_prob2=melt(pconnect_prob)\n\nUsing c(\"tyson_connect\", \"bruno_connect\") as id variables\n\nnames(pconnect_prob2)=c(\"player\",\"round\",\"value\")\nggplot(pconnect_prob2, aes(round, value, fill=player))+\n  geom_bar(stat='identity', position = 'dodge')+\n  geom_text(aes(label= paste(sprintf(\"%2.1f\", value),\"%\",sep=\"\")),vjust=-0.4, size=3,position = position_dodge(width = 1))+\n  theme_classic()+\n  ggtitle(\"Bruno & Tyson Power Punch Connect Comparison\")+\n  xlab(\"round\")+\n  ylab(\"value(prop(%)\")\n\n\n\n# tyson이 bruno에 비해서 power punch를 더 정확하게 하였다."
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html",
    "title": "Assignment 1",
    "section": "",
    "text": "ML Basics, Regression, Probit, Logistic, Softmax, Ridge, Lasso (Score: 96/100)"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#loading-dataset",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#loading-dataset",
    "title": "Assignment 1",
    "section": "Loading Dataset",
    "text": "Loading Dataset\n\n\nCode\n# Set Work Directory\nprint(\"Before Directory: %s\"%os.getcwd())\nos.chdir(\"E:/OneDrive - SNU/r\")\nprint(\"After Directory: %s\"%os.getcwd())\n\n\nBefore Directory: e:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제1\nAfter Directory: E:\\OneDrive - SNU\\r\n\n\n\n\nCode\n# Read Dataset\ncarseats=pd.read_csv(\"Carseats.csv\")\ndisplay(carseats.head(5))\n# Check if there is missing value\nprint(\"Total missing variable: \",carseats.isnull().sum().sum()) # no missing value\n\n\n\n\n\n\n  \n    \n      \n      Sales\n      CompPrice\n      Income\n      Advertising\n      Population\n      Price\n      ShelveLoc\n      Age\n      Education\n      Urban\n      US\n    \n  \n  \n    \n      0\n      9.50\n      138\n      73\n      11\n      276\n      120\n      Bad\n      42\n      17\n      Yes\n      Yes\n    \n    \n      1\n      11.22\n      111\n      48\n      16\n      260\n      83\n      Good\n      65\n      10\n      Yes\n      Yes\n    \n    \n      2\n      10.06\n      113\n      35\n      10\n      269\n      80\n      Medium\n      59\n      12\n      Yes\n      Yes\n    \n    \n      3\n      7.40\n      117\n      100\n      4\n      466\n      97\n      Medium\n      55\n      14\n      Yes\n      Yes\n    \n    \n      4\n      4.15\n      141\n      64\n      3\n      340\n      128\n      Bad\n      38\n      13\n      Yes\n      No\n    \n  \n\n\n\n\nTotal missing variable:  0\n\n\n\n\nCode\n# Data types\ncarseats.info() \n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 400 entries, 0 to 399\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Sales        400 non-null    float64\n 1   CompPrice    400 non-null    int64  \n 2   Income       400 non-null    int64  \n 3   Advertising  400 non-null    int64  \n 4   Population   400 non-null    int64  \n 5   Price        400 non-null    int64  \n 6   ShelveLoc    400 non-null    object \n 7   Age          400 non-null    int64  \n 8   Education    400 non-null    int64  \n 9   Urban        400 non-null    object \n 10  US           400 non-null    object \ndtypes: float64(1), int64(7), object(3)\nmemory usage: 34.5+ KB"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#a",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#a",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\n[10 pts] Fit a multiple linear regression model to predict Sales using Price, Urban, and US. Report the R2 of the model.\n\n\nCode\n# Sales: float, Price: income, Urban and US: Categorical variable\ncarseats.US=pd.Categorical(carseats.US, categories=[\"No\", \"Yes\"])\ncarseats.Urban=pd.Categorical(carseats.Urban, categories=[\"No\", \"Yes\"])\n\n\n\n\nCode\n# Fit a multiple linear regression model\nestimate = smf.ols('Sales ~  Urban + US + Price', carseats).fit()\nestimate.summary()\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          Sales        R-squared:             0.239\n\n\n  Model:                   OLS         Adj. R-squared:        0.234\n\n\n  Method:             Least Squares    F-statistic:           41.52\n\n\n  Date:             Sun, 09 Oct 2022   Prob (F-statistic): 2.39e-23\n\n\n  Time:                 09:07:43       Log-Likelihood:      -927.66\n\n\n  No. Observations:         400        AIC:                   1863.\n\n\n  Df Residuals:             396        BIC:                   1879.\n\n\n  Df Model:                   3                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                  coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept       13.0435     0.651    20.036  0.000    11.764    14.323\n\n\n  Urban[T.Yes]    -0.0219     0.272    -0.081  0.936    -0.556     0.512\n\n\n  US[T.Yes]        1.2006     0.259     4.635  0.000     0.691     1.710\n\n\n  Price           -0.0545     0.005   -10.389  0.000    -0.065    -0.044\n\n\n\n\n  Omnibus:        0.676   Durbin-Watson:         1.912\n\n\n  Prob(Omnibus):  0.713   Jarque-Bera (JB):      0.758\n\n\n  Skew:           0.093   Prob(JB):              0.684\n\n\n  Kurtosis:       2.897   Cond. No.               628.\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCode\n# Report R^2 of the model\nprint('R^2: ', round(estimate.rsquared,3))\n\n\nR^2:  0.239"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#b",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#b",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n[5 pts] Write out the model in equation form, being careful to handle the qualitative variables properly. Provide an interpretation of each coefficient in the model.\n\\[ Y= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon \\,, \\epsilon \\sim \\,iid  \\, N(0,\\sigma^2)  \\] \\[ E(Y)= 13.0435 - 0.0219 X_1 + 1.2006 X_2 - 0.0545 X_3 \\]\nwhere\n\\[ Y = Sales \\]\n\\[ \\begin{equation*}\nX_1 = \\begin{cases}\n             1  & \\text{if }\\,Urban=Yes \\\\\n             0  & \\text{if }\\,Urban=No\n       \\end{cases}\n\\end{equation*}\n\\]\n\\[\n\\begin{equation*}\nX_2 = \\begin{cases}\n             1  & \\text{if }\\,US=Yes \\\\\n             0  & \\text{if }\\,US=No\n       \\end{cases}\n\\end{equation*}\n\\]\n\\[ X_3 = Price \\]\n\nInterpreation of the coefficient\n\n\n\\(\\beta_0\\) 13.0435: The overall average value of the Sales is 13.0435, if all other variables are equal to zero ( $ X_1,, X_2,, X_3 =0$ )\n\\(\\beta_1\\) -0.0219: The average Sales difference between Urban and Non Urban customers is estimated as -0.0219. (Even though the point estimates of the regression coefficient not equal to zero, the parameter estimate is not statistically significant under significance level of 0.05. Can’t reject null hypothesis of \\(\\beta_1=0\\))\n\\(\\beta_2\\) 1.2006 : The average Sales difference between US and Non US customers is estimated as 1.2006.\n\\(\\beta_3\\) -0.0545 : Under \\(X_1,\\, X_2\\) (Urban and US dummy) are controlled, the one unit increase in Price results in -0.545 unit of Sales decrease."
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#c",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\n[5 pts] For which predictor variable j can you reject the null hypothesis H0 : \\(β_j\\) = 0? for which there is evidence of association with the outcome.\n\nUnder the significance level of 0.05, the parameter \\(\\beta_1\\) estimate for the variable \\(X_1\\) (Urban or Non-Urban) is not statistically significant as p-value is over 0.05 (0.936) and t-statistic is under critical region.\nNull: \\(H_0\\): \\(\\beta_1\\) = 0\nAlternative: \\(H_0\\): \\(\\beta_1\\) \\(\\neq\\) 0"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#d",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#d",
    "title": "Assignment 1",
    "section": "(d)",
    "text": "(d)\n[10 pts] Obtain 95% confidence intervals for the coefficient(s).\n\n\nCode\nestimate.conf_int(alpha=0.05, cols=None).rename({0: 'Lower CI', 1: 'Upper CI'}, axis='columns')\n\n\n\n\n\n\n  \n    \n      \n      Lower CI\n      Upper CI\n    \n  \n  \n    \n      Intercept\n      11.763597\n      14.323341\n    \n    \n      Urban[T.Yes]\n      -0.555973\n      0.512141\n    \n    \n      US[T.Yes]\n      0.691304\n      1.709841\n    \n    \n      Price\n      -0.064764\n      -0.044154"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#loading-dataset-1",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#loading-dataset-1",
    "title": "Assignment 1",
    "section": "Loading Dataset",
    "text": "Loading Dataset\n\n\nCode\nimport random\nrandom.seed(2022)\n\n# Read Dataset\ndefault=pd.read_csv(\"Default.csv\")\ndisplay(default.head(5))\n# Check if there is missing value\nprint(\"Total missing variable: \", default.isnull().sum().sum()) # no missing value\n\n\n\n\n\n\n  \n    \n      \n      default\n      student\n      balance\n      income\n    \n  \n  \n    \n      0\n      No\n      No\n      729.526495\n      44361.625074\n    \n    \n      1\n      No\n      Yes\n      817.180407\n      12106.134700\n    \n    \n      2\n      No\n      No\n      1073.549164\n      31767.138947\n    \n    \n      3\n      No\n      No\n      529.250605\n      35704.493935\n    \n    \n      4\n      No\n      No\n      785.655883\n      38463.495879\n    \n  \n\n\n\n\nTotal missing variable:  0"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#a-1",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#a-1",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\n[10 pts] Fit a logistic regression model that uses income and balance to predict default. Report the log-likelihood of the model.\n\n\nCode\ndefault.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   default  10000 non-null  object \n 1   student  10000 non-null  object \n 2   balance  10000 non-null  float64\n 3   income   10000 non-null  float64\ndtypes: float64(2), object(2)\nmemory usage: 312.6+ KB\n\n\n\n\nCode\ndisplay(default['default'].value_counts())\ndisplay(default['student'].value_counts())\n\n\nNo     9667\nYes     333\nName: default, dtype: int64\n\n\nNo     7056\nYes    2944\nName: student, dtype: int64\n\n\n\n\nCode\n# Make dummy variable - No=0, Yes=1\ndefault.default=pd.get_dummies(default.default, drop_first=True)\ndefault.student=pd.get_dummies(default.student, drop_first=True)\ndisplay(default['default'].value_counts())\ndisplay(default['student'].value_counts())\n\n\n0    9667\n1     333\nName: default, dtype: int64\n\n\n0    7056\n1    2944\nName: student, dtype: int64\n\n\n\n\nCode\nfig = plt.figure(figsize=(10,5))\ngs = mpl.gridspec.GridSpec(1, 2)\nax1 = plt.subplot(gs[0])\nax2 = plt.subplot(gs[1])\n\npal = {0 :'red', 1:'blue'}\nsns.boxplot(x='default', y='balance', data=default, orient='vertical', ax=ax1, palette=pal)\nsns.boxplot(x='default', y='income', data=default, orient='vertical', ax=ax2, palette=pal)\ngs.tight_layout(plt.gcf())\n\n\n\n\n\n\n\n\nCode\nestimate2 = smf.logit('default ~  income + balance', data=default).fit()\nestimate2.summary()\n\n\nOptimization terminated successfully.\n         Current function value: 0.078948\n         Iterations 10\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:        default       No. Observations:      10000  \n\n\n  Model:                 Logit        Df Residuals:           9997  \n\n\n  Method:                 MLE         Df Model:                  2  \n\n\n  Date:            Sun, 09 Oct 2022   Pseudo R-squ.:        0.4594  \n\n\n  Time:                09:07:44       Log-Likelihood:       -789.48 \n\n\n  converged:             True         LL-Null:              -1460.3 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        4.541e-292\n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept   -11.5405     0.435   -26.544  0.000   -12.393   -10.688\n\n\n  income     2.081e-05  4.99e-06     4.174  0.000   1.1e-05  3.06e-05\n\n\n  balance       0.0056     0.000    24.835  0.000     0.005     0.006\n\nPossibly complete quasi-separation: A fraction 0.14 of observations can beperfectly predicted. This might indicate that there is completequasi-separation. In this case some parameters will not be identified.\n\n\n\n\nCode\nprint(\"Log-likelihood of logit model: \", round(estimate2.llf,3))\n\n\nLog-likelihood of logit model:  -789.483\n\n\n\n\nCode\nestimate2.params\n\n\nIntercept   -11.540468\nincome        0.000021\nbalance       0.005647\ndtype: float64\n\n\n\n\nCode\n# Calculate Odds\nodds_ratio = pd.DataFrame( {\"Odds Ratio\": estimate2.params,\"Lower CI\": estimate2.conf_int()[0],\"Upper CI\": estimate2.conf_int()[1]})\nodds_ratio = np.exp(odds_ratio)\ndisplay (odds_ratio)\nintercept=odds_ratio.iloc[0,0]\nP = (intercept) / (1 + intercept) \nprint(\"p_o: \", round(P,15)) \n\n\n\n\n\n\n  \n    \n      \n      Odds Ratio\n      Lower CI\n      Upper CI\n    \n  \n  \n    \n      Intercept\n      0.000010\n      0.000004\n      0.000023\n    \n    \n      income\n      1.000021\n      1.000011\n      1.000031\n    \n    \n      balance\n      1.005663\n      1.005215\n      1.006111\n    \n  \n\n\n\n\np_o:  9.728234005e-06\n\n\n\n\nCode\nodds_ratio.iloc[0,0]\n\n\n9.728328644686842e-06"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#b-1",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#b-1",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n[5 pts] Write out the model in equation form and provide an interpretation of each coefficient in the trained model.\n\\[ P(Y=1|X_1, X_2) = p = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 )}{1 + \\exp (\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 )}  \\] \\[ \\ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2  = -11.5405 + 0.00002 X_1 + 0.0056 X_2  \\]\nwhere\n\\[\n\\begin{equation*}\nY = \\begin{cases}\n             1  & \\text{if }\\,Default=Yes \\\\\n             0  & \\text{if }\\,Default=No \\\\\n       \\end{cases}\n\\end{equation*} \\quad\n\\] \\[\nX_1 = Income,\\,X_2 = Balance\n\\]\n\nInterpreation of the coefficient\n\n\n\\(\\beta_0\\)= -11.5405 (odds ratio: 0.000010): The probability of having the outcome Default is 0.00000972823, if all other variables are equal to zero ($ X_1, , X_2 = 0 $)\n\\(\\beta_1\\)= 0.00002 (odds ratio: 1.000021): The one unit increase in Income, results in 1.000021 times increase in odds ratio of Default (increases odds of default by 0.0021%)\n\\(\\beta_2\\)= 0.0056 (odds ratio: 1.005663): The one unit increase in Balance, results in 1.005663 times increase in odds ratio of Default (increases odds of default by 0.5663%)"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#c-1",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#c-1",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\n[5 pts] Perform 5-fold cross-validation using the model in Part (a), and estimate the test error of this model.\n\n\nCode\nx = np.array(default.loc[ : , ['income','balance']]) # make array\ny = default.loc[:,'default']\n\n# Five-Fold cross-validation\nk_fold=KFold(n_splits = 5, shuffle = True, random_state=2022)\n\n\nTest error was calculated using Mean Squared Error \\[ \\text {Test Error of 5 folds: } CV_5 = \\sum_{k=1}^{5} \\frac{n_k}{n} MSE_k \\]\n\n\nCode\ntest_error = []\n\nfor train_index, test_index in k_fold.split(x):\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model = LogisticRegression() # Logistic Regression\n    model.fit(x_train, y_train) # Model Training\n    y_test_pred = model.predict(x_test) \n    test_error.append(1-accuracy_score(y_test_pred, y_test)) # Test Error of each five folds\n\n# Estimating Test Error\nprint(\"The test error of each five folds: \", np.round(test_error,3))\n\n\nThe test error of each five folds:  [0.031 0.025 0.026 0.029 0.031]\n\n\n\n\nCode\nmodel = LogisticRegression() # Logistic Regression\ntest_error=cross_val_score(model, x, y, cv=k_fold, scoring='neg_mean_squared_error', n_jobs=-1)\n\n\n\n\nCode\nsize=[]\nfor train_index, test_index in k_fold.split(x):\n    size.append(np.size(test_index))\nprint(\"The observations per each split: \", size)\n\n# Using formula\ntesterror=[]\nfor folds in np.arange(0,5):\n    testerror.append(np.negative(test_error)[folds]*size[folds])\ntesterrorvalue=round(sum(testerror)/len(default),4)\nprint(\"Test error of cross validation\", testerrorvalue)\n\n\nThe observations per each split:  [2000, 2000, 2000, 2000, 2000]\nTest error of cross validation 0.0286"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#d-1",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#d-1",
    "title": "Assignment 1",
    "section": "(d)",
    "text": "(d)\n[10 pts] Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the 5-fold cross-validation set approach. Comment on whether or not including a dummy variable for student would lead to a reduction in the test error rate.\n\n\nCode\nx1 = default.loc[ : , ['income','balance','student']] # make array\ny1 = default.loc[:,'default']\n\n# Five-Fold cross-validation\nk_fold=KFold(n_splits = 5, shuffle = True,random_state=2022)\n\n\n\n\nCode\nmodel = LogisticRegression() # Logistic Regression\ntest_error_withstu=cross_val_score(model, x1, y1, cv=k_fold, scoring='neg_mean_squared_error', n_jobs=-1)\n\n\n\n\nCode\nsize=[]\nfor train_index, test_index in k_fold.split(x):\n    size.append(np.size(test_index))\nprint(\"The observations per each split: \", size)\n\n# Using formula\ntesterror=[]\nfor folds in np.arange(0,5):\n    testerror.append(np.negative(test_error_withstu)[folds]*size[folds])\ntesterrorvalue_withstudents=round(sum(testerror)/len(default),4)\nprint(\"Test error of cross validation with students\", testerrorvalue_withstudents)\n\n\nThe observations per each split:  [2000, 2000, 2000, 2000, 2000]\nTest error of cross validation with students 0.0317\n\n\nIncluding a dummy variable for student would lead to almost no change in the test error rate. - Firstly, the strong negative linear correlation between student and income is statistically significant. Since the two variables are linearly correlatedm, even if the varaible student is inserted to the model it would lead to minor improvement in model prediction. (most of the areas that variable student can explain is also can be explained by variable income) - Secondly, the model’s predictive ability would be negatively affected by statistically learning from train-dataset. Being a student and whether client would be default is correlated and it is statistically significant. As we use generalized linear model there is high possibility that “multicollinearity” would make parameter estimate unstable.\n\nAs expected, the test error value with student variable is little(0.0001) higher than without student varaible.\n\n\n\nCode\nprint(\"Test error of cross validation\", testerrorvalue)\nprint(\"Test error of cross validation with students\", testerrorvalue_withstudents)\n\n# Correlation \nrho = default.corr()\npval = default.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\np = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\ndisplay(rho.round(2).astype(str) + p) \n# significant negative linear relationship between student and income (rho=-0.75)\n\n# Model including student variable\nestimate3 = smf.logit('default ~  income + balance + student', data=default).fit()\ndisplay(estimate3.summary2().tables[1])\n\n\n\nTest error of cross validation 0.0286\nTest error of cross validation with students 0.0317\n\n\n\n\n\n\n  \n    \n      \n      default\n      student\n      balance\n      income\n    \n  \n  \n    \n      default\n      1.0***\n      0.04***\n      0.35***\n      -0.02**\n    \n    \n      student\n      0.04***\n      1.0***\n      0.2***\n      -0.75***\n    \n    \n      balance\n      0.35***\n      0.2***\n      1.0***\n      -0.15***\n    \n    \n      income\n      -0.02**\n      -0.75***\n      -0.15***\n      1.0***\n    \n  \n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.078577\n         Iterations 10\n\n\n\n\n\n\n  \n    \n      \n      Coef.\n      Std.Err.\n      z\n      P>|z|\n      [0.025\n      0.975]\n    \n  \n  \n    \n      Intercept\n      -10.869045\n      0.492273\n      -22.079320\n      4.995499e-108\n      -11.833882\n      -9.904209\n    \n    \n      income\n      0.000003\n      0.000008\n      0.369808\n      7.115254e-01\n      -0.000013\n      0.000019\n    \n    \n      balance\n      0.005737\n      0.000232\n      24.736506\n      4.331521e-135\n      0.005282\n      0.006191\n    \n    \n      student\n      -0.646776\n      0.236257\n      -2.737595\n      6.189022e-03\n      -1.109831\n      -0.183721\n    \n  \n\n\n\n\ncorrelation 출처: https://stackoverflow.com/questions/25571882/pandas-columns-correlation-with-statistical-significance"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#loading-dataset-2",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#loading-dataset-2",
    "title": "Assignment 1",
    "section": "Loading Dataset",
    "text": "Loading Dataset\n\n\nCode\nimport random\nrandom.seed(2022)\n\n# Read Dataset\ncollege=pd.read_csv(\"College.csv\")\ndisplay(college.head(5))\n# Check if there is missing value\nprint(\"Total missing variable: \", college.isnull().sum().sum()) # no missing value\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Private\n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n    \n  \n  \n    \n      0\n      Abilene Christian University\n      Yes\n      1660\n      1232\n      721\n      23\n      52\n      2885\n      537\n      7440\n      3300\n      450\n      2200\n      70\n      78\n      18.1\n      12\n      7041\n      60\n    \n    \n      1\n      Adelphi University\n      Yes\n      2186\n      1924\n      512\n      16\n      29\n      2683\n      1227\n      12280\n      6450\n      750\n      1500\n      29\n      30\n      12.2\n      16\n      10527\n      56\n    \n    \n      2\n      Adrian College\n      Yes\n      1428\n      1097\n      336\n      22\n      50\n      1036\n      99\n      11250\n      3750\n      400\n      1165\n      53\n      66\n      12.9\n      30\n      8735\n      54\n    \n    \n      3\n      Agnes Scott College\n      Yes\n      417\n      349\n      137\n      60\n      89\n      510\n      63\n      12960\n      5450\n      450\n      875\n      92\n      97\n      7.7\n      37\n      19016\n      59\n    \n    \n      4\n      Alaska Pacific University\n      Yes\n      193\n      146\n      55\n      16\n      44\n      249\n      869\n      7560\n      4120\n      800\n      1500\n      76\n      72\n      11.9\n      2\n      10922\n      15\n    \n  \n\n\n\n\nTotal missing variable:  0\n\n\nRandomly split the data set into a training set and a test set by 90:10 ratio.  Data description: https://www.kaggle.com/faressayah/college-data\n\n\nCode\ncollege.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 777 entries, 0 to 776\nData columns (total 19 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Unnamed: 0   777 non-null    object \n 1   Private      777 non-null    object \n 2   Apps         777 non-null    int64  \n 3   Accept       777 non-null    int64  \n 4   Enroll       777 non-null    int64  \n 5   Top10perc    777 non-null    int64  \n 6   Top25perc    777 non-null    int64  \n 7   F.Undergrad  777 non-null    int64  \n 8   P.Undergrad  777 non-null    int64  \n 9   Outstate     777 non-null    int64  \n 10  Room.Board   777 non-null    int64  \n 11  Books        777 non-null    int64  \n 12  Personal     777 non-null    int64  \n 13  PhD          777 non-null    int64  \n 14  Terminal     777 non-null    int64  \n 15  S.F.Ratio    777 non-null    float64\n 16  perc.alumni  777 non-null    int64  \n 17  Expend       777 non-null    int64  \n 18  Grad.Rate    777 non-null    int64  \ndtypes: float64(1), int64(16), object(2)\nmemory usage: 115.5+ KB\n\n\n\n\nCode\n# Make dummy variable - No=0, Yes=1\n# Priavate: A factor with levels Yes and No indicating private or public university\ncollege.Private=pd.get_dummies(college.Private, drop_first=True)\ndisplay(college['Private'].value_counts())\n\n\n1    565\n0    212\nName: Private, dtype: int64\n\n\n\n\nCode\n# Accept: Number of applications accepted\ny=college.iloc[:,3]\n\n# Sclaing - Standardization of Predictors\nscaler = StandardScaler()  \ntrain_col=np.array(college.columns)\nx1 = college[np.delete(train_col,[0,3]).tolist()]\n\nx=pd.DataFrame(scaler.fit_transform(x1[x1.columns[range(1,x1.shape[1])]].to_numpy()))\nx.columns=x1.columns[range(1,x1.shape[1])]\nx.insert(loc=0,column=\"Private\", value=x1[\"Private\"])\n\n# train test split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True)\n\n\nRidge: \\[ \\sum_{i=1}^{n} (y_i-\\beta_0-\\sum_{j=1}^p \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2 \\]\nLasso: \\[ \\sum_{i=1}^{n} (y_i-\\beta_0-\\sum_{j=1}^p \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j| \\]\nIn this sense, \\(\\hat{\\beta^R_{i,j}}\\) will depend not only on the value of \\(\\lambda\\), but also on the scaling of the jth predictor (and other predictors). (RSS + \\(\\beta\\))  Therefore, it is best to apply ridge regression after \\(standardizing\\,the\\,predictors\\) using the formula \\[\\tilde{x_{ij}}=\\frac{x_ij}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(\\bar{x_{ij}}-\\bar{x_j})^2}}\\] (ISLR, p. 239)\n\n\nCode\n# Dataframe Dimensions\nprint(\"# of rows: \", college.shape[0])\nprint(\"# of train rows: \", x_train.shape[0])\nprint(\"# of test rows: \", x_test.shape[0])\n\n\n# of rows:  777\n# of train rows:  699\n# of test rows:  78"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#a-2",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#a-2",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\n[10 pts] Fit a linear model using least squares on the training set, and report the test error obtained.\nThe test error was calculated using Mean Squared Error \\[ MSE_{train} = \\frac{1}{699} \\sum_{i=1}^{699} (\\hat{Y_i} - Y_i)^2\\] \\[ MSE_{test} = \\frac{1}{78} \\sum_{i=1}^{78} (\\hat{Y_i} - Y_i)^2\\]\nLinear Regression: scale equivariant - multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scale equivariant scaling of the least squares coefficient estimates by a factor of \\(\\frac{1}{c}\\). In other words, regardless of how the \\(j_th\\) predictor is scaled, \\(X_{j}\\beta_j\\) will remain the same (ISLR, p. 239). Therefore, this assignment going to use standardized predictors for the model (for convenience).\n\n\nCode\n# Model \nregr=LinearRegression()\n\nmodel_ols= regr.fit(x_train,y_train)\n\ny_train_pred=model_ols.predict(x_train)\ny_test_pred=model_ols.predict(x_test)\n\nprint(\"Training R squared: \", round(regr.score(x_train,y_train),3))\nprint(\"Test R squared: \", round(regr.score(x_test,y_test),3))\n\nprint(\"Training Error: \", round(np.mean((y_train_pred-y_train)**2),3))\nprint(\"Test Error: \", round(np.mean((y_test_pred-y_test)**2),3))\nprint(\"Test Error: \", round(mean_squared_error(y_test_pred,y_test),3)) # alternate way \n\n\nTraining R squared:  0.952\nTest R squared:  0.963\nTraining Error:  291415.502\nTest Error:  192960.565\nTest Error:  192960.565"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#b-2",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#b-2",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n[10 pts] Fit a ridge regression model on the training set, with λ chosen by 10-fold cross-validation. Report the test error obtained.\n\n\nCode\n# ridge regression with default λ=1.0\nridreg = Ridge()\nmodel_ridge=ridreg.fit(x_train, y_train)\n\ny_train_pred=model_ridge.predict(x_train)\ny_test_pred=model_ridge.predict(x_test)\nprint(\"Training Error: \", round(mean_squared_error(y_train_pred,y_train),3))  \nprint(\"Test Error: \", round(mean_squared_error(y_test_pred,y_test),3))\n\n\nTraining Error:  291472.521\nTest Error:  191707.194\n\n\n\n\nCode\n# ridge regression with 10-fold cross validation\nk_fold=KFold(n_splits = 10, shuffle = True, random_state=2022) # set random seed\n\n# lambdas = np.arange(1,100, 1)\n# lambdas = np.arange(1,20, 0.5) \nlambdas = np.arange(2,10, 0.1)   \n\n# ridge\nridreg_cv = RidgeCV(alphas=lambdas, cv=k_fold) \nmodel_ridge_cv=ridreg_cv.fit(x_train, y_train)\n\ny_train_pred=model_ridge_cv.predict(x_train)\ny_test_pred=model_ridge_cv.predict(x_test)\n\nprint(\"Training Error (with 10 folds): \", round(mean_squared_error(y_train_pred,y_train),3))  \nprint(\"Test Error (with 10 folds): \", round(mean_squared_error(y_test_pred,y_test),3))\n\nprint(\"Training R Squared: \", round(r2_score(y_train, y_train_pred),2))\nprint(\"Testing R Squared: \", round(r2_score(y_test, y_test_pred),2))\nprint(\"Optimal Lambda: \", model_ridge_cv.alpha_)\nprint(\"Coefficients: \", model_ridge_cv.coef_)\n\n\nTraining Error (with 10 folds):  293156.156\nTest Error (with 10 folds):  186375.172\nTraining R Squared:  0.95\nTesting R Squared:  0.96\nOptimal Lambda:  6.400000000000004\nCoefficients:  [ 154.40316381 1606.59850184  942.98496287 -488.95762002  187.39331727\n   63.85717783  -67.20841469  260.99875502  -16.86522954    4.24805295\n  -32.17722811   75.26537243   12.02353776  -22.46635632  -83.30446315\n -161.72446977  -18.19291394]\n\n\n\n\nCode\n# Coefficients with λ=6.41\ndisplay(pd.DataFrame({\"Variable\":x_train.columns,\"Coefficients\":model_ridge_cv.coef_}))\n\n\n\n\n\n\n  \n    \n      \n      Variable\n      Coefficients\n    \n  \n  \n    \n      0\n      Private\n      154.403164\n    \n    \n      1\n      Apps\n      1606.598502\n    \n    \n      2\n      Enroll\n      942.984963\n    \n    \n      3\n      Top10perc\n      -488.957620\n    \n    \n      4\n      Top25perc\n      187.393317\n    \n    \n      5\n      F.Undergrad\n      63.857178\n    \n    \n      6\n      P.Undergrad\n      -67.208415\n    \n    \n      7\n      Outstate\n      260.998755\n    \n    \n      8\n      Room.Board\n      -16.865230\n    \n    \n      9\n      Books\n      4.248053\n    \n    \n      10\n      Personal\n      -32.177228\n    \n    \n      11\n      PhD\n      75.265372\n    \n    \n      12\n      Terminal\n      12.023538\n    \n    \n      13\n      S.F.Ratio\n      -22.466356\n    \n    \n      14\n      perc.alumni\n      -83.304463\n    \n    \n      15\n      Expend\n      -161.724470\n    \n    \n      16\n      Grad.Rate\n      -18.192914\n    \n  \n\n\n\n\n\n\nCode\n# ridge regression with optimal λ=6.41\nridreg = Ridge(alpha=6.41)\nmodel_ridge=ridreg.fit(x_train, y_train)\n\ny_train_pred=model_ridge.predict(x_train)\ny_test_pred=model_ridge.predict(x_test)\nprint(\"Training Error: \", round(mean_squared_error(y_train_pred,y_train),3))  \nprint(\"Test Error: \", round(mean_squared_error(y_test_pred,y_test),3))\n\n\nTraining Error:  293160.8\nTest Error:  186366.688"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#c-2",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#c-2",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\n[10 pts] Fit a lasso model on the training set, with λ chosen by 10-fold crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n\n\nCode\n# lasso regression with default λ=1.0\nlassoreg = Lasso()\nmodel_lasso=lassoreg.fit(x_train, y_train)\n\ny_train_pred=model_lasso.predict(x_train)\ny_test_pred=model_lasso.predict(x_test)\nprint(\"Training Error: \", round(mean_squared_error(y_train_pred,y_train),3))  \nprint(\"Test Error: \", round(mean_squared_error(y_test_pred,y_test),3))\n\n\nTraining Error:  291537.881\nTest Error:  190072.628\n\n\n\n\nCode\n# ridge regression with 10-fold cross validation\nk_fold=KFold(n_splits = 10, shuffle = True,random_state=2022) # set random seed\nmax_iter = 15000\n\n# lambdas = np.arange(0.1,10,0.01) # Smallest Test Error calculated at 2.97 \nlambdas = np.arange(4,50,0.1) \n    \nlassoreg_cv = LassoCV(alphas=lambdas, cv=k_fold, max_iter=max_iter) \nmodel_lasso_cv=lassoreg_cv.fit(x_train, y_train)\n\ny_train_pred=model_lasso_cv.predict(x_train)\ny_test_pred=model_lasso_cv.predict(x_test)\n\nprint(\"Training Error (with 10 folds): \", round(mean_squared_error(y_train_pred,y_train),3))  \nprint(\"Test Error (with 10 folds): \", round(mean_squared_error(y_test_pred,y_test),3))\n\nprint(\"Training R Squared: \", round(r2_score(y_train, y_train_pred),2))\nprint(\"Testing R Squared: \", round(r2_score(y_test, y_test_pred),2))\nprint(\"Optimal Lambda: \", model_lasso_cv.alpha_)\nprint(\"Coefficients: \", model_lasso_cv.coef_)\n\n\nTraining Error (with 10 folds):  292747.201\nTest Error (with 10 folds):  181011.81\nTraining R Squared:  0.95\nTesting R Squared:  0.97\nOptimal Lambda:  4.0\nCoefficients:  [ 126.65222298 1632.17789857  979.5277818  -487.43215826  179.97073683\n   -0.          -61.36589383  252.50632841   -8.20028126    0.\n  -25.42427626   69.09512846    6.55921777  -15.98829039  -71.29649584\n -156.34592682  -13.15656712]\n\n\nFYI: https://scikit-learn.org/stable/modules/grid_search.html#specifying-an-objective-metric  By default, parameter search uses the score function of the estimator to evaluate a parameter setting. (sklearn.metrics.r2_score for regression)\nEven though the 10-fold cross-validation r2 score is lowest when \\(\\lambda=3\\), I chose the lambda with less r2 score. Since the purpose of using lasso regression is selecting features (variable selection), lower lambdas are not estimating coefficients with 0. (so I used lambda with 4 which is closest to optimal lambda with some feature’s coefficients are exactly equal to zero)\n\n\nCode\n# Coefficients with λ=4\ncoefs=pd.DataFrame({\"Variable\":x_train.columns,\"Coefficients\":np.round(model_lasso_cv.coef_,3)})\ndisplay(coefs)\n\nprint(\"Training Error (with 10 folds): \", round(mean_squared_error(y_train_pred,y_train),3))  \nprint(\"Test Error (with 10 folds): \", round(mean_squared_error(y_test_pred,y_test),3),\"\\n\")\n\n\n# Variables selected\nprint(\"Variables selected\")\ndisplay(coefs[coefs['Coefficients']!=0]) \n\nprint(\"Variables not selected\")\ndisplay(coefs[coefs['Coefficients']==0].Variable.to_numpy())\n\n\n\n\n\n\n  \n    \n      \n      Variable\n      Coefficients\n    \n  \n  \n    \n      0\n      Private\n      126.652\n    \n    \n      1\n      Apps\n      1632.178\n    \n    \n      2\n      Enroll\n      979.528\n    \n    \n      3\n      Top10perc\n      -487.432\n    \n    \n      4\n      Top25perc\n      179.971\n    \n    \n      5\n      F.Undergrad\n      -0.000\n    \n    \n      6\n      P.Undergrad\n      -61.366\n    \n    \n      7\n      Outstate\n      252.506\n    \n    \n      8\n      Room.Board\n      -8.200\n    \n    \n      9\n      Books\n      0.000\n    \n    \n      10\n      Personal\n      -25.424\n    \n    \n      11\n      PhD\n      69.095\n    \n    \n      12\n      Terminal\n      6.559\n    \n    \n      13\n      S.F.Ratio\n      -15.988\n    \n    \n      14\n      perc.alumni\n      -71.296\n    \n    \n      15\n      Expend\n      -156.346\n    \n    \n      16\n      Grad.Rate\n      -13.157\n    \n  \n\n\n\n\nTraining Error (with 10 folds):  292747.201\nTest Error (with 10 folds):  181011.81 \n\nVariables selected\n\n\n\n\n\n\n  \n    \n      \n      Variable\n      Coefficients\n    \n  \n  \n    \n      0\n      Private\n      126.652\n    \n    \n      1\n      Apps\n      1632.178\n    \n    \n      2\n      Enroll\n      979.528\n    \n    \n      3\n      Top10perc\n      -487.432\n    \n    \n      4\n      Top25perc\n      179.971\n    \n    \n      6\n      P.Undergrad\n      -61.366\n    \n    \n      7\n      Outstate\n      252.506\n    \n    \n      8\n      Room.Board\n      -8.200\n    \n    \n      10\n      Personal\n      -25.424\n    \n    \n      11\n      PhD\n      69.095\n    \n    \n      12\n      Terminal\n      6.559\n    \n    \n      13\n      S.F.Ratio\n      -15.988\n    \n    \n      14\n      perc.alumni\n      -71.296\n    \n    \n      15\n      Expend\n      -156.346\n    \n    \n      16\n      Grad.Rate\n      -13.157\n    \n  \n\n\n\n\nVariables not selected\n\n\narray(['F.Undergrad', 'Books'], dtype=object)\n\n\nCoefficients of variable “F.Undergrad”, “Books” are equal to zero with lambda 4"
  },
  {
    "objectID": "posts/MLDL I/2021-29725_hw1-personal.html#d-2",
    "href": "posts/MLDL I/2021-29725_hw1-personal.html#d-2",
    "title": "Assignment 1",
    "section": "(d)",
    "text": "(d)\n[10 pts] Comment on the results obtained.\n\nHow accurately can you predict the number of college applications received? \nIs there much difference among the test errors resulting from these three approaches? \n\n\n\n\n\n\n\n\n\nMethods\nScore\nValue\n\n\n\n\nOLS\nTest Mean Squared Error\n192960.565\n\n\nRidge\nTest Mean Squared Error (with 10 folds)\n186366.688\n\n\nLasso\nTest Mean Squared Error (with 10 folds + variable selection)\n181011.81\n\n\n\nThe lowest mean squared error was calculated using Lasso regression with two variables’ coeficients are equal to zero. The test error of Lasso is lower compared to OLS (11948.76), Ridge (6593.88).\n\nWhich model would you use?\n\nThe ridge and lasso regression is a shirinkage methods that constrains and regularizes the coefficient estimates. It is true that these two models are improving the fit compared to OLS, but they are not without potential disadvantages. They comprised a unbiasedness of the OLS and reduce variance, which may result in overfitting. Therefore, with caution, the lasso regression would be selected. Lasso regression can be used as a techniques to selecting important predictors out of an comparatively less important predictors."
  },
  {
    "objectID": "posts/MLDL II/2021-29725_hw2.html",
    "href": "posts/MLDL II/2021-29725_hw2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "SGD, LDA, QDA, Decision Tree, Support Vector Machine, Ensembles (Boosting, Random Forests), Clustering, PCA (Score: 110/100 - Best Prediction Score in the Course)"
  },
  {
    "objectID": "posts/MLDL II/2021-29725_hw2.html#loading-dataset",
    "href": "posts/MLDL II/2021-29725_hw2.html#loading-dataset",
    "title": "Assignment 2",
    "section": "Loading Dataset",
    "text": "Loading Dataset\n\n\nCode\n# Set Work Directory\nprint(\"Before Directory: %s\"%os.getcwd())\nos.chdir(\"E:/OneDrive - SNU/r\")\nprint(\"After Directory: %s\"%os.getcwd())\n\n\nBefore Directory: e:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제2\nAfter Directory: E:\\OneDrive - SNU\\r\n\n\n\n\nCode\n# Read Dataset\ny_train=pd.read_csv(\"OnlineAd_Y_train.csv\",header=None)\nx_train=pd.read_csv(\"OnlineAd_X_train.csv\",header=None)\nx_test=pd.read_csv(\"OnlineAd_X_test.csv\",header=None)\n# Check if there is missing value\nprint(\"Total missing variable: \",x_test.isnull().sum().sum()) # no missing value\nprint(\"Total missing variable: \",x_train.isnull().sum().sum()) # no missing value\nprint(\"Total missing variable: \",y_train.isnull().sum().sum()) # no missing value\n# Dimension\nprint(\"xdim: \", x_train.shape)\nprint(\"ydim: \", y_train.shape)\nprint(\"xtestdim: \", x_test.shape)\n\n\nTotal missing variable:  0\nTotal missing variable:  0\nTotal missing variable:  0\nxdim:  (1452, 251)\nydim:  (1452, 3)\nxtestdim:  (300, 251)\n\n\n\n\nCode\nx_colname=[]\nfor i in np.arange(0,251):\n    x_colname.append(f\"x_{i}\")\nx_train.columns=x_colname\ny_colname=[\"No\",\"A\",\"B\"]\ny_train.columns=y_colname\n# no click response: No, second columnn: A, third column: B\n\n# make array for the x_train\nx_train2=x_train.to_numpy()\ndisplay(x_train2[:10])\n# make array for the y_train\ny_train2=np.array(np.where(y_train[\"No\"] == 1, 0, np.where(y_train[\"A\"] == 1, 1, 2)))\ndisplay(y_train2[:10])\n\n\narray([[ 9.6870e-01, -3.9480e-01, -5.4670e-01, ...,  5.1620e-01,\n        -3.3300e-01,  4.6600e-02],\n       [-4.6690e-01,  5.2220e-01,  6.9560e-01, ..., -7.5100e-02,\n        -1.6022e+00,  4.1810e-01],\n       [-1.1187e+00,  2.3400e-01, -5.5800e-02, ...,  2.4260e-01,\n        -1.4561e+00,  3.0000e-03],\n       ...,\n       [ 6.9000e-03,  2.6000e-03, -1.2000e-03, ..., -1.0200e-02,\n        -3.7000e-03, -1.8400e-02],\n       [ 4.0000e-02,  5.2400e-02,  5.3200e-02, ...,  3.9400e-02,\n        -7.8000e-03,  7.9500e-02],\n       [-5.8900e-01, -1.6920e-01, -7.9870e-01, ..., -4.2110e-01,\n        -8.0060e-01,  2.2250e-01]])\n\n\narray([0, 2, 0, 0, 0, 1, 0, 2, 0, 2])\n\n\n\n\nCode\nfrom collections import Counter\ndisplay(y_train.value_counts())\ndisplay(Counter(y_train2))\n\n\nNo   A    B  \n1.0  0.0  0.0    822\n0.0  0.0  1.0    353\n     1.0  0.0    277\ndtype: int64\n\n\nCounter({0: 822, 2: 353, 1: 277})\n\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n# CV\ncv_5 = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=2022)\n\n# Grid Search: Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\n                \"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                    results[\"mean_test_score\"][candidate],\n                    results[\"std_test_score\"][candidate],\n                )\n            )\n            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))\n            print(\"\")\n\n\nCitation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html"
  },
  {
    "objectID": "posts/MLDL II/2021-29725_hw2.html#a",
    "href": "posts/MLDL II/2021-29725_hw2.html#a",
    "title": "Assignment 2",
    "section": "(a)",
    "text": "(a)\n[40 pts] Using OnlineAd X train.csv and OnlineAd Y train.csv, train multiple models that you learned in class. You may try a model which is modified from the models we covered in the first half of the course. You are allowed to use the existing packages, but make sure to clearly explain what models are used. Report the training results. What metric did you use? How do different models perform on the training data?\n\n1) multinomial logistic regression\ncitation: https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n\n\nCode\n# multinomial logistic regression\nmodel_mlogit_l = LogisticRegression(multi_class='multinomial', solver='lbfgs',max_iter=1000)\nmodel_mlogit_s = LogisticRegression(multi_class='multinomial', solver='saga',max_iter=1000)\n#  ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. (already scaled)\n\n# evaluate the model and collect the scores\naccuracy_mlogit_s = cross_val_score(model_mlogit_l, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_mlogit_s = cross_val_score(model_mlogit_l, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\naccuracy_mlogit_l = cross_val_score(model_mlogit_s, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_mlogit_l= cross_val_score(model_mlogit_l, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean Accuracy_saga: %.3f (%.3f)' % (np.mean(accuracy_mlogit_s), np.std(accuracy_mlogit_s)))\nprint('Mean f1 weighted saga: %.3f (%.3f)' % (np.mean(f1w_mlogit_s), np.std(f1w_mlogit_s)))\nprint('Mean Accuracy_lbfgs: %.3f (%.3f)' % (np.mean(accuracy_mlogit_l), np.std(accuracy_mlogit_l)))\nprint('Mean f1 weighted_lbfgs: %.3f (%.3f)' % (np.mean(f1w_mlogit_l), np.std(f1w_mlogit_l)))\n\n\nMean Accuracy_saga: 0.548 (0.023)\nMean f1 weighted saga: 0.542 (0.022)\nMean Accuracy_lbfgs: 0.549 (0.024)\nMean f1 weighted_lbfgs: 0.542 (0.022)\n\n\n\n\nCode\nmodel_mlogit_l.fit(x_train2, y_train2)\ny_pred_mlogit = model_mlogit_l.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_mlogit, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_mlogit),2))\n\n\nTraining result (f1_weighted):  0.72\nTraining result (accuracy):  0.73\n\n\n\n\n2) Support vector machine\n\n\nCode\nfrom sklearn.svm import SVC\n\n\n\n\nCode\n# SVM: linear\nmodel_svm_l_g = SVC(kernel='linear',random_state=2022)\nparameters= {'C': [0.0001, 0.001,0.01, 0.1]}\n\n# Find Best Parameter C\nmodel_svm_l = GridSearchCV(model_svm_l_g, param_grid = parameters, cv = cv_5, n_jobs=-1, scoring='f1_weighted')\n\nmodel_svm_l.fit(x_train2, y_train2)\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=2022),\n             estimator=SVC(kernel='linear', random_state=2020), n_jobs=-1,\n             param_grid={'C': [0.0001, 0.001, 0.01, 0.1]},\n             scoring='f1_weighted')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=2022),\n             estimator=SVC(kernel='linear', random_state=2020), n_jobs=-1,\n             param_grid={'C': [0.0001, 0.001, 0.01, 0.1]},\n             scoring='f1_weighted')estimator: SVCSVC(kernel='linear', random_state=2020)SVCSVC(kernel='linear', random_state=2020)\n\n\n\n\nCode\nmodel_svm_l_result = pd.DataFrame(model_svm_l.cv_results_['params'])\nmodel_svm_l_result['mean_f1_weighted_score'] = model_svm_l.cv_results_['mean_test_score']\nmodel_svm_l_result['std_f1_weighted_score'] = model_svm_l.cv_results_['std_test_score']\ndisplay(model_svm_l_result.sort_values(by='mean_f1_weighted_score', ascending=False))\n\n\n\n\n\n\n  \n    \n      \n      C\n      mean_f1_weighted_score\n      std_f1_weighted_score\n    \n  \n  \n    \n      2\n      0.0100\n      0.561710\n      0.020828\n    \n    \n      1\n      0.0010\n      0.554006\n      0.019123\n    \n    \n      3\n      0.1000\n      0.550008\n      0.021432\n    \n    \n      0\n      0.0001\n      0.464239\n      0.016612\n    \n  \n\n\n\n\n\n\nCode\nmodel_svm_linear = SVC(kernel='linear',C=0.01,random_state=2022)\n\n# evaluate the model and collect the scores\naccuracy_svm_linear = cross_val_score(model_svm_linear, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_svm_linear = cross_val_score(model_svm_linear, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_svm_linear), np.std(f1w_svm_linear)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_svm_linear), np.std(accuracy_svm_linear)))\n\n\nMean f1_weighted: 0.562 (0.021)\nMean Accuracy: 0.645 (0.018)\n\n\n\n\nCode\n# training result\nmodel_svm_linear.fit(x_train2, y_train2)\ny_pred_svml = model_svm_linear.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_svml, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_svml),2))\n\n\nTraining result (f1_weighted):  0.57\nTraining result (accuracy):  0.66\n\n\n\n\nCode\n# SVM: rbf\nmodel_svm_r_g = SVC(kernel='rbf',random_state=2022)\nparameters= {\n    'C': [ 0.1, 1, 10,100, 500,1000], 'gamma': [0.00001, 0.00005, 0.0001, 0.0025, 0.0005, 0.001, 0.01]\n    }\n\n# run grid search\nmodel_svm_r = GridSearchCV(model_svm_r_g, param_grid = parameters, cv = cv_5, n_jobs=-1,scoring='f1_weighted')\nstart = time()\nmodel_svm_r.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_svm_r.cv_results_[\"params\"]))\n)\nreport(model_svm_r.cv_results_)\n# Find Best Parameter C and gamma\n\n\nGridSearchCV took 34.72 seconds for 42 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.579 (std: 0.024)\nParameters: {'C': 10, 'gamma': 0.01}\n\nModel with rank: 2\nMean validation score: 0.574 (std: 0.022)\nParameters: {'C': 1, 'gamma': 0.01}\n\nModel with rank: 3\nMean validation score: 0.562 (std: 0.021)\nParameters: {'C': 500, 'gamma': 1e-05}\n\n\n\n\n\nCode\nmodel_svm_r_result = pd.DataFrame(model_svm_r.cv_results_['params'])\nmodel_svm_r_result['mean_f1_weighted_score'] = model_svm_r.cv_results_['mean_test_score']\nmodel_svm_r_result['std_f1_weighted_score'] = model_svm_r.cv_results_['std_test_score']\ndisplay(model_svm_r_result.sort_values(by='mean_f1_weighted_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      C\n      gamma\n      mean_f1_weighted_score\n      std_f1_weighted_score\n    \n  \n  \n    \n      20\n      10.0\n      0.01000\n      0.579045\n      0.023799\n    \n    \n      13\n      1.0\n      0.01000\n      0.573532\n      0.022499\n    \n    \n      28\n      500.0\n      0.00001\n      0.561827\n      0.020969\n    \n    \n      22\n      100.0\n      0.00005\n      0.561588\n      0.021113\n    \n    \n      18\n      10.0\n      0.00050\n      0.561335\n      0.020375\n    \n  \n\n\n\n\n\n\nCode\nmodel_svm_rbf = SVC(kernel='rbf',C=10,gamma=0.01,random_state=2022)\n\n# evaluate the model and collect the scores\naccuracy_svm_rbf= cross_val_score(model_svm_rbf, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_svm_rbf= cross_val_score(model_svm_rbf, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_svm_rbf), np.std(f1w_svm_rbf)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_svm_rbf), np.std(accuracy_svm_rbf)))\n\n\nMean f1_weighted: 0.579 (0.024)\nMean Accuracy: 0.612 (0.023)\n\n\n\n\nCode\n# training result\nmodel_svm_rbf.fit(x_train2, y_train2)\ny_pred_svmq = model_svm_rbf.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_svmq, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_svmq),2))\n\n\nTraining result (f1_weighted):  0.87\nTraining result (accuracy):  0.87\n\n\n\n\n3) SGD Classifier\nCitation: https://github.com/jiminAn/ML_Project/blob/main/jman/practice_code/sgd_classifier_ex.ipynb  Citation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n\n\nCode\nfrom sklearn.linear_model import SGDClassifier\n\n\n\n\nCode\n# build a classifier\nmodel_sgd_g = SGDClassifier(fit_intercept=True, max_iter=30000)\n\n# use a full grid over all parameters\nparam_grid = {\n    \"average\": [True, False],\n    'loss': ['hinge','log_loss'], \n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'l1_ratio': np.arange(0.3,0.7,0.1),\n    'alpha': np.power(10, np.arange(-2, 1, dtype=float)),\n}\n\n# run grid search\nmodel_sgd = GridSearchCV(model_sgd_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring=\"f1_weighted\")\nstart = time()\nmodel_sgd.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_sgd.cv_results_[\"params\"]))\n)\nreport(model_sgd.cv_results_)\n\n\nFitting 25 folds for each of 144 candidates, totalling 3600 fits\nGridSearchCV took 37.57 seconds for 144 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.573 (std: 0.018)\nParameters: {'alpha': 0.01, 'average': True, 'l1_ratio': 0.3, 'loss': 'log_loss', 'penalty': 'l2'}\n\nModel with rank: 2\nMean validation score: 0.572 (std: 0.018)\nParameters: {'alpha': 0.01, 'average': True, 'l1_ratio': 0.4, 'loss': 'log_loss', 'penalty': 'l2'}\n\nModel with rank: 3\nMean validation score: 0.571 (std: 0.016)\nParameters: {'alpha': 0.01, 'average': True, 'l1_ratio': 0.6000000000000001, 'loss': 'log_loss', 'penalty': 'l2'}\n\n\n\n\n\nCode\nmodel_sgd_result = pd.DataFrame(model_sgd.cv_results_['params'])\nmodel_sgd_result['mean_f1_weighted_score'] = model_sgd.cv_results_['mean_test_score']\nmodel_sgd_result['std_f1_weighted_score'] = model_sgd.cv_results_['std_test_score']\ndisplay(model_sgd_result.sort_values(by='mean_f1_weighted_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      alpha\n      average\n      l1_ratio\n      loss\n      penalty\n      mean_f1_weighted_score\n      std_f1_weighted_score\n    \n  \n  \n    \n      3\n      0.01\n      True\n      0.3\n      log_loss\n      l2\n      0.573024\n      0.018362\n    \n    \n      9\n      0.01\n      True\n      0.4\n      log_loss\n      l2\n      0.572308\n      0.017537\n    \n    \n      21\n      0.01\n      True\n      0.6\n      log_loss\n      l2\n      0.570664\n      0.016208\n    \n    \n      15\n      0.01\n      True\n      0.5\n      log_loss\n      l2\n      0.569818\n      0.017345\n    \n    \n      27\n      0.01\n      False\n      0.3\n      log_loss\n      l2\n      0.568760\n      0.018627\n    \n  \n\n\n\n\n\n\nCode\nmodel_sgd = SGDClassifier(max_iter=30000, average=True, alpha=0.01, l1_ratio=0.3, loss='log_loss', penalty='l2')\n\n# evaluate the model and collect the scores\naccuracy_sgd= cross_val_score(model_sgd, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_sgd= cross_val_score(model_sgd, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_sgd), np.std(f1w_sgd)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_sgd), np.std(accuracy_sgd)))\n\n\nMean f1_weighted: 0.573 (0.016)\nMean Accuracy: 0.628 (0.017)\n\n\n\n\nCode\n# training result\nmodel_sgd.fit(x_train2, y_train2)\ny_pred_sgd = model_sgd.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_sgd, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_sgd),2))\n\n\nTraining result (f1_weighted):  0.64\nTraining result (accuracy):  0.69\n\n\n\n\n4) Nearest neighbors (knn)\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\n\nCode\n# build a classifier\nmodel_knn_g = KNeighborsClassifier()\n\n# use a full grid over all parameters\nparam_grid = {\n    'n_neighbors' : np.arange(1,20).tolist(),\n    'weights' : [\"uniform\", \"distance\"],\n    'metric' : ['euclidean', 'manhattan', 'minkowski']\n}\n\n# run grid search\nmodel_knn = GridSearchCV(model_knn_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring=\"f1_weighted\")\nstart = time()\nmodel_knn.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_knn .cv_results_[\"params\"]))\n)\nreport(model_knn.cv_results_)\n\n\nFitting 25 folds for each of 114 candidates, totalling 2850 fits\nGridSearchCV took 16.74 seconds for 114 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.572 (std: 0.027)\nParameters: {'metric': 'euclidean', 'n_neighbors': 6, 'weights': 'uniform'}\n\nModel with rank: 1\nMean validation score: 0.572 (std: 0.027)\nParameters: {'metric': 'minkowski', 'n_neighbors': 6, 'weights': 'uniform'}\n\nModel with rank: 3\nMean validation score: 0.567 (std: 0.023)\nParameters: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n\nModel with rank: 3\nMean validation score: 0.567 (std: 0.023)\nParameters: {'metric': 'minkowski', 'n_neighbors': 5, 'weights': 'uniform'}\n\n\n\n\n\nCode\nmodel_knn_result = pd.DataFrame(model_knn.cv_results_['params'])\nmodel_knn_result['mean_f1_weighted_score'] = model_knn.cv_results_['mean_test_score']\nmodel_knn_result['std_f1_weighted_score'] = model_knn.cv_results_['std_test_score']\ndisplay(model_knn_result.sort_values(by='mean_f1_weighted_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      metric\n      n_neighbors\n      weights\n      mean_f1_weighted_score\n      std_f1_weighted_score\n    \n  \n  \n    \n      86\n      minkowski\n      6\n      uniform\n      0.571810\n      0.026588\n    \n    \n      10\n      euclidean\n      6\n      uniform\n      0.571810\n      0.026588\n    \n    \n      8\n      euclidean\n      5\n      uniform\n      0.567070\n      0.022849\n    \n    \n      84\n      minkowski\n      5\n      uniform\n      0.567070\n      0.022849\n    \n    \n      85\n      minkowski\n      5\n      distance\n      0.566627\n      0.025381\n    \n  \n\n\n\n\n\n\nCode\nmodel_knn = KNeighborsClassifier(metric='euclidean',n_neighbors=6, weights='uniform')\n\n# evaluate the model and collect the scores\naccuracy_knn= cross_val_score(model_knn, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_knn= cross_val_score(model_knn, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_knn), np.std(f1w_knn)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_knn), np.std(accuracy_knn)))\n\n\nMean f1_weighted: 0.572 (0.027)\nMean Accuracy: 0.583 (0.024)\n\n\n\n\nCode\n# training result\nmodel_knn.fit(x_train2, y_train2)\ny_pred_knn = model_knn.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_knn, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_knn),2))\n\n\nTraining result (f1_weighted):  0.69\nTraining result (accuracy):  0.7\n\n\n\n\n5) Random Forests\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n\nCode\n# build a classifier\nmodel_rf_g = RandomForestClassifier(random_state=2020)\n\n# use a full grid over all parameters\nparam_grid = {\n    'n_estimators': [200, 500, 700],\n    'max_features': ['sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8,9,10],\n    'criterion' :['gini', 'entropy', 'log_loss']\n}\n\n# run grid search\nmodel_rf = GridSearchCV(model_rf_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring=\"f1_weighted\")\nstart = time()\nmodel_rf.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_rf.cv_results_[\"params\"]))\n)\nreport(model_rf.cv_results_)\n\n\nFitting 25 folds for each of 126 candidates, totalling 3150 fits\nGridSearchCV took 1417.48 seconds for 126 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.584 (std: 0.023)\nParameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 200}\n\nModel with rank: 1\nMean validation score: 0.584 (std: 0.023)\nParameters: {'criterion': 'log_loss', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 200}\n\nModel with rank: 3\nMean validation score: 0.580 (std: 0.024)\nParameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500}\n\n\n\n\n\nCode\nmodel_rf1 = RandomForestClassifier(random_state=2020,criterion='entropy', max_features='sqrt', n_estimators= 200, max_depth=10)\n\n# evaluate the model and collect the scores\naccuracy_rf1= cross_val_score(model_rf1, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_rf1= cross_val_score(model_rf1, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted rf1: %.3f (%.3f)' % (np.mean(f1w_rf1), np.std(f1w_rf1)))\nprint('Mean Accuracy rf1: %.3f (%.3f)' % (np.mean(accuracy_rf1), np.std(accuracy_rf1)))\n\n\nMean f1_weighted rf1: 0.584 (0.023)\nMean Accuracy rf1: 0.639 (0.020)\n\n\n\n\nCode\n# training result\nmodel_rf1.fit(x_train2, y_train2)\ny_pred_rf1 = model_rf1.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_rf1, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_rf1),2))\n\n\nTraining result (f1_weighted):  1.0\nTraining result (accuracy):  1.0\n\n\n\n\n6) (Gradient Boosted) Tree\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n\n\nCode\n# build a classifier\nmodel_bt_g = GradientBoostingClassifier(random_state=2022,loss='log_loss', n_iter_no_change = 15)\n\n# use a full grid over all parameters\nparam_grid = {\n    'max_depth': [1,2],\n    'n_estimators': np.arange(25,175,50).tolist(),\n    'max_depth' : [1,3,5,7],\n    'learning_rate': [0.01,0.05,0.1],\n    'max_features': [5,10]+np.arange(1,180,30).tolist()\n}\n\n# run grid search\nmodel_bt = GridSearchCV(model_bt_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1,scoring=\"f1_weighted\")\nstart = time()\nmodel_bt.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_bt.cv_results_[\"params\"]))\n)\nreport(model_bt.cv_results_)\n\n\nFitting 25 folds for each of 288 candidates, totalling 7200 fits\nGridSearchCV took 1935.11 seconds for 288 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.598 (std: 0.022)\nParameters: {'learning_rate': 0.1, 'max_depth': 1, 'max_features': 31, 'n_estimators': 75}\n\nModel with rank: 2\nMean validation score: 0.597 (std: 0.022)\nParameters: {'learning_rate': 0.05, 'max_depth': 1, 'max_features': 31, 'n_estimators': 125}\n\nModel with rank: 3\nMean validation score: 0.596 (std: 0.024)\nParameters: {'learning_rate': 0.05, 'max_depth': 1, 'max_features': 31, 'n_estimators': 75}\n\n\n\n\n\nCode\nmodel_bt = GradientBoostingClassifier(random_state=2022,loss='log_loss', learning_rate=0.1, max_depth=1, max_features=90, n_estimators=75)\n\n# evaluate the model and collect the scores\naccuracy_bt= cross_val_score(model_bt, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_bt= cross_val_score(model_bt, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted bt: %.3f (%.3f)' % (np.mean(f1w_bt), np.std(f1w_bt)))\nprint('Mean Accuracy bt: %.3f (%.3f)' % (np.mean(accuracy_bt), np.std(accuracy_bt)))\n\n\nMean f1_weighted bt: 0.589 (0.023)\nMean Accuracy bt: 0.641 (0.021)\n\n\n\n\nCode\n# training result\nmodel_bt.fit(x_train2, y_train2)\ny_pred_bt = model_bt.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_bt, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_bt),2))\n\n\nTraining result (f1_weighted):  0.64\nTraining result (accuracy):  0.68\n\n\n\n\n7) Xgboost\n\n\nCode\nfrom xgboost import XGBClassifier\n\n\n\n\nCode\n# build a classifier\nmodel_xgb_g = XGBClassifier(random_state=2022, objective='multi:softmax',num_class=3, max_depth= 1)\n\n# use a full grid over all parameters\nparam_grid = {\n    'n_estimators' : [25, 50, 100, 125], \n    'learning_rate' : [0.01,0.05],\n    'gamma' : np.arange(0.01,0.02,0.005).tolist(),\n    'alpha' : np.arange(0.1,1,0.05).tolist(),\n    'lambda' : np.arange(0.01,0.02,0.005).tolist()\n}\n\n# run grid search\nmodel_xgb = GridSearchCV(model_xgb_g, verbose= 3, param_grid=param_grid, cv=cv_5,n_jobs=-1,scoring='f1_weighted')\nstart = time()\nmodel_xgb.fit(x_train2, y_train2)\n\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_xgb.cv_results_[\"params\"]))\n)\nreport(model_xgb.cv_results_)\n\n\nFitting 25 folds for each of 576 candidates, totalling 14400 fits\nGridSearchCV took 3349.99 seconds for 576 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.582 (std: 0.022)\nParameters: {'alpha': 0.9500000000000003, 'gamma': 0.01, 'lambda': 0.01, 'learning_rate': 0.05, 'n_estimators': 125}\n\nModel with rank: 1\nMean validation score: 0.582 (std: 0.022)\nParameters: {'alpha': 0.9500000000000003, 'gamma': 0.01, 'lambda': 0.015, 'learning_rate': 0.05, 'n_estimators': 125}\n\nModel with rank: 1\nMean validation score: 0.582 (std: 0.022)\nParameters: {'alpha': 0.9500000000000003, 'gamma': 0.015, 'lambda': 0.01, 'learning_rate': 0.05, 'n_estimators': 125}\n\nModel with rank: 1\nMean validation score: 0.582 (std: 0.022)\nParameters: {'alpha': 0.9500000000000003, 'gamma': 0.015, 'lambda': 0.015, 'learning_rate': 0.05, 'n_estimators': 125}\n\n\n\n\n\nCode\nmodel_xgb = XGBClassifier(random_state=2022, objective='multi:softmax',num_class=3, booster=\"gbtree\", learning_rate=0.05, alpha=0.95, gamma=0.01, reg_lambda=0.01, n_estimators=125, max_depth=1)\n\n# evaluate the model and collect the scores\naccuracy_xgb= cross_val_score(model_xgb, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_xgb= cross_val_score(model_xgb, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted xgb: %.3f (%.3f)' % (np.mean(f1w_xgb), np.std(f1w_xgb)))\nprint('Mean Accuracy xgb: %.3f (%.3f)' % (np.mean(accuracy_xgb), np.std(accuracy_xgb)))\n\n\nMean f1_weighted xgb: 0.582 (0.022)\nMean Accuracy xgb: 0.641 (0.018)\n\n\n\n\nCode\n# training result\nmodel_xgb.fit(x_train2, y_train2)\ny_pred_xgb = model_xgb.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_xgb, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_xgb),2))\n\n\nTraining result (f1_weighted):  0.62\nTraining result (accuracy):  0.66\n\n\n\n\n8) linear discriminant analysis\n\n\nCode\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\n\n\nCode\nmodel_lda_g = LinearDiscriminantAnalysis()\nparameters= {\n    'solver': ['lsqr', 'eigen'], \n    'shrinkage': ['auto']+np.arange(0.1,0.9,0.1).tolist()\n    }\n\n# run grid search\nmodel_lda = GridSearchCV(model_lda_g, verbose= 1, param_grid=parameters, cv=cv_5, n_jobs=-1, scoring=\"f1_weighted\")\nstart = time()\nmodel_lda.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_lda.cv_results_[\"params\"]))\n)\nreport(model_lda.cv_results_)\n\n\nFitting 25 folds for each of 18 candidates, totalling 450 fits\nGridSearchCV took 3.53 seconds for 18 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.617 (std: 0.022)\nParameters: {'shrinkage': 0.5, 'solver': 'lsqr'}\n\nModel with rank: 1\nMean validation score: 0.617 (std: 0.022)\nParameters: {'shrinkage': 0.5, 'solver': 'eigen'}\n\nModel with rank: 3\nMean validation score: 0.616 (std: 0.024)\nParameters: {'shrinkage': 0.6, 'solver': 'lsqr'}\n\nModel with rank: 3\nMean validation score: 0.616 (std: 0.024)\nParameters: {'shrinkage': 0.6, 'solver': 'eigen'}\n\n\n\n\n\nCode\nmodel_lda = LinearDiscriminantAnalysis(shrinkage=0.5,solver='lsqr')\n\n# evaluate the model and collect the scores\naccuracy_lda = cross_val_score(model_lda, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_lda = cross_val_score(model_lda, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted lda: %.3f (%.3f)' % (np.mean(f1w_lda), np.std(f1w_lda)))\nprint('Mean Accuracy lda: %.3f (%.3f)' % (np.mean(accuracy_lda), np.std(accuracy_lda)))\n\n\nMean f1_weighted lda: 0.617 (0.022)\nMean Accuracy lda: 0.621 (0.024)\n\n\n\n\nCode\n# training result\nmodel_lda.fit(x_train2, y_train2)\ny_pred_lda = model_lda.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_lda, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_lda),2))\n\n\nTraining result (f1_weighted):  0.65\nTraining result (accuracy):  0.65\n\n\n\n\n9) quadratic discriminant analysis\n\n\nCode\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n\n\n\nCode\nmodel_qda_g = QuadraticDiscriminantAnalysis()\nparameters= {'reg_param': np.arange(0,1,0.1).tolist()}\n\n# run grid search\nmodel_qda = GridSearchCV(model_qda_g, verbose= 1, param_grid=parameters, cv=cv_5, n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_qda.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_qda.cv_results_[\"params\"]))\n)\nreport(model_qda.cv_results_)\n\n\n\n\nCode\nmodel_qda = QuadraticDiscriminantAnalysis(reg_param=0.9)\n\n# evaluate the model and collect the scores\naccuracy_qda = cross_val_score(model_qda_f, x_train2, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_qda = cross_val_score(model_qda_f, x_train2, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted qda: %.3f (%.3f)' % (np.mean(f1w_qda), np.std(f1w_qda)))\nprint('Mean Accuracy qda: %.3f (%.3f)' % (np.mean(accuracy_qda), np.std(accuracy_qda)))\n\n\nMean f1_weighted qda: 0.543 (0.025)\nMean Accuracy qda: 0.525 (0.027)\n\n\n\n\nCode\n# training result\nmodel_qda.fit(x_train2, y_train2)\ny_pred_qda = model_qda.predict(x_train2)\nprint(\"Training result (f1_weighted): \", round(f1_score(y_train2, y_pred_qda, average='weighted'),2))\nprint(\"Training result (accuracy): \", round(accuracy_score(y_train2, y_pred_qda),2))\n\n\nTraining result (f1_weighted):  0.62\nTraining result (accuracy):  0.62\n\n\nc:\\Users\\jsw06\\miniconda3\\envs\\koreait\\lib\\site-packages\\sklearn\\discriminant_analysis.py:887: UserWarning: Variables are collinear\n  warnings.warn(\"Variables are collinear\")"
  },
  {
    "objectID": "posts/MLDL II/2021-29725_hw2.html#b",
    "href": "posts/MLDL II/2021-29725_hw2.html#b",
    "title": "Assignment 2",
    "section": "(b)",
    "text": "(b)\n[20 pts] Do you think dimension reduction on features (or feature selection) is needed here? If so, provide analysis on which features may be important. If not, please justify your answer.\nDimension reduction on features or feature selection might provide better accuracy score, since models training with a subset of features (such as gradient boosting classifier, random forest etc.) showed better mean CV f1_weighted score (or accuracy score) compared to other models without feature selection. There might be a possibility of model specification issue on including non-informative or redundant predictors. Therefore, using dimension reduction on features or feature selection to improve prediction might be desirable.\n\n\nCode\ncorr=x_train.corr()\ncorr.values[corr.values==1]=0\ncorr2=corr[(corr.values>0.5).any(axis=1)&(corr.values>0.5).any(axis=0)]\n\n\n\n\nCode\nlen(corr.values[corr.values>0.5].tolist())/2\n\n\n1753.0\n\n\n\n1) PCA\n\n\nCode\nmodel_pca = PCA(n_components=251)\npca_full_array = model_pca.fit_transform(x_train)\npca_full_df = pd.DataFrame(pca_full_array, index=x_train.index,\n                      columns=[f\"pca{num+1}\" for num in range(x_train.shape[1])])\n\n\n\\[ proportion \\: = specific\\: component's\\: proportion =  \\frac{specific\\: component's\\: eigenvalue\\: (주성분\\:분산)} {sum\\:of\\:all\\:eigenvalues (모든\\:주성분\\:분산의\\:합)}\\]\n\n\nCode\npca_table = pd.DataFrame({'Eigenvalue':model_pca.explained_variance_, 'Proportion':model_pca.explained_variance_ratio_},\n            index=np.array([f\"pca{num+1}\" for num in range(x_train2.shape[1])]))\npca_table['Cumulative'] = pca_table['Proportion'].cumsum()\ndisplay(pca_table.head(10))\n\n\n\n\n\n\n  \n    \n      \n      Eigenvalue\n      Proportion\n      Cumulative\n    \n  \n  \n    \n      pca1\n      29.741754\n      0.331111\n      0.331111\n    \n    \n      pca2\n      12.455516\n      0.138666\n      0.469776\n    \n    \n      pca3\n      0.546720\n      0.006087\n      0.475863\n    \n    \n      pca4\n      0.535468\n      0.005961\n      0.481824\n    \n    \n      pca5\n      0.522446\n      0.005816\n      0.487640\n    \n    \n      pca6\n      0.507087\n      0.005645\n      0.493286\n    \n    \n      pca7\n      0.484598\n      0.005395\n      0.498681\n    \n    \n      pca8\n      0.473509\n      0.005272\n      0.503952\n    \n    \n      pca9\n      0.466847\n      0.005197\n      0.509150\n    \n    \n      pca10\n      0.446134\n      0.004967\n      0.514116\n    \n  \n\n\n\n\n\n\nCode\nfrom psynlig import pca_scree\nplt.clf()\nplt.style.use('seaborn')\npca_scree(model_pca, marker='o', markersize=16, lw=3)\nplt.rcParams.update({'font.size': 16})\nplt.xlim([0, 30])\nplt.show()\n\n\n<Figure size 800x550 with 0 Axes>\n\n\n\n\n\nAccording to screeplot there is not much difference on eigenvalue after 3, therefore the number of components were set to three.\n\n\nCode\nfrom pca import pca\nplt.clf()\nmodel_pca = pca(n_components=3)\npca_reduced_array = model_pca.fit_transform(x_train)\npca_df = pd.DataFrame(pca_reduced_array, index=x_train.index,columns=np.array([f\"pca{num+1}\" for num in range(2)]))\n                      \nresult_pca = model_pca.fit_transform(x_train)\nfig, ax = model_pca.biplot3d(n_feat=3, legend=False)\nplt.show()\n\n\n[pca] >Processing dataframe..\n[pca] >The PCA reduction is performed on the [251] columns of the input dataframe.\n[pca] >Fit using PCA.\n[pca] >Compute loadings and PCs.\n[pca] >Compute explained variance.\n[pca] >Outlier detection using Hotelling T2 test with alpha=[0.05] and n_components=[3]\n[pca] >Outlier detection using SPE/DmodX with n_std=[2]\n[pca] >Cleaning previous fitted model results..\n[pca] >Processing dataframe..\n[pca] >The PCA reduction is performed on the [251] columns of the input dataframe.\n[pca] >Fit using PCA.\n[pca] >Compute loadings and PCs.\n[pca] >Compute explained variance.\n[pca] >Outlier detection using Hotelling T2 test with alpha=[0.05] and n_components=[3]\n[pca] >Outlier detection using SPE/DmodX with n_std=[2]\n[pca] >Plot PC1 vs PC2 vs PC3 with loadings.\n[colourmap]> Warning: Colormap [Set1] can not create [1452] unique colors! Available unique colors: [9].\n[colourmap]> Warning: Colormap [Set1] can not create [1452] unique colors! Available unique colors: [9].\n[pca] >Set parameter \"label=None\" to ignore the labels and significanly speed up the scatter plot.\n\n\n<Figure size 800x550 with 0 Axes>\n\n\n\n\n\n\n\nCode\nplt.clf()\nmodel_pca = pca(n_components=0.80)\npca_reduced_array = model_pca.fit_transform(x_train)\npca_df = pd.DataFrame(pca_reduced_array, index=x_train.index,columns=np.array([f\"pca{num+1}\" for num in range(2)]))\n                      \nresult_pca = model_pca.fit_transform(x_train)\nfig, ax = model_pca.biplot(n_feat=2, legend=False)\nplt.figure(figsize=(8,6))\nplt.show( )\n\n\n[pca] >Processing dataframe..\n[pca] >The PCA reduction is performed to capture [80.0%] explained variance using the [251] columns of the input data.\n[pca] >Fit using PCA.\n[pca] >Compute loadings and PCs.\n[pca] >Compute explained variance.\n[pca] >Number of components is [98] that covers the [80.00%] explained variance.\n[pca] >The PCA reduction is performed on the [251] columns of the input dataframe.\n[pca] >Fit using PCA.\n[pca] >Compute loadings and PCs.\n[pca] >Outlier detection using Hotelling T2 test with alpha=[0.05] and n_components=[98]\n[pca] >Outlier detection using SPE/DmodX with n_std=[2]\n[pca] >Cleaning previous fitted model results..\n[pca] >Processing dataframe..\n[pca] >The PCA reduction is performed on the [251] columns of the input dataframe.\n[pca] >Fit using PCA.\n[pca] >Compute loadings and PCs.\n[pca] >Compute explained variance.\n[pca] >Outlier detection using Hotelling T2 test with alpha=[0.05] and n_components=[98]\n[pca] >Outlier detection using SPE/DmodX with n_std=[2]\n[pca] >Plot PC1 vs PC2 with loadings.\n[colourmap]> Warning: Colormap [Set1] can not create [1452] unique colors! Available unique colors: [9].\n[colourmap]> Warning: Colormap [Set1] can not create [1452] unique colors! Available unique colors: [9].\n[pca] >Set parameter \"label=None\" to ignore the labels and significanly speed up the scatter plot.\n\n\n<Figure size 800x550 with 0 Axes>\n\n\n\n\n\n<Figure size 800x600 with 0 Axes>\n\n\n\n\nCode\nplt.clf()\nplt.show(model_pca.plot())\n\n\n<Figure size 800x550 with 0 Axes>\n\n\n\n\n\n<Figure size 800x550 with 0 Axes>\n\n\nHowever, 3 components can not be used since the variance explained by 3 components is only 48% of the total variance. If the variance explained by the components needs to be over 80%, then at least 98 components need to be used. This discrepancy between significant eigenvalues and very small variance explained by marginal components makes it difficult to use PCA in the analysis. Therefore, the PCA was not used in this analysis as a demension reduction technique.\n\n\nCode\nplt.clf()\nplt.show(model_pca.plot())\n\n\n<Figure size 800x550 with 0 Axes>\n\n\n\n\n\n<Figure size 800x550 with 0 Axes>\n\n\n\n\nCode\nm=x_train.shape[1]\nk=2\npca_full=PCA(n_components=m)\npca_df = pd.DataFrame(pca_full.fit_transform(x_train),columns=[\"PC%d\" % k for k in range(1,m + 1)]).iloc[:,:K]\npal ={0:'red',1:'blue', 2:'orange'}\nlabel={0:'No',1:'A', 2:'B'}\npca1=pd.Series(pca_df.iloc[:,0])\npca2=pca_df.iloc[:,1]\nfig,ax=plt.subplots(figsize=(7,5))\nfig.patch.set_facecolor('white')\nfor i in [0,1,2]:\n    l = np.where(y_train2==i)\n    ax.scatter(pca1.iloc[l],pca2.iloc[l],s=40,label=label[i],c=pal[i])\nplt.legend()\nplt.xlabel(\"PC 1\")\nplt.ylabel(\"PC 2\")\nplt.title(\"PCA to Visualize\")\nplt.show()\n\n\n\n\n\n\n\nCode\npca_df2 = pca_df[ (pca_df[\"PC1\"]>=10) | (pca_df[\"PC2\"]>=0) ]\nfig,ax=plt.subplots(figsize=(7,5))\nfig.patch.set_facecolor('white')\npca1_2=pca_df2.iloc[:,0]\npca2_2=pca_df2.iloc[:,1]\nfor i in [0,1,2]:\n    l = np.where(y_train2[pca_df2.index]==i)\n    ax.scatter(pca1_2.iloc[l],pca2_2.iloc[l],s=40,label=label[i],c=pal[i])\nplt.legend()\nplt.xlabel(\"PC 1\")\nplt.ylabel(\"PC 2\")\nplt.title(\"(FIGURE 1) PCA to visualize: magnify relatively separated part (n=724)\")\nplt.show()\n\n\n\n\n\n\n\nCode\npca_df3 = pca_df[(pca_df[\"PC1\"]<10) & (pca_df[\"PC2\"]<0) ]\nfig,ax=plt.subplots(figsize=(7,5))\nfig.patch.set_facecolor('white')\npca1_3=pca_df3.iloc[:,0]\npca2_3=pca_df3.iloc[:,1]\nfor i in [0,1,2]:\n    l = np.where(y_train2[pca_df3.index]==i)\n    ax.scatter(pca1_3.iloc[l],pca2_3.iloc[l],s=40,label=label[i],c=pal[i])\nplt.legend()\nplt.xlabel(\"PC 1\")\nplt.ylabel(\"PC 2\")\nplt.title(\"(FIGURE 2) PCA to visualize: magnify not well separated part (n=728)\")\nplt.show()\n\n\n\n\n\n\n\n2) L1-based feature selection / Tree-based feature selection\ncitation: https://scikit-learn.org/stable/modules/feature_selection.html\n\nfirst, l1-regularized linear SVM and logistic regression was used to select features. In order to minimize the possibility of model overfit, cross-validation was adopted to choose best lambdas for l1-regularization.\n\n\n\nCode\n# SVM: linear\nfrom sklearn.svm import LinearSVC\nmodel_svm_l_g = LinearSVC(penalty=\"l1\", dual=False, random_state=2020, max_iter=20000)\nparameters= {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n# Find Best Parameter C\nmodel_svm_l = GridSearchCV(model_svm_l_g, param_grid = parameters, cv = cv_5, n_jobs=-1)\n\nmodel_svc_fs= model_svm_l.fit(x_train2, y_train2)\n\n\n\n\nCode\nmodel_svc_fs_result = pd.DataFrame(model_svc_fs.cv_results_['params'])\nmodel_svc_fs_result['mean_accuracy_score'] = model_svc_fs.cv_results_['mean_test_score']\ndisplay(model_svc_fs_result.sort_values(by='mean_accuracy_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      C\n      mean_accuracy_score\n    \n  \n  \n    \n      1\n      0.010\n      0.648080\n    \n    \n      2\n      0.100\n      0.623004\n    \n    \n      0\n      0.001\n      0.566114\n    \n    \n      3\n      1.000\n      0.555101\n    \n    \n      4\n      10.000\n      0.531134\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.feature_selection import SelectFromModel\nmodel_svc_l=LinearSVC(C= 0.01, penalty=\"l1\", dual=False, random_state=2020, max_iter=20000)\nmodel_svc_fs= model_svc_l.fit(x_train2, y_train2)\nmodel_svc_final = SelectFromModel(model_svc_fs, prefit=True)\n\nx_train3= model_svc_final.transform(x_train2)\nprint(\"original dataset: \", x_train2.shape,\", feature selected: \", x_train3.shape)\n\n\noriginal dataset:  (1452, 251) , feature selected:  (1452, 26)\n\n\n\n\nCode\n# Logistic\nmodel_lr_g= LogisticRegression(penalty='l1', multi_class='multinomial', solver='saga', max_iter=10000, dual=False, random_state=2020)\nparameters= {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n# Find Best Parameter C\nmodel_lr = GridSearchCV(model_lr_g, param_grid = parameters, cv = cv_5, n_jobs=-1)\n\nmodel_lr_fs= model_lr.fit(x_train2, y_train2)\n\n\n\n\nCode\nmodel_lr_fs_result = pd.DataFrame(model_lr_fs.cv_results_['params'])\nmodel_lr_fs_result['mean_accuracy_score'] = model_lr_fs.cv_results_['mean_test_score']\ndisplay(model_lr_fs_result.sort_values(by='mean_accuracy_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      C\n      mean_accuracy_score\n    \n  \n  \n    \n      2\n      0.100\n      0.644903\n    \n    \n      1\n      0.010\n      0.590767\n    \n    \n      3\n      1.000\n      0.571637\n    \n    \n      0\n      0.001\n      0.566114\n    \n    \n      4\n      10.000\n      0.528654\n    \n  \n\n\n\n\n\n\nCode\nmodel_lr= LogisticRegression(penalty='l1',C=0.1,multi_class='multinomial', solver='saga', max_iter=10000, dual=False, random_state=2020)\nmodel_lr_fs= model_lr.fit(x_train2, y_train2)\nmodel_lr_final = SelectFromModel(model_lr_fs, prefit=True)\n\nx_train4= model_lr_final.transform(x_train2)\nprint(\"original dataset: \", x_train2.shape,\", feature selected: \", x_train4.shape)\n\n\noriginal dataset:  (1452, 251) , feature selected:  (1452, 60)\n\n\n\n\nCode\n# Tree-based\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\n\n\nCode\nmodel_et_g= ExtraTreesClassifier(n_estimators=50, max_features='sqrt', random_state=2022)\n\n# use a full grid over all parameters\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'n_estimators': np.arange(25,175,50).tolist(),\n    'max_depth' : [1,3,5,7],\n    'max_features': [5,10]+np.arange(1,180,30).tolist()\n}\n\n# run grid search\nmodel_et = GridSearchCV(model_et_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1)\nstart = time()\nmodel_et.fit(x_train2, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_et.cv_results_[\"params\"]))\n)\nreport(model_et.cv_results_)\n\n\nFitting 25 folds for each of 192 candidates, totalling 4800 fits\nGridSearchCV took 126.86 seconds for 192 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.644 (std: 0.017)\nParameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 121, 'n_estimators': 125}\n\nModel with rank: 2\nMean validation score: 0.644 (std: 0.016)\nParameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 121, 'n_estimators': 125}\n\nModel with rank: 3\nMean validation score: 0.644 (std: 0.016)\nParameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 151, 'n_estimators': 25}\n\n\n\n\n\nCode\nmodel_et= ExtraTreesClassifier(criterion= 'entropy', max_features=121, n_estimators= 125, max_depth=3 , random_state=2022)\nmodel_et_fs= model_et.fit(x_train2, y_train2)\nmodel_et_final = SelectFromModel(model_et_fs, prefit=True)\n\nx_train4= model_et_final.transform(x_train2)\nprint(\"original dataset: \", x_train2.shape,\", feature selected: \", x_train4.shape)\n\n\noriginal dataset:  (1452, 251) , feature selected:  (1452, 45)\n\n\nThe features that are selected in at least two of the each selection methods were included in the final feature selection.\n\n\nCode\nfeatures = pd.DataFrame([model_lr_final.fit(x_train2,y_train2).get_support(), model_svc_final.fit(x_train2,y_train2).get_support(),model_et_final.fit(x_train2,y_train2).get_support()],\n                        index=[\"LR\",\"SVM\",\"ET\"]).T\n\nfeatures1=pd.DataFrame(features[(features[\"LR\"] == True)|(features[\"SVM\"] == True)|(features[\"ET\"] == True)].sum(axis=1)>1, columns=[\"features\"])\nprint(\"features chosen: \", features1[features1[\"features\"]==True].shape)\nfeatures2=features1[features1[\"features\"]==True]\nwith np.printoptions(threshold=np.inf):\n    print(\"selected features:\",np.array(features2.index))\n\n\nfeatures chosen:  (27, 1)\nselected features: [  2  17  22  28  30  38  44  49  61  64  70  79  90 104 113 129 134 145\n 169 172 177 190 193 207 228 231 246]\n\n\n\n\n3) Re-estimation with selected features\nUsing selected features, the crossvalidation accuracy and f1_scores for the models without feature selection was calculated again  (logistic, linear svm, quadratic svm, knn, random forest, gbm, xgboost, lda, qda )\n\n\nCode\n# selection\nx_train_fs=x_train.iloc[:,features2.index]\nx_train5=x_train_fs.to_numpy()\nx_train5.shape\n\n\n(1452, 27)\n\n\n\n\nCode\nplt.clf()\ncorr=x_train_fs.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(8, 6))\ncmap = sns.color_palette(\"Blues\", as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()\n\n\n<Figure size 640x480 with 0 Axes>\n\n\n\n\n\n\n- multinomial logit\n\n\nCode\n# multinomial logistic \nmodel_lr_f= LogisticRegression(C=0.01,multi_class='multinomial', max_iter=10000, dual=False, random_state=2020)\naccuracy_lr_f = cross_val_score(model_lr_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_lr_f= cross_val_score(model_lr_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1 weighted: %.3f (%.3f)' % (np.mean(f1w_lr_f), np.std(f1w_lr_f)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_lr_f), np.std(accuracy_lr_f)))\n\n\nMean f1 weighted: 0.568 (0.018)\nMean Accuracy: 0.647 (0.016)\n\n\n\n\n- linear svm\n\n\nCode\n# linear svm\nmodel_svcl_f = LinearSVC(C= 0.01 , penalty=\"l1\", dual=False, random_state=2020, max_iter=20000)\naccuracy_svcl_f = cross_val_score(model_svcl_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_svcl_f= cross_val_score(model_svcl_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1 weighted: %.3f (%.3f)' % (np.mean(f1w_svcl_f), np.std(f1w_svcl_f)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_svcl_f), np.std(accuracy_svcl_f)))\n\n\nMean f1 weighted: 0.563 (0.018)\nMean Accuracy: 0.648 (0.015)\n\n\n\n\n- rbf svm\n\n\nCode\n# quadratic svm\nmodel_svcq_g = SVC(kernel='rbf', random_state=2022, max_iter=22000)\nparameters= {'C': [500, 600, 650, 700, 750, 800, 900], 'gamma': [0.00004,0.00006,0.00008, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.001]}\n\n# Find Best Parameter C and gamma\nmodel_svcq_f = GridSearchCV(model_svcq_g, verbose=2, param_grid = parameters, cv = cv_5, n_jobs=-1,scoring='f1_weighted')\nmodel_svcq_f.fit(x_train5, y_train2)\n\n\nFitting 25 folds for each of 63 candidates, totalling 1575 fits\n\n\nGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=2022),\n             estimator=SVC(max_iter=22000, random_state=2022), n_jobs=-1,\n             param_grid={'C': [500, 600, 650, 700, 750, 800, 900],\n                         'gamma': [4e-05, 6e-05, 8e-05, 0.0001, 0.0002, 0.0003,\n                                   0.0004, 0.0005, 0.001]},\n             scoring='f1_weighted', verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=5, n_splits=5, random_state=2022),\n             estimator=SVC(max_iter=22000, random_state=2022), n_jobs=-1,\n             param_grid={'C': [500, 600, 650, 700, 750, 800, 900],\n                         'gamma': [4e-05, 6e-05, 8e-05, 0.0001, 0.0002, 0.0003,\n                                   0.0004, 0.0005, 0.001]},\n             scoring='f1_weighted', verbose=2)estimator: SVCSVC(max_iter=22000, random_state=2022)SVCSVC(max_iter=22000, random_state=2022)\n\n\n\n\nCode\nmodel_svcq_f_result = pd.DataFrame(model_svcq_f.cv_results_['params'])\nmodel_svcq_f_result['mean_f1_weighted_score'] = model_svcq_f.cv_results_['mean_test_score']\nmodel_svcq_f_result['std_f1_weighted_score'] = model_svcq_f.cv_results_['std_test_score']\ndisplay(model_svcq_f_result.sort_values(by='mean_f1_weighted_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      C\n      gamma\n      mean_f1_weighted_score\n      std_f1_weighted_score\n    \n  \n  \n    \n      15\n      600\n      0.0004\n      0.568534\n      0.019032\n    \n    \n      50\n      800\n      0.0003\n      0.568374\n      0.019318\n    \n    \n      52\n      800\n      0.0005\n      0.568364\n      0.018764\n    \n    \n      51\n      800\n      0.0004\n      0.568262\n      0.019490\n    \n    \n      24\n      650\n      0.0004\n      0.568231\n      0.019152\n    \n  \n\n\n\n\n\n\nCode\nmodel_svcq_f = SVC(kernel='rbf',C=600,gamma=0.0004, random_state=2022)\n\n# evaluate the model and collect the scores\naccuracy_svcq_f= cross_val_score(model_svcq_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_svcq_f= cross_val_score(model_svcq_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_svcq_f), np.std(f1w_svcq_f)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_svcq_f), np.std(accuracy_svcq_f)))\n\n\nMean f1_weighted: 0.569 (0.019)\nMean Accuracy: 0.651 (0.018)\n\n\n\n\n- knn\n\n\nCode\n# build a classifier\nmodel_knn_g = KNeighborsClassifier()\n\n# use a full grid over all parameters\nparam_grid = {\n    'n_neighbors' : np.arange(1,30).tolist(),\n    'weights' : [\"uniform\", \"distance\"],\n    'metric' : ['euclidean', 'manhattan', 'minkowski']\n}\n\n\n# run grid search\nmodel_knn = GridSearchCV(model_knn_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_knn.fit(x_train5, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_knn.cv_results_[\"params\"]))\n)\nreport(model_knn.cv_results_)\n\n\nFitting 25 folds for each of 174 candidates, totalling 4350 fits\nGridSearchCV took 5.55 seconds for 174 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.605 (std: 0.024)\nParameters: {'metric': 'manhattan', 'n_neighbors': 20, 'weights': 'distance'}\n\nModel with rank: 2\nMean validation score: 0.604 (std: 0.026)\nParameters: {'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'distance'}\n\nModel with rank: 3\nMean validation score: 0.604 (std: 0.021)\nParameters: {'metric': 'euclidean', 'n_neighbors': 19, 'weights': 'distance'}\n\nModel with rank: 3\nMean validation score: 0.604 (std: 0.021)\nParameters: {'metric': 'minkowski', 'n_neighbors': 19, 'weights': 'distance'}\n\n\n\n\n\nCode\nmodel_knn_f = KNeighborsClassifier(metric='manhattan', n_neighbors=20, weights='distance')\n\n# evaluate the model and collect the scores\naccuracy_knn_f= cross_val_score(model_knn_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_knn_f= cross_val_score(model_knn_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_knn_f), np.std(f1w_knn_f)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_knn_f), np.std(accuracy_knn_f)))\n\n\nMean f1_weighted: 0.605 (0.024)\nMean Accuracy: 0.635 (0.020)\n\n\n\n\n- SGD classifier\n\n\nCode\n# SGD classifier\n# build a classifier\nmodel_sgd_g = SGDClassifier(fit_intercept=True, max_iter=30000, alpha=0.1)\n\n# use a full grid over all parameters\nparam_grid = {\n    \"average\": [True, False],\n    'loss': ['hinge','log_loss','modified_huber','perceptron','squared_hinge'], \n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'l1_ratio': np.arange(0.3,0.7,0.1)\n}\n\n# run grid search\nmodel_sgd = GridSearchCV(model_sgd_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_sgd.fit(x_train5, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_sgd.cv_results_[\"params\"]))\n)\nreport(model_sgd.cv_results_)\n\n\nFitting 25 folds for each of 120 candidates, totalling 3000 fits\nGridSearchCV took 802.35 seconds for 120 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.583 (std: 0.032)\nParameters: {'average': True, 'l1_ratio': 0.5, 'loss': 'squared_hinge', 'penalty': 'l2'}\n\nModel with rank: 2\nMean validation score: 0.577 (std: 0.031)\nParameters: {'average': True, 'l1_ratio': 0.6000000000000001, 'loss': 'squared_hinge', 'penalty': 'l2'}\n\nModel with rank: 3\nMean validation score: 0.576 (std: 0.019)\nParameters: {'average': True, 'l1_ratio': 0.3, 'loss': 'perceptron', 'penalty': 'l2'}\n\n\n\n\n\nCode\nmodel_sgd_result = pd.DataFrame(model_sgd.cv_results_['params'])\nmodel_sgd_result['mean_f1_weighted_score'] = model_sgd.cv_results_['mean_test_score']\nmodel_sgd_result['std_f1_weighted_score'] = model_sgd.cv_results_['std_test_score']\ndisplay(model_sgd_result.sort_values(by='mean_f1_weighted_score', ascending=False).head(5))\n\n\n\n\n\n\n  \n    \n      \n      average\n      l1_ratio\n      loss\n      penalty\n      mean_f1_weighted_score\n      std_f1_weighted_score\n    \n  \n  \n    \n      42\n      True\n      0.5\n      squared_hinge\n      l2\n      0.582982\n      0.031619\n    \n    \n      57\n      True\n      0.6\n      squared_hinge\n      l2\n      0.577381\n      0.031336\n    \n    \n      9\n      True\n      0.3\n      perceptron\n      l2\n      0.576001\n      0.019470\n    \n    \n      54\n      True\n      0.6\n      perceptron\n      l2\n      0.575578\n      0.017628\n    \n    \n      24\n      True\n      0.4\n      perceptron\n      l2\n      0.574806\n      0.020559\n    \n  \n\n\n\n\n\n\nCode\nmodel_sgd_f = SGDClassifier(max_iter=30000, average=True, alpha=0.1, l1_ratio=0.5, loss='perceptron', penalty='l2')\n\n# evaluate the model and collect the scores\naccuracy_sgd_f = cross_val_score(model_sgd_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_sgd_f = cross_val_score(model_sgd_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_sgd_f), np.std(f1w_sgd_f)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_sgd_f), np.std(accuracy_sgd_f)))\n\n\nMean f1_weighted: 0.572 (0.019)\nMean Accuracy: 0.644 (0.017)\n\n\n\n\n- random forest\n\n\nCode\n# build a classifier\nmodel_rf_g = RandomForestClassifier(random_state=2022)\n\n# use a full grid over all parameters\nparam_grid = {\n    'n_estimators': [200, 500],\n    'max_depth' : [2,4,6,8,10],\n    'criterion' :['gini', 'entropy']\n}\n\n# run grid search\nmodel_rf = GridSearchCV(model_rf_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_rf.fit(x_train5, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_rf.cv_results_[\"params\"]))\n)\nreport(model_rf.cv_results_)\n\n\nFitting 25 folds for each of 20 candidates, totalling 500 fits\nGridSearchCV took 85.33 seconds for 20 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.593 (std: 0.024)\nParameters: {'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 200}\n\nModel with rank: 2\nMean validation score: 0.593 (std: 0.023)\nParameters: {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 200}\n\nModel with rank: 3\nMean validation score: 0.592 (std: 0.023)\nParameters: {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 200}\n\n\n\n\n\nCode\nmodel_rf_f = RandomForestClassifier(criterion=\"entropy\", max_depth=10, n_estimators=200,random_state=2020)\n\n# evaluate the model and collect the scores\naccuracy_rf_f = cross_val_score(model_rf_f , x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_rf_f = cross_val_score(model_rf_f , x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted: %.3f (%.3f)' % (np.mean(f1w_rf_f), np.std(f1w_rf_f)))\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(accuracy_rf_f), np.std(accuracy_rf_f)))\n\n\nMean f1_weighted: 0.594 (0.023)\nMean Accuracy: 0.640 (0.017)\n\n\n\n\n- gradient boosting tree\n\n\nCode\n# build a classifier\nmodel_bt_g = GradientBoostingClassifier(random_state=2022,loss='log_loss', n_iter_no_change = 10)\n\n# use a full grid over all parameters\nparam_grid = {\n    'n_estimators': np.arange(25,225,50).tolist(),\n    'max_depth' : [2,3,4],\n    'learning_rate': [0.01,0.05],\n    'max_features': np.arange(5,32,5).tolist()\n}\n\n# run grid search\nmodel_bt = GridSearchCV(model_bt_g, verbose= 1, param_grid=param_grid, cv=cv_5, n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_bt.fit(x_train5, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_bt.cv_results_[\"params\"]))\n)\nreport(model_bt.cv_results_)\n\n\nFitting 25 folds for each of 144 candidates, totalling 3600 fits\nGridSearchCV took 412.38 seconds for 144 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.599 (std: 0.024)\nParameters: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 15, 'n_estimators': 175}\n\nModel with rank: 2\nMean validation score: 0.599 (std: 0.025)\nParameters: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 15, 'n_estimators': 125}\n\nModel with rank: 3\nMean validation score: 0.599 (std: 0.024)\nParameters: {'learning_rate': 0.05, 'max_depth': 2, 'max_features': 15, 'n_estimators': 75}\n\n\n\n\n\nCode\nmodel_bt_f = GradientBoostingClassifier(random_state=2022,loss='log_loss', learning_rate=0.05, max_depth=2, max_features=15, n_estimators=175)\n\n# evaluate the model and collect the scores\naccuracy_bt_f= cross_val_score(model_bt_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_bt_f= cross_val_score(model_bt_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted bt: %.3f (%.3f)' % (np.mean(f1w_bt_f), np.std(f1w_bt_f)))\nprint('Mean Accuracy bt: %.3f (%.3f)' % (np.mean(accuracy_bt_f), np.std(accuracy_bt_f)))\n\n\nMean f1_weighted bt: 0.602 (0.022)\nMean Accuracy bt: 0.642 (0.021)\n\n\n\n\n- xgboost\n\n\nCode\n# build a classifier\nmodel_xgb_g = XGBClassifier(random_state=2022, objective='multi:softmax',num_class=3, n_estimators=25, learning_rate=0.1, max_depth=3)\n\n# use a full grid over all parameters\nparam_grid = {\n    'gamma' : np.arange(0,1.2,0.1).tolist(),\n    'alpha' : np.arange(0.5,1.5,0.1).tolist(),\n    'lambda' : np.arange(0.5,1.5,0.1).tolist()\n}\n\n# run grid search\nmodel_xgb = GridSearchCV(model_xgb_g, verbose= 3, param_grid=param_grid, cv=cv_5,n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_xgb.fit(x_train5, y_train2)\n\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_xgb.cv_results_[\"params\"]))\n)\nreport(model_xgb.cv_results_)\n\n\nFitting 25 folds for each of 1200 candidates, totalling 30000 fits\nGridSearchCV took 913.40 seconds for 1200 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.601 (std: 0.024)\nParameters: {'alpha': 0.5, 'gamma': 0.30000000000000004, 'lambda': 0.6}\n\nModel with rank: 2\nMean validation score: 0.601 (std: 0.025)\nParameters: {'alpha': 0.5, 'gamma': 0.2, 'lambda': 0.6}\n\nModel with rank: 3\nMean validation score: 0.600 (std: 0.025)\nParameters: {'alpha': 0.5, 'gamma': 0.0, 'lambda': 0.6}\n\nModel with rank: 3\nMean validation score: 0.600 (std: 0.025)\nParameters: {'alpha': 0.5, 'gamma': 0.1, 'lambda': 0.6}\n\n\n\n\n\nCode\nmodel_xgb_f = XGBClassifier(random_state=2022, objective='multi:softmax',num_class=3, n_estimators=25, learning_rate=0.1, max_depth=3,alpha=0.5, gamma=0.3, get_lambda=0.6)\n\n# evaluate the model and collect the scores\naccuracy_xgb_f = cross_val_score(model_xgb_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_xgb_f = cross_val_score(model_xgb_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted xgb: %.3f (%.3f)' % (np.mean(f1w_xgb_f), np.std(f1w_xgb_f)))\nprint('Mean Accuracy xgb: %.3f (%.3f)' % (np.mean(accuracy_xgb_f), np.std(accuracy_xgb_f)))\n\n\nMean f1_weighted xgb: 0.597 (0.024)\nMean Accuracy xgb: 0.645 (0.018)\n\n\n\n\n- linear discriminant analysis\n\n\nCode\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\n\n\nCode\nmodel_lda_g = LinearDiscriminantAnalysis()\nparameters= {\n    'solver': ['lsqr', 'eigen'], \n    'shrinkage': ['auto']+np.arange(0.1,0.9,0.1).tolist()\n    }\n\n# run grid search\nmodel_lda = GridSearchCV(model_lda_g, verbose= 1, param_grid=parameters, cv=cv_5, n_jobs=-1, scoring=\"f1_weighted\")\nstart = time()\nmodel_lda.fit(x_train5, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_lda.cv_results_[\"params\"]))\n)\nreport(model_lda.cv_results_)\n\n\nFitting 25 folds for each of 18 candidates, totalling 450 fits\nGridSearchCV took 0.53 seconds for 18 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.625 (std: 0.022)\nParameters: {'shrinkage': 0.5, 'solver': 'lsqr'}\n\nModel with rank: 1\nMean validation score: 0.625 (std: 0.022)\nParameters: {'shrinkage': 0.5, 'solver': 'eigen'}\n\nModel with rank: 3\nMean validation score: 0.624 (std: 0.020)\nParameters: {'shrinkage': 0.4, 'solver': 'lsqr'}\n\nModel with rank: 3\nMean validation score: 0.624 (std: 0.020)\nParameters: {'shrinkage': 0.4, 'solver': 'eigen'}\n\n\n\n\n\nCode\nmodel_lda_f = LinearDiscriminantAnalysis(shrinkage=0.5,solver='lsqr')\n\n# evaluate the model and collect the scores\naccuracy_lda_f = cross_val_score(model_lda_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_lda_f = cross_val_score(model_lda_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted lda: %.3f (%.3f)' % (np.mean(f1w_lda_f), np.std(f1w_lda_f)))\nprint('Mean Accuracy lda: %.3f (%.3f)' % (np.mean(accuracy_lda_f), np.std(accuracy_lda_f)))\n\n\nMean f1_weighted lda: 0.625 (0.022)\nMean Accuracy lda: 0.633 (0.023)\n\n\n\n\n- quadratic discriminant analysis\n\n\nCode\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n\n\n\nCode\nmodel_qda_g = QuadraticDiscriminantAnalysis()\nparameters= {'reg_param': np.arange(0,1,0.1).tolist()}\n\n# run grid search\nmodel_qda = GridSearchCV(model_qda_g, verbose= 1, param_grid=parameters, cv=cv_5, n_jobs=-1, scoring='f1_weighted')\nstart = time()\nmodel_qda.fit(x_train5, y_train2)\n\nprint(\n    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n    % (time() - start, len(model_qda.cv_results_[\"params\"]))\n)\nreport(model_qda.cv_results_)\n\n\nFitting 25 folds for each of 10 candidates, totalling 250 fits\nGridSearchCV took 0.25 seconds for 10 candidate parameter settings.\nModel with rank: 1\nMean validation score: 0.626 (std: 0.022)\nParameters: {'reg_param': 0.9}\n\nModel with rank: 2\nMean validation score: 0.626 (std: 0.022)\nParameters: {'reg_param': 0.8}\n\nModel with rank: 3\nMean validation score: 0.624 (std: 0.025)\nParameters: {'reg_param': 0.7000000000000001}\n\n\n\n\n\nCode\nmodel_qda_f = QuadraticDiscriminantAnalysis(reg_param=0.8)\n\n# evaluate the model and collect the scores\naccuracy_qda_f = cross_val_score(model_qda_f, x_train5, y_train2, scoring='accuracy', cv=cv_5, n_jobs=-1)\nf1w_qda_f = cross_val_score(model_qda_f, x_train5, y_train2, scoring='f1_weighted', cv=cv_5, n_jobs=-1)\n\n# report the model performance\nprint('Mean f1_weighted qda: %.3f (%.3f)' % (np.mean(f1w_qda_f), np.std(f1w_qda_f)))\nprint('Mean Accuracy qda: %.3f (%.3f)' % (np.mean(accuracy_qda_f), np.std(accuracy_qda_f)))\n\n\nMean f1_weighted qda: 0.626 (0.022)\nMean Accuracy qda: 0.627 (0.023)"
  },
  {
    "objectID": "posts/MLDL II/2021-29725_hw2.html#a-1",
    "href": "posts/MLDL II/2021-29725_hw2.html#a-1",
    "title": "Assignment 2",
    "section": "(a)",
    "text": "(a)\n[20 pts] Report the estimated test performance for your best model. Provide a reason for your choice of a model among the models you considered.\n\n\nCode\nresult=pd.DataFrame({\"multinomial logistic\": [np.mean(f1w_lr_f),np.std(f1w_lr_f),np.mean(accuracy_lr_f),np.std(accuracy_lr_f)],\n             \"linear svm\":[np.mean(f1w_svcl_f), np.std(f1w_svcl_f),np.mean(accuracy_svcl_f), np.std(accuracy_svcl_f)],\n             \"rbf svm\":[np.mean(f1w_svcq_f), np.std(f1w_svcq_f),np.mean(accuracy_svcq_f), np.std(accuracy_svcq_f)],\n             \"knn\":[np.mean(f1w_knn_f), np.std(f1w_knn_f),np.mean(accuracy_knn_f), np.std(accuracy_knn_f)],\n             \"sgd classifier\":[np.mean(f1w_sgd_f), np.std(f1w_sgd_f),np.mean(accuracy_sgd_f), np.std(accuracy_sgd_f)],\n             \"ramdom forest\":[np.mean(f1w_rf_f), np.std(f1w_rf_f),np.mean(accuracy_rf_f), np.std(accuracy_rf_f)],\n             \"gradient boosted tree\": [np.mean(f1w_bt_f), np.std(f1w_bt_f),np.mean(accuracy_bt_f), np.std(accuracy_bt_f)],\n             \"xgboost\": [np.mean(f1w_xgb_f), np.std(f1w_xgb_f),np.mean(accuracy_xgb_f), np.std(accuracy_xgb_f)],\n             \"linear discriminant analysis\":[np.mean(f1w_lda_f), np.std(f1w_lda_f),np.mean(accuracy_lda_f), np.std(accuracy_lda_f)],\n             \"quadratic discriminant analysis\":[np.mean(f1w_qda_f), np.std(f1w_qda_f),np.mean(accuracy_qda_f), np.std(accuracy_qda_f)] },\n    index=[\"Mean F1 weighted\", \"Std F1 weighted\",\"Mean Accuracy Score\",\"Std Accuracy Score\"]).round(3)\nresult.index.name='5-fold CV / feature selected (n=27)'\nresult=result.T\nresult[\"mean\"] = result.iloc[:, [0,2]].mean(axis=1)\ndisplay(result.sort_values(by='mean', ascending=False).round(3))\n\n\n\n\n\n\n  \n    \n      5-fold CV / feature selected (n=27)\n      Mean F1 weighted\n      Std F1 weighted\n      Mean Accuracy Score\n      Std Accuracy Score\n      mean\n    \n  \n  \n    \n      linear discriminant analysis\n      0.625\n      0.022\n      0.633\n      0.023\n      0.629\n    \n    \n      quadratic discriminant analysis\n      0.626\n      0.022\n      0.627\n      0.023\n      0.626\n    \n    \n      gradient boosted tree\n      0.602\n      0.022\n      0.642\n      0.021\n      0.622\n    \n    \n      xgboost\n      0.597\n      0.024\n      0.645\n      0.018\n      0.621\n    \n    \n      knn\n      0.605\n      0.024\n      0.635\n      0.020\n      0.620\n    \n    \n      ramdom forest\n      0.594\n      0.023\n      0.640\n      0.017\n      0.617\n    \n    \n      rbf svm\n      0.569\n      0.019\n      0.651\n      0.018\n      0.610\n    \n    \n      sgd classifier\n      0.572\n      0.019\n      0.644\n      0.017\n      0.608\n    \n    \n      multinomial logistic\n      0.568\n      0.018\n      0.647\n      0.016\n      0.607\n    \n    \n      linear svm\n      0.563\n      0.018\n      0.648\n      0.015\n      0.605"
  },
  {
    "objectID": "posts/MLDL II/2021-29725_hw2.html#b-1",
    "href": "posts/MLDL II/2021-29725_hw2.html#b-1",
    "title": "Assignment 2",
    "section": "(b)",
    "text": "(b)\n[20 pts] Predict on the provided test dataset, OnlineAd_X_test.csv, and save those predictions as a CSV file named [your-student-ID] pred.csv. The CSV file should only contain the array of dimension [300, 3] in the same format as the OnlineAd_Y_train.csv except the number of rows, since there are only 300 observations in the test data, i.e., the first column corresponds to no click, the second column corresponds to the ad A, and the third column corresponds to the ad B. A violation of this format guideline will result in 10 point penalty.\n\n\nCode\nx_test2 = x_test.iloc[:,features2.index].to_numpy()\n\n\n\n\nCode\n# choosed gradient boosted tree\nmodel_lda_f.fit(x_train5, y_train2)\nprediction = model_lda_f.predict(x_test2)\n\n\n\n\nCode\nprint(Counter(y_train2))\nprint(Counter(prediction))\n\n\nCounter({0: 822, 2: 353, 1: 277})\nCounter({0: 188, 2: 65, 1: 47})\n\n\n\n\nCode\nprediction_comp=pd.DataFrame({0: [822,188], 1: [277,47], 2: [353,65]})\nprediction_comp\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      822\n      277\n      353\n    \n    \n      1\n      188\n      47\n      65\n    \n  \n\n\n\n\n\n\nCode\nfrom scipy.stats import chi2_contingency\nchi, p, dof, expected = chi2_contingency(prediction_comp.to_numpy())\nprint(f\"chi-squared: {chi}\",\n      f\"p-value (0.05): {p}\",\n      f\"degree of freedom: {dof}\",\n      f\"expectation: \\n{pd.DataFrame(expected)}\")\n# It cannot be rejected that real clicks and the classifications are from different distributions (only if the tested data is randomly selected from the same distribution of training data)\n\n\nchi-squared: 3.8738967610162973 p-value (0.05): 0.1441431493460496 degree of freedom: 2 expectation: \n            0           1           2\n0  837.054795  268.520548  346.424658\n1  172.945205   55.479452   71.575342\n\n\n\n\nCode\nprediction_df=pd.DataFrame({0:np.where(prediction==0,1,0), 1:np.where(prediction==1,1,0), 2:np.where(prediction==2,1,0)})\nprediction_df.to_csv(\"E:/OneDrive - SNU/(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/과제2/OnlineAd_Y_test.csv\",index=False, header=False)"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html",
    "href": "posts/MLDL III1/hw3_1_knn.html",
    "title": "Assignment 3-1",
    "section": "",
    "text": "Deep Learning Basics, k-Nearest Neighbors (Score: 107/100)"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html#local-setup",
    "href": "posts/MLDL III1/hw3_1_knn.html#local-setup",
    "title": "Assignment 3-1",
    "section": "Local Setup",
    "text": "Local Setup\n\n\nCode\nimport os\n# get current path\nprint(os.getcwd())\n# change path\nos.chdir(\"E:/OneDrive - SNU/(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/HW3\")\nprint(os.getcwd())\n\n\ne:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\HW3\nE:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\HW3"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html#import-modules",
    "href": "posts/MLDL III1/hw3_1_knn.html#import-modules",
    "title": "Assignment 3-1",
    "section": "Import Modules",
    "text": "Import Modules\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html#k-nearest-neighbor-implementation",
    "href": "posts/MLDL III1/hw3_1_knn.html#k-nearest-neighbor-implementation",
    "title": "Assignment 3-1",
    "section": "K-Nearest Neighbor Implementation",
    "text": "K-Nearest Neighbor Implementation\n\n\nCode\nclass KNN:\n    \"\"\" k-nearest neighbor classifier class \"\"\"\n    ### counter 함수 \n    from collections import Counter \n    ###\n    def train(self, X, y):\n        \"\"\"\n        Train the classifier using the given training data (X, y).\n        Recall that for k-nearest neighbors this is just memorizing the training data.\n\n        Do NOT Modify this method.\n\n        Inputs\n        - X: A numpy array of shape (N, D), where N is the number of data points,\n            D is the dimensionality of each data point.\n        - y: A numpy array of shape (N,) containing the training labels, where\n            y[i] is the label for X[i]. With C classes, each y[i] is an integer\n            from 0 to C-1.\n        \"\"\"\n        self.X_train = X\n        self.y_train = y\n\n    def inference(self, X_test, k=1, dist_metric='dot'):\n        \"\"\"\n        For each test example in X, this method predicts its label by majority vote\n        from the k nearest training samples. It returns the predicted labels. \n\n        Do NOT Modify this method.\n\n        Inputs\n        - X_test: A numpy array of shape (N, D), where N is the number of test data points,\n            D is the dimensionality of each data point.\n        - X_train: A numpy array of shape (M, D), where M is the number of training data points,\n            D is the dimensionality of each data point.\n        - k: The number of neighbors to participate in voting.\n            dist_metric: Determines the distance metric to use. The default is dot-product ('dot'),\n            but you will need to implement 'l2' for question (b).\n        Returns\n        - y_pred: A numpy array of shape (N,) containing predicted labels for the test data X,\n            where y_pred[i] is the predicted label for the test point X[i].\n        \"\"\"\n        dists = self.compute_distance(X_test, dist_metric)\n        y_pred = self.predict_labels(X_test, dists, k)\n        return y_pred\n\n    def compute_distance(self, X_test, dist_metric='l2'):\n        \"\"\"\n        Computes the distance between the training data and test data, \n        using dot-product similarity or Euclidean (L2) distance as the distance metric.\n\n        Question (a)\n\n        Inputs\n        - X_test: A numpy array of shape (N, D), where N is the number of test data points,\n            D is the dimensionality of each data point.\n        - X_train: A numpy array of shape (M, D), where M is the number of training data points,\n            D is the dimensionality of each data point.\n        - dist_metric: Determines the distance metric to use.\n        Returns\n        - dists: A numpy array of shape (N, M) where N is the number of test data points, \n            and M is the number of traininig data points, containing distances between \n            each pair of test and train data points based on the given distance metric.\n        \"\"\"\n        if dist_metric=='dot':\n            dist_list=[]\n            for i in np.arange(X_test.shape[0]):\n                row=[]\n                for k in np.arange(self.X_train.shape[0]):\n                    cos_dist=-np.dot(X_test[i], self.X_train[k])\n                    row.append(cos_dist)\n                dist_list.append(row)\n            dists=np.array(dist_list) \n\n        elif dist_metric=='cos':\n            dist_list=[]\n            for i in np.arange(X_test.shape[0]):\n                row=[]\n                for k in np.arange(self.X_train.shape[0]):\n                    cos_dist=-np.dot(X_test[i], self.X_train[k])/(np.linalg.norm(X_test[i])*np.linalg.norm(self.X_train[k]))\n                    row.append(cos_dist)                \n                dist_list.append(row)\n            dists=np.array(dist_list) \n\n        elif dist_metric=='l2':          \n            dist_list=[]\n            for i in np.arange(X_test.shape[0]):\n                row=[]\n                for k in np.arange(self.X_train.shape[0]):\n                    l2_dist=np.sqrt(np.sum((X_test[i] - self.X_train[k]) ** 2))\n                    row.append(l2_dist)\n                dist_list.append(row)\n            dists=np.array(dist_list)          \n        return dists\n\n    def predict_labels(self, X_test, dists, k):\n        \"\"\"\n        For the given test image, this method takes a majority vote from k closest points\n        to predict the class of the test image.\n\n        Question (b)\n\n        Inputs\n        - X_test: A numpy array of shape (N, D), where N is the number of test data points,\n            D is the dimensionality of each data point.\n        - dists: A numpy array of shape (N, M) where N is the number of test data points, \n            and M is the number of traininig data points, containing distances between \n            each pair of test and train data points based on the given distance metric.\n        - k: The number of neighbors to participate in voting.\n        Returns\n        - y_pred: A numpy array of shape (N,) containing predicted labels for the test data X,\n            where y_pred[i] is the predicted label for the test point X[i].\n        \"\"\"\n        y_pred_list = []\n        for idx in np.argsort(dists,axis=1)[:,-k:]:\n            idx_count = Counter(self.y_train[idx].flatten())\n            y_pred_list.append(idx_count.most_common(1)[0][0])\n            \n        y_pred=np.array(y_pred_list)\n        \n        return y_pred\n\n    def evaluate(self, y, y_hat):\n        \"\"\"\n        Compares the predicted labels to the ground truth y, and prints the\n        classification accuracy.\n        \n        Do NOT Modify this method.\n\n        Inputs\n        - y: A numpy array of shape (N,) containing the ground truth labels, where\n            N is the number of test examples. With C classes, each y[i] is an integer\n            from 0 to C-1.\n        - y_hat: A numpy array of shape (N,) containing the predicted labels, where\n            N is the number of test examples. With C classes, each y_pred[i] is\n            an integer from 0 to C-1.\n        Returns:\n        - accuracy\n        \"\"\"\n        y_hat = np.expand_dims(y_hat, axis=1)\n        num_correct = np.sum(y_hat == y)\n        accuracy = float(num_correct) / y.shape[0]\n        return accuracy"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html#data-loading-mnist",
    "href": "posts/MLDL III1/hw3_1_knn.html#data-loading-mnist",
    "title": "Assignment 3-1",
    "section": "Data Loading (MNIST)",
    "text": "Data Loading (MNIST)\n\n\nCode\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nclass MNIST:\n    \"\"\" Dataset and DataLoader for the MNIST dataset \"\"\"\n\n    def __init__(self, train_batch_size, test_batch_size):\n        \"\"\"\n        Do NOT modify this function.\n        \"\"\"\n        self.test_batch_size = test_batch_size\n        self.train_batch_size = train_batch_size\n  \n    def set_data(self):\n        \"\"\"\n        Download and Set the Training and Test data using the MNIST dataset\n        Use \"torchvision.datasets\" module.\n\n        Question (c)\n\n        Conditions :\n        - Data should be \"downloaded\" in the \"data\" directory of the current path\n        - Data should be in \"Tensor\" form of pytorch.\n\n        Returns :\n        - train_data \n        - test_data \n        \"\"\" \n        \n        print(\"Downloading the MNIST data...\")\n        ##### YOUR CODE #####\n        train_data = datasets.MNIST(root=\"\\data\", train=True, download=True, transform=ToTensor())\n        test_data = datasets.MNIST(root=\"\\data\", train=False, download=True, transform=ToTensor())\n        #####################\n\n        return train_data, test_data\n\n    def load_data(self, train_data, test_data):\n        \"\"\"\n        Create DataLoaders that iterate the train_data and the test_data.\n        Each iteration of the dataloaders should return a batch of the data.\n        Make sure that the data is shuffled after iterating over all batches.\n        Use DataLoader class in pytorch.\n\n        Question (c)\n\n        Input :\n        - train_data\n        - test_data\n\n        Returns :\n        - train_loader \n        - test_loader \n        \"\"\" \n        ##### YOUR CODE #####\n        train_loader = DataLoader(train_data, batch_size=self.train_batch_size, shuffle=True)\n        test_loader = DataLoader(test_data, batch_size=self.test_batch_size, shuffle=True)\n        #####################\n\n        return train_loader, test_loader\n\n    def print_example(self, test_loader):\n        \"\"\"\n        Check out a sample of the test data.\n\n        Question (c)\n        (1) Extract one batch from the test_loader and print out a shape of the images in the batch.\n        (2) Print out 6 images in a rectangular form (height*width = 2*3).\n\n        Conditions :\n        - Print out the Ground truths (labels) above each image as title.\n            ex. Ground Truch : 8\n        - Remove ticks and tick labels in the plot.\n        - Print out the images in grayscale.\n        - Refer to the format of the expected result below for your answer. \n\n        Input :\n        - test_loader \n        \"\"\" \n\n        ##### YOUR CODE for (1) #####\n        dataiter = enumerate(test_loader)\n        batch_idx, (example_data, example_targets) = next(dataiter)\n        #############################\n        print(\"A shape of the images in a batch:\", example_data.shape)\n\n        \n        fig = plt.figure()\n        for i in range(6):\n          plt.subplot(2, 3, i + 1)\n          ##### YOUR CODE for (2) #####\n          plt.tight_layout()\n          plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n          plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n          plt.xticks([])\n          plt.yticks([])\n          #############################\n        print(fig)\n\n\n\n\nCode\nnum_train_data = 5000\nnum_test_data = 1000\n\nmnist = MNIST(train_batch_size=num_train_data, test_batch_size=num_test_data)\ntrain_data, test_data = mnist.set_data()\ntrain_loader, test_loader = mnist.load_data(train_data, test_data)\nmnist.print_example(test_loader)\n\n\nDownloading the MNIST data...\nA shape of the images in a batch: torch.Size([1000, 1, 28, 28])\nFigure(640x480)\n\n\n\n\n\n\n\nCode\nX_train, y_train = next(iter(train_loader))\nX_train = X_train.reshape(num_train_data, -1).numpy()\ny_train = y_train.unsqueeze(1).numpy()\n\nX_test, y_test = next(iter(test_loader))\nX_test = X_test.reshape(num_test_data, -1).numpy()\ny_test = y_test.unsqueeze(1).numpy()\n\nprint('Sampling Training & Test Data.....\\n')\nprint('Training data shape: {}'.format(X_train.shape))\nprint('Training labels shape: {}'.format(y_train.shape))\nprint('Test data shape: {}'.format(X_test.shape))\nprint('Test labels shape: {}'.format(y_test.shape))\n\n\nSampling Training & Test Data.....\n\nTraining data shape: (5000, 784)\nTraining labels shape: (5000, 1)\nTest data shape: (1000, 784)\nTest labels shape: (1000, 1)"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html#model-training-evaluation",
    "href": "posts/MLDL III1/hw3_1_knn.html#model-training-evaluation",
    "title": "Assignment 3-1",
    "section": "Model Training & Evaluation",
    "text": "Model Training & Evaluation\n\n\nCode\nmodel = KNN()\nmodel.train(X_train, y_train)\n\n\n\n\nCode\n\"\"\"\nModel usage for test.\n\"\"\"\nK = 15\ny_pred = model.inference(X_test, k=K, dist_metric='l2')\nacc = model.evaluate(y_test, y_pred)\nprint(\"Accuarcy:\", acc)\n\n\nAccuarcy: 0.928"
  },
  {
    "objectID": "posts/MLDL III1/hw3_1_knn.html#experiments",
    "href": "posts/MLDL III1/hw3_1_knn.html#experiments",
    "title": "Assignment 3-1",
    "section": "Experiments",
    "text": "Experiments\n\n\nCode\n# Modify the number of k's and metrics to try as you want\nnum_ks = 50\nmetrics = ['dot', 'cos', 'l2']\n\n\n\n\nCode\n# Run experiments\nprint_k_interval = 5\nresult = dict(zip(metrics, [[] for _ in range(len(metrics))]))\nfor metric in metrics:\n    print(\"running KNN with {} distance metric\".format(metric))\n    for k in range(1, num_ks+1):\n        if k % print_k_interval==0:\n            print(\"    processing... k={:3d}\".format(k))\n        y_pred = model.inference(X_test, k=k, dist_metric=metric)\n        acc = model.evaluate(y_test, y_pred)\n        result[metric].append(acc)\n    print()\n\n\nrunning KNN with dot distance metric\n    processing... k=  5\n    processing... k= 10\n    processing... k= 15\n    processing... k= 20\n    processing... k= 25\n    processing... k= 30\n    processing... k= 35\n    processing... k= 40\n    processing... k= 45\n    processing... k= 50\n\nrunning KNN with cos distance metric\n    processing... k=  5\n    processing... k= 10\n    processing... k= 15\n    processing... k= 20\n    processing... k= 25\n    processing... k= 30\n    processing... k= 35\n    processing... k= 40\n    processing... k= 45\n    processing... k= 50\n\nrunning KNN with l2 distance metric\n    processing... k=  5\n    processing... k= 10\n    processing... k= 15\n    processing... k= 20\n    processing... k= 25\n    processing... k= 30\n    processing... k= 35\n    processing... k= 40\n    processing... k= 45\n    processing... k= 50\n\n\n\n\n\nCode\n# Visualize the result\nfig = plt.figure(figsize=(13,6))\nax = fig.add_subplot(1,1,1)\n\nx_axis = np.arange(1, num_ks+1, 1)\nfor i, metric in enumerate(metrics):\n    ax.plot(x_axis, result[metric], 'o-', label = metric)\n\nax.set(title=\"K-Nearest Neighbor Accuracies on different Ks\")\nax.set(xlabel='K', ylabel='Accuracy')\nax.set(xticks=np.arange(0, num_ks+1,5), yticks=np.arange(0.5,1.0,0.05))\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\n### \n# Question (d)\n# Briefly report what you observe in the plot above.\n###\n\n\nCosine Similarity를 통해 계산한 MNIST Dataset의 train set과 test set내 각각의 그림에 대한 학습이 3가지 유사도 기법 중 가장 높게 나타난 것으로 보인다. Dot Similarity의 경우 다른 두 유사도 metric 들에 비해서 정확도가 0.2가량 낮게 계산되었으며 Negative least squares과 Cosine similarity의 경우 서로 값에서 큰 차이가 나타나지 않는 것으로 보인다. 고려해야 하는 이웃의 수 (k) 값의 경우 그 값이 증가할 수록 정확도가 일정 수준까지는 증가하지만, k값이 계속 증가할경우 overfitting 으로 특정 k값 부터는 정확도가 오히려 감소하게 되는 것으로 보인다. Cosine Similarity의 차원에서 k=7일 때 test data에 대한 분류 정확도가 가장 높게 나타나는 것으로 보인다는 점에서 이웃의 갯수가 많다고 해서 반드시 정확도가 높아지는 것은 아닌 것으로 보인다. MNIST와 같이 간단한 그림의 경우, 색이 흑백으로 단순하고 데이터의 크기가 크지 않다는 점에서 딥러닝 기법들을 사용하지 않더라도 예측 정확도가 충분히 높게 나타나고 있는 것으로 보인다. 특히, train 데이터가 500개일 때보다 5000개일 때 정확도 값이 더 높게 계산되어진다는 점에서 knn의 경우 matching 되는 데이터의 갯수가 클 때 충분히 높은 정확도를 보인다. 다만, 분류 모델에 train set 그림 전부를 포함해야 한다는 점에서 모형의 용량이 과도하게 커진다는 근본적 한계가 존재하므로 유의해야 한다."
  },
  {
    "objectID": "posts/MLDL III2/hw3_2_2nn.html",
    "href": "posts/MLDL III2/hw3_2_2nn.html",
    "title": "Assignment 3-2",
    "section": "",
    "text": "2NN and Fully Connected Layers (Score: 107/100)\n\nLocal Setup\n\n\nCode\nimport os\n# get current path\nprint(os.getcwd())\n# change path\nos.chdir(\"E:/OneDrive - SNU/(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/과제3\")\nprint(os.getcwd())\n\n\ne:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제3\nE:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제3\n\n\n\n\nImport Modules\n\n\nCode\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mnist.data_utils import load_data\n\n\n\n\nUtils\n\n\nCode\ndef tanh(z):\n    \"\"\"\n    Implement the tanh activation function.\n    The method takes the input z and returns the output of the function.\n\n    Question (a)\n\n    \"\"\"\n    ##### YOUR CODE #####\n    tanh = lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n    result = tanh(z)\n    return result   \n    ##################### \n\ndef softmax(X):\n    \"\"\"\n    Implement the softmax function.\n    The method takes the input X and returns the output of the function.\n\n    Question (a)\n\n    \"\"\"\n    ##### YOUR CODE #####\n    elements = np.exp(X-np.amax(X,axis=1,keepdims=True))\n    sums = np.sum(elements, axis=1, keepdims=True)\n    return elements/sums\n    #####################\n\n\ndef load_batch(X, Y, batch_size, shuffle=True):\n    \"\"\"\n    Generates batches with the remainder dropped.\n\n    Do NOT modify this function\n    \"\"\"\n    if shuffle:\n        permutation = np.random.permutation(X.shape[0])\n        X = X[permutation, :]\n        Y = Y[permutation, :]\n    num_steps = int(X.shape[0])//batch_size\n    step = 0\n    while step<num_steps:\n        X_batch = X[batch_size*step:batch_size*(step+1)]\n        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n        step+=1\n        yield X_batch, Y_batch\n\n\n\n\n2-Layer Neural Network\n\n\nCode\nclass TwoLayerNN:\n    \"\"\" a neural network with 2 layers \"\"\"\n\n    def __init__(self, input_dim, num_hiddens, num_classes):\n        \"\"\"\n        Do NOT modify this function.\n        \"\"\"\n        self.input_dim = input_dim\n        self.num_hiddens = num_hiddens\n        self.num_classes = num_classes\n        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n\n    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n        \"\"\"\n        initializes parameters with Xavier Initialization.\n\n        Question (b)\n        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization \n        \n        Inputs\n        - input_dim\n        - num_hiddens\n        - num_classes\n        Returns\n        - params: a dictionary with the initialized parameters.\n        \"\"\"\n        params = {}\n        ##### YOUR CODE #####\n        params[\"W1\"] = np.random.uniform(low=-1/(np.sqrt(num_hiddens)), high=1/(np.sqrt(num_hiddens)), size=(input_dim, num_hiddens))\n        params[\"b1\"] = np.zeros(num_hiddens)\n        params[\"W2\"] = np.random.uniform(low=-1/(np.sqrt(num_classes)), high=1/(np.sqrt(num_classes)), size=(num_hiddens, num_classes))\n        params[\"b2\"] = np.zeros(num_classes)\n        #####################\n        return params\n\n    def forward(self, X):\n        \"\"\"\n        Define and perform the feed forward step of a two-layer neural network.\n        Specifically, the network structue is given by\n\n          y = softmax(tanh(X W1 + b1) W2 + b2)\n\n        where X is the input matrix of shape (N, D), y is the class distribution matrix\n        of shape (N, C), N is the number of examples (either the entire dataset or\n        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n\n        Question (c)\n        - ff_dict will be used to run backpropagation in backward method.\n\n        Inputs\n        - X: the input matrix of shape (N, D)\n\n        Returns\n        - y: the output of the model\n        - ff_dict: a dictionary with all the fully connected units and activations.\n        \"\"\"\n        ff_dict = {}\n        ##### YOUR CODE #####\n        params = self.params\n        w1 = params[\"W1\"] \n        b1 = params[\"b1\"] \n        w2 = params[\"W2\"] \n        b2 = params[\"b2\"] \n\n        R1 = np.dot(X, w1) + b1 # N * H\n        Act1 = tanh(R1) # N * H\n        R2 = np.dot(Act1, w2) + b2 # N * C \n        Act2 = softmax(R2) # N * C\n\n        ff_dict['Affine1'] =  R1\n        ff_dict['Tanh'] = Act1\n        ff_dict['Affine2'] = R2\n        ff_dict['Softmax'] = Act2\n        y = Act2\n        #####################\n        return y, ff_dict\n\n    def backward(self, X, Y, ff_dict):\n        \"\"\"\n        Performs backpropagation over the two-layer neural network, and returns\n        a dictionary of gradients of all model parameters.\n\n        Question (d)\n\n        Inputs:\n         - X: the input matrix of shape (B, D), where B is the number of examples\n              in a mini-batch, D is the feature dimensionality.\n         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n              where B is the number of examples in a mini-batch, C is the number\n              of classes.\n         - ff_dict: the dictionary containing all the fully connected units and\n              activations.\n\n        Returns:\n         - grads: a dictionary containing the gradients of corresponding weights and biases.\n        \"\"\"\n        grads = {}\n        ##### YOUR CODE #####\n        batch_size = X.shape[0]\n        Act1 = ff_dict['Affine1'] \n        z1 = ff_dict['Tanh']\n        z2 = ff_dict['Softmax']\n        w2 = self.params[\"W2\"]\n\n        dz2 = (z2-Y) / batch_size  # normalization & entropy diff\n        grads[\"dW2\"] = np.dot(z1.T, dz2)\n        grads[\"db2\"] = np.sum(dz2, axis = 0)\n    \n        dz1 = (1-tanh(Act1)) * (1+tanh(Act1)) * np.dot(dz2, w2.T) # tanh diff\n        grads[\"dW1\"] = np.dot(X.T, dz1)\n        grads[\"db1\"] = np.sum(dz1, axis = 0)\n        #####################\n        return grads\n\n\n    def compute_loss(self, Y, Y_hat):\n        \"\"\"\n        Computes cross entropy loss.\n\n        Do NOT modify this function.\n\n        Inputs\n            Y:\n            Y_hat:\n        Returns\n            loss:\n        \"\"\"\n        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n        return loss\n\n    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n        \"\"\"\n        Runs mini-batch gradient descent.\n\n        Do NOT Modify this method.\n\n        Inputs\n        - X\n        - Y\n        - X_val\n        - Y_Val\n        - lr\n        - n_epochs\n        - batch_size\n        - log_interval\n        \"\"\"\n        for epoch in range(n_epochs):\n            for X_batch, Y_batch in load_batch(X, Y, batch_size):\n                self.train_step(X_batch, Y_batch, batch_size, lr)\n            if epoch % log_interval==0:\n                Y_hat, ff_dict = self.forward(X)\n                train_loss = self.compute_loss(Y, Y_hat)\n                train_acc = self.evaluate(Y, Y_hat)\n                Y_hat, ff_dict = self.forward(X_val)\n                valid_loss = self.compute_loss(Y_val, Y_hat)\n                valid_acc = self.evaluate(Y_val, Y_hat)\n                print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n                      format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n\n    def train_step(self, X_batch, Y_batch, batch_size, lr):\n        \"\"\"\n        Updates the parameters using gradient descent.\n\n        Do NOT Modify this method.\n\n        Inputs\n        - X_batch\n        - Y_batch\n        - batch_size\n        - lr\n        \"\"\"\n        _, ff_dict = self.forward(X_batch)\n        grads = self.backward(X_batch, Y_batch, ff_dict)\n        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n\n    def evaluate(self, Y, Y_hat):\n        \"\"\"\n        Computes classification accuracy.\n        \n        Do NOT modify this function\n\n        Inputs\n        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n             where C is the number of classes.\n        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n             where C is the number of classes.\n\n        Returns\n            accuracy: the classification accuracy in float\n        \"\"\"        \n        classes_pred = np.argmax(Y_hat, axis=1)\n        classes_gt = np.argmax(Y, axis=1)\n        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n        return accuracy\n\n\n\n\nLoad MNIST\n\n\nCode\nX_train, Y_train, X_test, Y_test = load_data()\n\nidxs = np.arange(len(X_train))\nnp.random.shuffle(idxs)\nsplit_idx = int(np.ceil(len(idxs)*0.8))\nX_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\nX_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\nprint()\nprint('Set validation data aside')\nprint('Training data shape: ', X_train.shape)\nprint('Training labels shape: ', Y_train.shape)\nprint('Validation data shape: ', X_valid.shape)\nprint('Validation labels shape: ', Y_valid.shape)\n\n\nMNIST data loaded:\nTraining data shape: (60000, 784)\nTraining labels shape: (60000, 10)\nTest data shape: (10000, 784)\nTest labels shape: (10000, 10)\n\nSet validation data aside\nTraining data shape:  (48000, 784)\nTraining labels shape:  (48000, 10)\nValidation data shape:  (12000, 784)\nValidation labels shape:  (12000, 10)\n\n\n\n\nTraining & Evaluation\n\n\nCode\n### \n# Question (e)\n# Tune the hyperparameters with validation data, \n# and print the results by running the lines below.\n###\n\n\n\n\nCode\n# model instantiation\nmodel = TwoLayerNN(input_dim=784, num_hiddens=80, num_classes=10)\n\n\n\n\nCode\n# train the model\nlr, n_epochs, batch_size = 3, 40, 128\nmodel.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n\n\nepoch 00 - train loss/acc: 0.512 0.867, valid loss/acc: 0.517 0.866\nepoch 01 - train loss/acc: 0.406 0.888, valid loss/acc: 0.411 0.888\nepoch 02 - train loss/acc: 0.361 0.899, valid loss/acc: 0.366 0.899\nepoch 03 - train loss/acc: 0.333 0.906, valid loss/acc: 0.340 0.904\nepoch 04 - train loss/acc: 0.314 0.911, valid loss/acc: 0.321 0.909\nepoch 05 - train loss/acc: 0.299 0.915, valid loss/acc: 0.307 0.913\nepoch 06 - train loss/acc: 0.286 0.918, valid loss/acc: 0.295 0.917\nepoch 07 - train loss/acc: 0.275 0.921, valid loss/acc: 0.285 0.920\nepoch 08 - train loss/acc: 0.266 0.925, valid loss/acc: 0.277 0.922\nepoch 09 - train loss/acc: 0.257 0.927, valid loss/acc: 0.268 0.924\nepoch 10 - train loss/acc: 0.249 0.930, valid loss/acc: 0.261 0.927\nepoch 11 - train loss/acc: 0.241 0.932, valid loss/acc: 0.254 0.928\nepoch 12 - train loss/acc: 0.234 0.935, valid loss/acc: 0.248 0.930\nepoch 13 - train loss/acc: 0.228 0.937, valid loss/acc: 0.242 0.931\nepoch 14 - train loss/acc: 0.222 0.938, valid loss/acc: 0.237 0.932\nepoch 15 - train loss/acc: 0.216 0.940, valid loss/acc: 0.232 0.933\nepoch 16 - train loss/acc: 0.211 0.942, valid loss/acc: 0.226 0.935\nepoch 17 - train loss/acc: 0.205 0.943, valid loss/acc: 0.222 0.935\nepoch 18 - train loss/acc: 0.200 0.944, valid loss/acc: 0.218 0.938\nepoch 19 - train loss/acc: 0.196 0.945, valid loss/acc: 0.213 0.939\nepoch 20 - train loss/acc: 0.192 0.947, valid loss/acc: 0.209 0.940\nepoch 21 - train loss/acc: 0.187 0.948, valid loss/acc: 0.206 0.941\nepoch 22 - train loss/acc: 0.183 0.949, valid loss/acc: 0.202 0.942\nepoch 23 - train loss/acc: 0.179 0.950, valid loss/acc: 0.199 0.943\nepoch 24 - train loss/acc: 0.176 0.951, valid loss/acc: 0.196 0.943\nepoch 25 - train loss/acc: 0.172 0.953, valid loss/acc: 0.192 0.945\nepoch 26 - train loss/acc: 0.169 0.953, valid loss/acc: 0.190 0.946\nepoch 27 - train loss/acc: 0.166 0.954, valid loss/acc: 0.186 0.947\nepoch 28 - train loss/acc: 0.162 0.955, valid loss/acc: 0.183 0.947\nepoch 29 - train loss/acc: 0.160 0.955, valid loss/acc: 0.181 0.948\nepoch 30 - train loss/acc: 0.157 0.957, valid loss/acc: 0.179 0.949\nepoch 31 - train loss/acc: 0.154 0.958, valid loss/acc: 0.176 0.949\nepoch 32 - train loss/acc: 0.151 0.959, valid loss/acc: 0.173 0.949\nepoch 33 - train loss/acc: 0.148 0.959, valid loss/acc: 0.171 0.950\nepoch 34 - train loss/acc: 0.146 0.960, valid loss/acc: 0.169 0.951\nepoch 35 - train loss/acc: 0.144 0.960, valid loss/acc: 0.167 0.952\nepoch 36 - train loss/acc: 0.141 0.962, valid loss/acc: 0.165 0.952\nepoch 37 - train loss/acc: 0.139 0.962, valid loss/acc: 0.163 0.953\nepoch 38 - train loss/acc: 0.136 0.962, valid loss/acc: 0.160 0.953\nepoch 39 - train loss/acc: 0.135 0.963, valid loss/acc: 0.159 0.954\n\n\n\n\nCode\n# evalute the model on test data\nY_hat, _ = model.forward(X_test)\ntest_loss = model.compute_loss(Y_test, Y_hat)\ntest_acc = model.evaluate(Y_test, Y_hat)\nprint(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))\n\n\nFinal test loss = 0.149, acc = 0.956\n\n\n\n\nExtra Credit (Optional)\n\n\nCode\n######### define relu #############\nclass relu:\n    def fw(x):\n        z=x.copy()\n        fwd = lambda k: np.maximum(0, k)\n        result = np.vectorize(fwd)(z)\n        return result\n    def bw(x):\n        z=x.copy()\n        bwd = lambda k : 0 if k<=0 else 1\n        result = np.vectorize(bwd)(z)\n        return result        \n###################################\n\ndef initialize_parameters(self, input_dim, num_hiddens, num_classes):\n    \"\"\"\n    initializes parameters with He Initialization.\n\n    Question (f)\n    - refer to https://paperswithcode.com/method/he-initialization for He initialization \n    \n    Inputs\n    - input_dim\n    - num_hiddens\n    - num_classes\n    Returns\n    - params: a dictionary with the initialized parameters.\n    \"\"\"\n\n    params = {}\n    ##### YOUR CODE #####\n    params[\"W1\"] = np.random.uniform(low=-1/(np.sqrt(num_hiddens)), high=1/(np.sqrt(num_hiddens)), size=(input_dim, num_hiddens))\n    params[\"b1\"] = np.zeros(num_hiddens)\n    params[\"W2\"] = np.random.uniform(low=-1/(np.sqrt(num_classes)), high=1/(np.sqrt(num_classes)), size=(num_hiddens, num_classes))\n    params[\"b2\"] = np.zeros(num_classes)\n    #####################\n    return params\n\ndef forward_relu(self, X):\n    \"\"\"\n    Defines and performs the feed forward step of a two-layer neural network.\n    Specifically, the network structue is given by\n\n        y = softmax(relu(X W1 + b1) W2 + b2)\n\n    where X is the input matrix of shape (N, D), y is the class distribution matrix\n    of shape (N, C), N is the number of examples (either the entire dataset or\n    a mini-batch), D is the feature dimensionality, and C is the number of classes.\n\n    Question (f)\n\n    Inputs\n        X: the input matrix of shape (N, D)\n\n    Returns\n        y: the output of the model\n        ff_dict: a dictionary containing all the fully connected units and activations.\n    \"\"\"\n\n    ff_dict = {}        \n    ##### YOUR CODE #####\n    params = self.params\n    w1 = params[\"W1\"] \n    b1 = params[\"b1\"] \n    w2 = params[\"W2\"] \n    b2 = params[\"b2\"] \n\n    R1 = np.dot(X, w1) + b1 # N * H\n    Act1 = relu.fw(R1) # N * H\n    R2 = np.dot(Act1, w2) + b2 # N * C \n    Act2 = softmax(R2) # N * C\n\n    ff_dict['Affine1'] =  R1\n    ff_dict['ReLU'] = Act1\n    ff_dict['Affine2'] = R2\n    ff_dict['Softmax'] = Act2\n    y = Act2\n    #####################\n    return y, ff_dict\n\ndef backward_relu(self, X, Y, ff_dict):\n    \"\"\"\n    Performs backpropagation over the two-layer neural network, and returns\n    a dictionary of gradients of all model parameters.\n\n    Question (f)\n\n    Inputs:\n        - X: the input matrix of shape (B, D), where B is the number of examples\n            in a mini-batch, D is the feature dimensionality.\n        - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n            where B is the number of examples in a mini-batch, C is the number\n            of classes.\n        - ff_dict: the dictionary containing all the fully connected units and\n            activations.\n\n    Returns:\n        - grads: a dictionary containing the gradients of corresponding weights\n            and biases.\n    \"\"\"\n\n    grads = {}\n    ##### YOUR CODE #####\n    batch_size = X.shape[0]\n    Act1 = ff_dict['Affine1'] \n    z1 = ff_dict['ReLU']\n    z2 = ff_dict['Softmax']\n    w2 = self.params[\"W2\"]\n\n    dz2 = (z2-Y) / batch_size  # normalization & entropy diff\n    grads[\"dW2\"] = np.dot(z1.T, dz2)\n    grads[\"db2\"] = np.sum(dz2, axis = 0)\n    \n    dz1 = relu.bw(Act1) * np.dot(dz2, w2.T) # relu diff\n    grads[\"dW1\"] = np.dot(X.T, dz1)\n    grads[\"db1\"] = np.sum(dz1, axis = 0)\n    #####################\n    return grads\n\nTwoLayerNNRelu = copy.copy(TwoLayerNN)\nTwoLayerNNRelu.initialize_parameters = initialize_parameters\nTwoLayerNNRelu.forward = forward_relu\nTwoLayerNNRelu.backward = backward_relu\n\n\n\n\nCode\n### \n# Question (f)\n# Tune the hyperparameters with validation data,\n# and print the results by running the lines below.\n###\n\n\n\n\nCode\n# model instantiation\nmodel_relu = TwoLayerNNRelu(input_dim=784, num_hiddens=80, num_classes=10)\n\n\n\n\nCode\n# train the model\nlr, n_epochs, batch_size = 2.5, 40, 128\nhistory = model_relu.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)\n\n\nepoch 00 - train loss/acc: 0.511 0.868, valid loss/acc: 0.517 0.866\nepoch 01 - train loss/acc: 0.399 0.890, valid loss/acc: 0.408 0.886\nepoch 02 - train loss/acc: 0.353 0.901, valid loss/acc: 0.361 0.897\nepoch 03 - train loss/acc: 0.325 0.909, valid loss/acc: 0.333 0.905\nepoch 04 - train loss/acc: 0.306 0.913, valid loss/acc: 0.314 0.909\nepoch 05 - train loss/acc: 0.290 0.919, valid loss/acc: 0.299 0.915\nepoch 06 - train loss/acc: 0.277 0.922, valid loss/acc: 0.286 0.918\nepoch 07 - train loss/acc: 0.264 0.926, valid loss/acc: 0.274 0.921\nepoch 08 - train loss/acc: 0.254 0.929, valid loss/acc: 0.265 0.923\nepoch 09 - train loss/acc: 0.245 0.932, valid loss/acc: 0.256 0.926\nepoch 10 - train loss/acc: 0.238 0.933, valid loss/acc: 0.250 0.927\nepoch 11 - train loss/acc: 0.230 0.937, valid loss/acc: 0.241 0.930\nepoch 12 - train loss/acc: 0.223 0.938, valid loss/acc: 0.234 0.932\nepoch 13 - train loss/acc: 0.216 0.940, valid loss/acc: 0.229 0.933\nepoch 14 - train loss/acc: 0.211 0.942, valid loss/acc: 0.222 0.935\nepoch 15 - train loss/acc: 0.205 0.943, valid loss/acc: 0.218 0.936\nepoch 16 - train loss/acc: 0.200 0.944, valid loss/acc: 0.213 0.937\nepoch 17 - train loss/acc: 0.195 0.945, valid loss/acc: 0.209 0.940\nepoch 18 - train loss/acc: 0.190 0.947, valid loss/acc: 0.203 0.941\nepoch 19 - train loss/acc: 0.186 0.949, valid loss/acc: 0.199 0.941\nepoch 20 - train loss/acc: 0.182 0.950, valid loss/acc: 0.195 0.943\nepoch 21 - train loss/acc: 0.178 0.950, valid loss/acc: 0.192 0.944\nepoch 22 - train loss/acc: 0.174 0.952, valid loss/acc: 0.189 0.945\nepoch 23 - train loss/acc: 0.171 0.952, valid loss/acc: 0.186 0.946\nepoch 24 - train loss/acc: 0.167 0.953, valid loss/acc: 0.182 0.947\nepoch 25 - train loss/acc: 0.164 0.954, valid loss/acc: 0.179 0.947\nepoch 26 - train loss/acc: 0.160 0.955, valid loss/acc: 0.176 0.948\nepoch 27 - train loss/acc: 0.157 0.957, valid loss/acc: 0.173 0.950\nepoch 28 - train loss/acc: 0.154 0.957, valid loss/acc: 0.170 0.950\nepoch 29 - train loss/acc: 0.151 0.958, valid loss/acc: 0.168 0.951\nepoch 30 - train loss/acc: 0.148 0.959, valid loss/acc: 0.165 0.952\nepoch 31 - train loss/acc: 0.146 0.959, valid loss/acc: 0.163 0.953\nepoch 32 - train loss/acc: 0.143 0.960, valid loss/acc: 0.160 0.955\nepoch 33 - train loss/acc: 0.141 0.961, valid loss/acc: 0.159 0.954\nepoch 34 - train loss/acc: 0.139 0.961, valid loss/acc: 0.157 0.956\nepoch 35 - train loss/acc: 0.136 0.962, valid loss/acc: 0.155 0.956\nepoch 36 - train loss/acc: 0.134 0.963, valid loss/acc: 0.152 0.958\nepoch 37 - train loss/acc: 0.132 0.963, valid loss/acc: 0.150 0.958\nepoch 38 - train loss/acc: 0.130 0.964, valid loss/acc: 0.149 0.958\nepoch 39 - train loss/acc: 0.128 0.965, valid loss/acc: 0.148 0.959\n\n\n\n\nCode\nY_hat, _ = model_relu.forward(X_test)\ntest_loss = model_relu.compute_loss(Y_test, Y_hat)\ntest_acc = model_relu.evaluate(Y_test, Y_hat)\nprint(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))\n\n\nFinal test loss = 0.142, acc = 0.960"
  },
  {
    "objectID": "posts/MLDL III3/hw3_3_cnn.html",
    "href": "posts/MLDL III3/hw3_3_cnn.html",
    "title": "Assignment 3-3",
    "section": "",
    "text": "Convolutional Neural Networks (Score: 107/100)\n\nConvolution Function\n\n\nCode\n\"\"\"\nQuestion (a)\n\nImplement your own conv functions which performs convolution operation without using any neural network packages.\nMake sure to handle all possible edge cases to receive full credits.\nKeep in mind that height and width of the given image or filter are not always the same.\n\"\"\"\n\nimport numpy as np\n\n\ndef convolution_naive(image, filter, stride=1, padding=0):\n    \"\"\"Performs 2D convolution operation with \"4 nested for-loops\".\n\n    Args:\n    - image: 2D numpy array\n    - filter: 2D numpy array\n    - stride, padding: integers\n\n    Returns:\n    - 2D numpy array : convolution results of the given image and filter.\n    - Return None if stride is not compatible. (ex. image of 5*5 with filter 2*2 with stride 2, padding 0)\n    - Return None if filter is larger than the given image.\n    \"\"\"\n\n    ##### YOUR CODE #####\n    h_i, w_i = image.shape\n    h_f, w_f = filter.shape\n    \n    output_h = ((h_i + 2 * padding - h_f) / stride) + 1\n    output_w = ((w_i + 2 * padding - w_f) / stride) + 1\n    \n    h_int = float(output_h).is_integer()\n    w_int = float(output_w).is_integer()\n    \n    input = np.pad(image, [(padding, padding), (padding, padding)], 'constant')\n    out_temp = np.zeros((np.int64(output_h), np.int64(output_w)))\n\n    \n    if (h_f < h_i) and (w_f < w_i) and (h_int == True & w_int == True) :\n        for h in np.arange(np.int64(output_h)):\n            s_h = h * stride\n            for w in np.arange(np.int64(output_w)):\n                s_w = w * stride\n                \n                value_list=[]\n                for filter_h in np.arange(np.int64(h_f)): \n                    for filter_w in np.arange(np.int64(w_f)):\n                        value = input[s_h+filter_h,s_w+filter_w] * filter[filter_h,filter_w]\n                        value_list.append(value)\n                        \n                out_temp[h, w] = np.sum(value_list)\n                \n        output = out_temp\n    else:\n        output = None\n    #####################\n    return output\n    \n\ndef convolution_vectorized(image, filter, stride=1, padding=0):\n    \"\"\"Performs 2D convolution operation with \"less than or equal to 2 nested for-loops\".\n\n    Args:\n    - image: 2D numpy array \n    - filter: 2D numpy array\n    - stride, padding: integers\n\n    Returns:\n    - 2D numpy array : convolution results of the given image and filter..\n    - Return None if stride is not compatible. (ex. image of 5*5 with filter 2*2 with stride 2, padding 0)\n    - Return None if filter is larger than the given image.\n    \"\"\"\n\n    ##### YOUR CODE #####\n    h_i, w_i = image.shape\n    h_f, w_f = filter.shape\n    \n    output_h = ((h_i + 2 * padding - h_f) / stride) + 1\n    output_w = ((w_i + 2 * padding - w_f) / stride) + 1\n    \n    h_int = float(output_h).is_integer()\n    w_int = float(output_w).is_integer()\n    \n    input = np.pad(image, [(padding, padding), (padding, padding)], 'constant')\n    out_temp = np.zeros((np.int64(output_h), np.int64(output_w)))\n    \n    if (h_f < h_i) and (w_f < w_i) and (h_int == True & w_int == True) :\n        for h in np.arange(np.int64(output_h)):\n            s_h = h * stride\n            e_h = s_h + h_f\n            for w in np.arange(np.int64(output_w)): \n                s_w = w * stride\n                e_w = s_w + w_f\n                out_temp[h, w] = np.sum(input[s_h:e_h, s_w:e_w] * filter)\n        output = out_temp           \n    else:\n        output = None\n    #####################    \n    return output\n\n\n\n\nCode\nimage = np.random.randint(10, size=(256, 256))\nfilter = np.random.randint(10, size=(16, 16))\nstride = 1\npadding = 0\n\nprint(\"Compare the time complexity of 2 convolution operations\")\nprint(\"1. Convolution operation with 4 nested loops\")\n%timeit -n 3 -r 1 convolution_naive(image, filter, stride, padding)\nprint(\"2. Convolution operation with less than or equal to 2 nested loops\")\n%timeit -n 3 -r 1 convolution_vectorized(image, filter, stride, padding)\n\n\nCompare the time complexity of 2 convolution operations\n1. Convolution operation with 4 nested loops\n12.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 3 loops each)\n2. Convolution operation with less than or equal to 2 nested loops\n387 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 3 loops each)\n\n\n\n\nCode\nimage = np.array([[1, 2, 3, 2, 1], [2, 3, 4, 5, 6], [-1, -2, -3, -4, -5], [0, 0, 1, 0, 0], [7, 1, 7, 1, 7]])\nfilter = np.array([[1, 0], [0, 1]])\nstride = 3\npadding = 0\n\n# Expected Result for each convolution functions.\n# [[4. 8.]\n#  [1. 7.]]\nprint(convolution_naive(image, filter, stride, padding))\nprint(convolution_vectorized(image, filter, stride, padding))\n\n\n[[4. 8.]\n [1. 7.]]\n[[4. 8.]\n [1. 7.]]\n\n\n\n\nLocal Setup\n\n\nCode\n\"\"\"\nChange directory to where this file is located\n\"\"\"\nimport os\n# get current path\nprint(os.getcwd())\n# change path\nos.chdir(\"E:/OneDrive - SNU/(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/과제3\")\nprint(os.getcwd())\n\n\ne:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제3\nE:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제3\n\n\n\n\nImport Modules\n\n\nCode\nimport cv2\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets\n\n\n\n\nCode\n\"\"\"\nimport modules you need\n\"\"\"\nimport seaborn as sns\nimport plotnine as pn\nfrom plotnine import *\n\n\n\n\nUtils\n\n\nCode\ndef plot_dataset(dataloader, grid_width=8, grid_height=2, figure_width=12, figure_height=3, y_hats=None):\n    \"\"\"\n    Plots image and labels.\n\n    Do NOT modify this function.\n    \"\"\"\n    images, labels = next(iter(dataloader))\n    f, ax = plt.subplots(grid_height, grid_width)\n    f.set_size_inches(figure_width, figure_height)\n    img_idx = 0\n    for i in range(0, grid_height):\n        for j in range(0, grid_width):\n            image = images[img_idx]\n            label = labels[img_idx]\n            title_color = 'k'\n            if y_hats is None:\n                label_idx = int(label)\n            else:\n                label_idx = int(y_hats[img_idx])\n                if int(labels[img_idx]) != label_idx:\n                    title_color = 'r'\n            label = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'][label_idx]\n            ax[i][j].axis('off')\n            ax[i][j].set_title(label, color=title_color)\n            ax[i][j].imshow(np.transpose(image, (1, 2, 0)), aspect='auto')\n            img_idx += 1\n        plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0.25)\n    plt.show()\n\n\n\n\nCode\ndef train(model, train_loader, optimizer):\n    \"\"\"\n    Trains the model with training data.\n\n    Do NOT modify this function.\n    \"\"\"\n    model.train()\n    tqdm_bar = tqdm(train_loader)\n    for batch_idx, (image, label) in enumerate(tqdm_bar):\n        image = image.to(DEVICE)\n        label = label.to(DEVICE)\n        optimizer.zero_grad()\n        output = model(image)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n\n\ndef evaluate(model, test_loader):\n    \"\"\"\n    Evaluates the trained model with test data.\n\n    Do NOT modify this function.\n    \"\"\"\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        for image, label in tqdm(test_loader):\n            image = image.to(DEVICE)\n            label = label.to(DEVICE)\n            output = model(image)\n            test_loss += criterion(output, label).item()\n            prediction = output.max(1, keepdim=True)[1]\n            correct += prediction.eq(label.view_as(prediction)).sum().item()\n    \n    test_loss /= len(test_loader.dataset)\n    test_accuracy = 100. * correct / len(test_loader.dataset)\n    return test_loss, test_accuracy\n\n\n\n\nCIFAR-10 Data Augmentation\n\n\nCode\ndef gaussian_smoothing(image, filter_size=3, sigma=1.0):\n    \"\"\"\n    Inputs\n    - image: an input image of shape (32,32,3).\n    Returns\n    - image: image blurred with a Gaussian Filter.\n\n    Do NOT modify this function.\n    \"\"\"\n    center = (filter_size-1)/2\n    gaussian_filter = np.zeros((filter_size, filter_size))\n    for row in range(filter_size):\n        for col in range(filter_size):\n            gaussian_filter[row, col] = np.exp((-(row-center) ** 2 - (col-center) ** 2) / (2 * sigma ** 2)) / (2 * np.pi * sigma ** 2)\n    gaussian_filter = gaussian_filter / np.sum(gaussian_filter)\n    image = cv2.filter2D(image, -1, gaussian_filter)\n    return image\n\n\ndef color_jitter(image):\n    \"\"\"\n    Inputs\n    - image: an input image of shape (32,32,3).\n    Returns\n    - image: image blurred with a Gaussian Filter.\n\n    Do NOT modify this function.\n    \"\"\"\n    image = cv2.convertScaleAbs(image, alpha=1.5, beta=20)\n    return image\n\n\n\n\nCode\ndef horizontal_flip(image):\n    \"\"\"Flips the image horizontally.\n\n    Question (b)\n    - Do not use TorchVision or OpenCV library for this question.\n    - You can solve the problem using numpy only.\n\n    Inputs\n    - image: an input image of shape (32, 32, 3).\n\n    Returns\n    - image: a horizontally-flipped image of shape (32, 32, 3).\n    \"\"\"\n    ##### YOUR CODE #####\n    newimage = image [:,::-1]\n    image = newimage\n    #####################\n    return image\n\n\n\n\nCode\na = np.random.randint(10,size=(32,32,3))\nnp.sum(horizontal_flip(a) != np.flip(a,axis=1))\n\n\n0\n\n\n\n\nCode\n\"\"\"\nVisualize how the augmentations are applied to a single image.\n\nDo NOT modify.\n\"\"\"\n\nraw_test = datasets.CIFAR10(root=\"./CIFAR_10\", train=False, download=True)\n\nf, ax = plt.subplots(1, 5)\nf.set_size_inches(12, 3)\nx = raw_test[0][0]\nx = np.array(x)\ng = gaussian_smoothing(x)\nj = color_jitter(x)\nh = horizontal_flip(x)\ntitle = ['original image', 'gaussian filter', 'color jitter', 'horizontal flip', 'original image']\nfor i, img in enumerate([x, g, j, h, x]):\n    ax[i].imshow(img)\n    ax[i].axis('off')\n    ax[i].set_title(title[i], color='k')\n\n\nFiles already downloaded and verified\n\n\n\n\n\n\n\nCode\n### \n# Question (b)\n# Briefly explain whether horizontal flip is a good augmentation method for image classification task.\n###\n\n\nWrite your answer to question (b) in this cell.\n일반적으로 data augmentation은 추가적인 데이터에 대한 축적이나 라벨링 없이 완전히 새롭고 다양한 인스턴스들을 만들 수 있으므로 딥러닝 모델의 성능을 개선할 수 있다는 점에서 매우 효과적이다. 일반적으로 사진들을 찍을 때 자동적으로 좌우반전을 해주는 경우가 많다는 점에서, 또 이러한 맥 데이터의 갯수를 증가시킬 수 있고, 데이터의 오버피팅을 감소시키며 각 class들의 unbalance를 어느정도 극복하는데 사용될 수 있다는 점에서 그러하다. 그러나, MNIST와 같이 필체인식(OCR)을 위한 데이터들의 경우 좌우반전이나 상하반전등의 augmentation을 적용시에 완전히 이미지가 가진 의미가 사라지게 만든다는 점에서 좌우반전을 통해 이미지의 의미가 변화하는지 여부를 미리 판단해야할 필요성이 존재한다. 일반적인 상황에서는 좌우반전을 적용하면 모형 학습이 개선되어지므로, 모델 적용시 데이터 생성 맥락을 정확히 이해한 상태에서 augmentation을 적용함이 필요하다.\n\n\nCode\nclass CustomDataset(Dataset):\n    def __init__(self, train, prob=0.5, data_dir=\"./CIFAR_10\"):\n        \"\"\"\n        Do NOT modify this method.\n        \"\"\"\n        self.data = datasets.CIFAR10(root=data_dir, train=train, download=True)\n        self.prob = prob\n\n    def __len__(self):\n        \"\"\"\n        Do NOT modify this method.\n        \"\"\"\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Do NOT modify this method.\n        \"\"\"\n        return self.data[idx]\n    \n    def transform(self, image):\n        \"\"\"\n        Apply stochastic data augmentation to the given image.\n\n        Question (c)\n        - Convert the given RGB image into BGR scale using opencv library.\n        - Apply random augmentation (gaussian smoothing, color jitter, and horizontal flip).\n        - Random augmentation is applied with the probability of self.prob.\n        - If self.prob = 0.5, 5 out of 10 images will be augmented on average.\n        - Convert the augmented image back to RGB scale for training.\n\n        Inputs\n        - image: numpy array of an input image of shape (32,32,3).\n        Returns\n        - image: numpy array of the augmented input image with shape (32,32,3).\n        \"\"\"\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        ##### YOUR CODE #####\n        augment_pool = [color_jitter, gaussian_smoothing,horizontal_flip]\n    \n        \n        prob = self.prob\n        augment = augment_pool[np.random.randint(len(augment_pool))]\n        image = augment(image) if np.random.rand() <= prob else image      \n        #####################        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n\n    def collate_fn(self, data):\n        \"\"\"\n        Creates a batch of images and label tensors.\n\n        Question (d)\n        - Convert each image in the batch from PIL image to numpy array.\n        - Transform the image using self.transform method to apply random augmentation.\n        - Normalize the transformed image by mapping the range [0, 255] to range [0, 1].\n        - Transpose the (H * W * C) format of the image into (C * H * W) format.\n          - To be specific, the dimension of the original image is (32, 32, 3).\n          - We want the dimension of the transposed image to be (3, 32, 32).\n        - Convert the batch of preprocessed images into PyTorch float tensors.\n        - Convert the batch of labels into PyTorch long tensors.\n        - Do NOT use torchvision.transforms library!\n\n        Inputs\n        - list of tuples, each containing a PIL image and an integer label\n        - number of tuples in the list == BATCH SIZE\n\n        Returns\n        - batch of image tensors, batch of label tensors\n        - size: (BATCH, CHANNEL, HEIGHT, WIDTH), (BATCH)\n        \"\"\"\n        batch_x, batch_y = [], []\n\n        ##### YOUR CODE #####\n        for image, label_idx in data:\n          image = np.array(image)\n          image = self.transform(image)\n          image = (image - 0) / (255 - 0)\n          image = image.transpose(2,0,1)\n          image= torch.Tensor(image)          \n          label_idx = torch.Tensor([label_idx])\n          batch_x.append(image)\n          batch_y.append(label_idx)\n        batch_x = torch.stack(batch_x).float()\n        batch_y = torch.cat(batch_y).long()\n        ##############################\n        return batch_x, batch_y\n    \n\n\n\n\nCode\n\"\"\"\nPlot some example images and class labels without applying data augmentation.\n\nDo NOT modify.\n\"\"\"\n\nraw_test_dataset = CustomDataset(train=False, prob=0, data_dir=\"./CIFAR_10\")\nraw_test_loader = DataLoader(dataset=raw_test_dataset, batch_size=16, shuffle=False, collate_fn=raw_test_dataset.collate_fn)\n\nplot_dataset(raw_test_loader)\n\n\nFiles already downloaded and verified\n\n\n\n\n\n\n\nCode\n\"\"\"\nSame examples after applying data augmentation with 50% probability.\nIf your transform (c) and collate_fn (d) methods have been implemented well, some of the results should look different from the ones above.\n\nDo NOT modify.\n\"\"\"\n\ntrain_dataset = CustomDataset(train=True, prob=0.5)\ntest_dataset = CustomDataset(train=False, prob=0.5)\n\nBATCH_SIZE = 64\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=BATCH_SIZE, \n                          shuffle=True, \n                          collate_fn=train_dataset.collate_fn)\ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=BATCH_SIZE, \n                         shuffle=False, \n                         collate_fn=test_dataset.collate_fn)\n\nplot_dataset(test_loader)\n\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n\n\n\n\nConvNet Image Classification\n\n\nCode\n### \n# Question (e)\n# Train your ConvNet to achieve test accuracy above 70%\n# You can try or add other training options such as SGD or callbacks to schedule learning rates if you want.\n###\n\n\n\n\nCode\nclass ConvNet(nn.Module):\n    \"\"\"\n    Builds a ConvNet model.\n\n    Question (e)\n    - things that might be useful...\n    - stack [Conv2D + Conv2D + MaxPool2D] at least three times, \n    - follwed by at least three Linear layers.\n    - 3x3 filter is enough, but feel free to use larger filter size.\n    - channels used: [10, 32, 64, 128, 256, 512, 1024]\n    - you can choose smaller or larger channel size as well.\n    - The model may include BatchNormalization, regularizers, and Dropout, but they are not necessary.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Define the layers that you would like to use in your model.\n        \"\"\"\n        super(ConvNet, self).__init__()\n\n        ########## Your Code #########\n        # convolutional layer\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1, stride=1), # 3,32*32 -> 32,32*32\n            nn.ReLU(),                              \n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, stride=1), \n            nn.ReLU(),                       \n            nn.BatchNorm2d(32), \n            nn.MaxPool2d(2),\n            nn.Dropout2d(p=0.25),        \n                                           \n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1), # 32,16*16 -> 64,16*16\n            nn.ReLU(),\n            nn.BatchNorm2d(64),                   \n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1), \n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2), # 64,16*16 -> 64, 8*8\n            nn.Dropout2d(p=0.3),\n            \n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1), # 64,8*8 -> 128, 8*8\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(in_channels=128,out_channels=128, kernel_size=3, padding=1, stride=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128), \n            nn.MaxPool2d(2), # 128,8*8 -> 128, 4*4\n            nn.Dropout2d(p=0.35),   \n            )\n        \n        self.fc_layers = nn.Sequential(\n            nn.Linear(128 * 4 * 4, 1024), \n            nn.BatchNorm1d(1024), \n            nn.ReLU(),\n            nn.Linear(1024,10) \n        )\n        ############################## \n    \n    def forward(self, x):\n        \"\"\"\n        Apply forward pass of the given batch of input images.\n        Inputs\n        - x: batch of input images.\n        Returns\n        - softmax probabilites of the input image for each class label\n        \"\"\"\n\n        ##### YOUR CODE #####\n        x = self.conv_layers(x) \n        x = x.view(x.size(0), -1) # flatten\n        x = self.fc_layers(x)\n        #####################\n\n        return x\n\n\n\n\nCode\n\"\"\"\nMake sure your runtime type is GPU and you are using PyTorch version higher than 1.8!\n\nDo NOT modify.\n\"\"\"\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\nprint(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\n\n\nUsing PyTorch version: 1.13.0+cu117, Device: cuda\n\n\n\n\nCode\n\"\"\"\nLoad your customized model \"ConvNet\" and its training settings.\nYou may choose the number of epochs that you would like to train.\nYou might want to use different optimizers or learning rates.\n\"\"\"\n\nEPOCHS = 30\nmodel = ConvNet().to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# Adam, AdamW, RMSprop, SGD 전부 사용했는데 Adam이 가장 결과가 적절하게 나옴.\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.1)\ncriterion = nn.CrossEntropyLoss()\n\n\nprint(model)\n\n\nConvNet(\n  (conv_layers): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Dropout2d(p=0.25, inplace=False)\n    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU()\n    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU()\n    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Dropout2d(p=0.3, inplace=False)\n    (15): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (16): ReLU()\n    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (19): ReLU()\n    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (22): Dropout2d(p=0.35, inplace=False)\n  )\n  (fc_layers): Sequential(\n    (0): Linear(in_features=2048, out_features=1024, bias=True)\n    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Linear(in_features=1024, out_features=10, bias=True)\n  )\n)\n\n\n\n\nCode\n\"\"\"\nTrain your model \"ConvNet\" with the augmented CIFAR-10 dataset.\nUpon successful training, test accuracy of your model should be above 70%.\n\nDo NOT modify.\n\"\"\"\n\nfor epoch in range(1, EPOCHS + 1):\n    train(model, train_loader, optimizer)\n    test_loss, test_accuracy = evaluate(model, test_loader)\n    print(\"\\n[EPOCH: {}], \\tModel: ConvNet, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n        epoch, test_loss, test_accuracy))\n\n\n\n\n\n\n\n\n\n[EPOCH: 1],     Model: ConvNet,     Test Loss: 0.0172,  Test Accuracy: 61.08 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 2],     Model: ConvNet,     Test Loss: 0.0141,  Test Accuracy: 68.58 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 3],     Model: ConvNet,     Test Loss: 0.0121,  Test Accuracy: 73.30 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 4],     Model: ConvNet,     Test Loss: 0.0113,  Test Accuracy: 75.06 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 5],     Model: ConvNet,     Test Loss: 0.0108,  Test Accuracy: 76.69 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 6],     Model: ConvNet,     Test Loss: 0.0101,  Test Accuracy: 78.09 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 7],     Model: ConvNet,     Test Loss: 0.0094,  Test Accuracy: 79.44 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 8],     Model: ConvNet,     Test Loss: 0.0098,  Test Accuracy: 78.95 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 9],     Model: ConvNet,     Test Loss: 0.0092,  Test Accuracy: 80.10 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 10],    Model: ConvNet,     Test Loss: 0.0089,  Test Accuracy: 80.93 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 11],    Model: ConvNet,     Test Loss: 0.0093,  Test Accuracy: 79.92 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 12],    Model: ConvNet,     Test Loss: 0.0090,  Test Accuracy: 81.00 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 13],    Model: ConvNet,     Test Loss: 0.0086,  Test Accuracy: 81.35 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 14],    Model: ConvNet,     Test Loss: 0.0089,  Test Accuracy: 81.17 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 15],    Model: ConvNet,     Test Loss: 0.0084,  Test Accuracy: 82.29 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 16],    Model: ConvNet,     Test Loss: 0.0087,  Test Accuracy: 81.46 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 17],    Model: ConvNet,     Test Loss: 0.0088,  Test Accuracy: 81.43 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 18],    Model: ConvNet,     Test Loss: 0.0083,  Test Accuracy: 82.69 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 19],    Model: ConvNet,     Test Loss: 0.0087,  Test Accuracy: 82.19 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 20],    Model: ConvNet,     Test Loss: 0.0085,  Test Accuracy: 82.58 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 21],    Model: ConvNet,     Test Loss: 0.0083,  Test Accuracy: 82.67 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 22],    Model: ConvNet,     Test Loss: 0.0092,  Test Accuracy: 81.66 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 23],    Model: ConvNet,     Test Loss: 0.0084,  Test Accuracy: 82.99 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 24],    Model: ConvNet,     Test Loss: 0.0088,  Test Accuracy: 82.29 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 25],    Model: ConvNet,     Test Loss: 0.0085,  Test Accuracy: 83.00 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 26],    Model: ConvNet,     Test Loss: 0.0083,  Test Accuracy: 83.27 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 27],    Model: ConvNet,     Test Loss: 0.0087,  Test Accuracy: 82.97 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 28],    Model: ConvNet,     Test Loss: 0.0087,  Test Accuracy: 82.80 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 29],    Model: ConvNet,     Test Loss: 0.0087,  Test Accuracy: 82.86 % \n\n\n\n\n\n\n\n\n\n\n[EPOCH: 30],    Model: ConvNet,     Test Loss: 0.0089,  Test Accuracy: 82.85 %"
  },
  {
    "objectID": "posts/MLDL IV1/hw4_rnn.html",
    "href": "posts/MLDL IV1/hw4_rnn.html",
    "title": "Assignment 4-1",
    "section": "",
    "text": "Recurrent Neural Networks (Score: 84/100)\n\n\nCode\nimport os\n# get current path\nprint(os.getcwd())\n# change path\nos.chdir(\"E:/OneDrive - SNU\\(B) 대학원/수업/2022 2학기/데이터사이언스를위한머신러닝과딥러닝/과제4/HW4\")\nprint(os.getcwd())\n\n\ne:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제4\\HW4\nE:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제4\\HW4\n\n\n\n\nCode\n!pip install torchdata\n\n\n\n\nCode\n%load_ext tensorboard\n\n\n\n\nCode\nimport time\nimport math\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchtext\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.data.functional import to_map_style_dataset\n\n\n\n\nCode\n\"\"\"\nimport modules you need\n\"\"\"\nfrom plotnine import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nCode\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))\nprint(\"Using torchtext version: {}\".format(torchtext.__version__))\n\n\nUsing PyTorch version: 1.13.1+cpu, Device: cpu\nUsing torchtext version: 0.14.1\n\n\n\nLoad Data\n\n\nCode\n\"\"\"\nLoad AG_NEWS dataset and set up the tokenizer and encoder pipeline.\n\nDo NOT modify.\n\"\"\"\n\ntrain_data, test_data = torchtext.datasets.AG_NEWS(root='./data')\n\ntokenizer = get_tokenizer('basic_english')\n\ndef tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\nencoder = build_vocab_from_iterator(tokens(train_data), specials=[\"<unk>\"])\nencoder.set_default_index(encoder[\"<unk>\"])\n\ntext_pipeline = lambda x: encoder(tokenizer(x))\nlabel_pipeline = lambda x: int(x) - 1\n\n\n\n\nCode\ndef collate_batch(\n    batch: List[Tuple[int, str]]\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Creates a batch of encoded text, label and token length tensors.\n\n    Question (a)\n    - The input texts in the batch have different lengths.\n    - Complete your code to make them have same length using their average.\n    - This means that the length of token sequence in each batch is determined by \n      the average of token length of all sequences in each batch.\n    - Text tensors are stacked with dimension of (TOKEN_LENGTH, BATCH),\n      for easier process in RNN model.\n    - Token length tensors are used to index the last valid hidden token for classification.\n\n    Args:\n      batch: list of tuples, each containing an integer label and a text input.\n      - ex) [(3, \"Wall St. Bears...\"), (4, \"Comtes, Asteroids and ...\"), ...]\n      - number of tuples in the list is same as BATCH SIZE.\n\n    Returns:\n      text_list: batch of encoded long type text tensors with size (TOKEN_LENGTH, BATCH)\n      label_list: batch of label tensors with size (BATCH)\n      len_list: batch of token length tensors with size (BATCH)\n    \"\"\"\n\n    ##### YOUR CODE #####\n    \n    text_list, label_list, len_list = [], [], []\n    for (_label, _text) in batch:\n        length = torch.tensor(text_pipeline(_text), dtype=torch.int64).size(0)\n        len_list.append(length)\n    AVG_LEN = int(sum(len_list) / len(len_list)) # bankers round 때문에 np.round 대신 사용\n    \n    for (_label, _text) in batch:    \n        label_list.append(label_pipeline(_label))\n        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n        if processed_text.size(0)>= AVG_LEN:\n          processed_text = processed_text[:AVG_LEN]\n        else:\n          processed_text = torch.cat([processed_text,torch.zeros(AVG_LEN - processed_text.size(0))])\n        text_list.append(processed_text)\n        \n    \n    text_list = torch.stack(text_list, dim = 1).long()\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    len_list = torch.tensor(len_list)\n    \n    assert text_list.size(1) == len(batch)\n    return (text_list, label_list, len_list)\n    #####################\n\n\n\n\nCode\n\"\"\"\nLoad the data loader.\n\nDo NOT modify.\n\"\"\"\n\nBATCH_SIZE = 512\n\ntrain_dataset = to_map_style_dataset(train_data)\ntest_dataset = to_map_style_dataset(test_data)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                              shuffle=False, collate_fn=collate_batch)\n\n\n\n\nCode\n\"\"\"\nPrint out the first batch in the train loader.\nCheck if the collate function is implemented correctly.\n\nDo NOT modify.\n\"\"\"\n\nbatch_x, batch_y, len_x = next(iter(train_dataloader))\nprint(batch_x[:10])\nprint(batch_y[:10])\nprint(len_x[:10])\n\n\ntensor([[  846,   472,   340,  ..., 20308,   452,  2209],\n        [    3,  2188,   108,  ..., 13928,    58,    12],\n        [   77,     7,   946,  ...,    20,    92,  8976],\n        ...,\n        [  846,    16,    58,  ...,  3468,    23,  2820],\n        [ 1299,  1434,    43,  ..., 49584,    73,    10],\n        [   77,   875,   294,  ...,  3212,   452,   964]])\ntensor([3, 0, 2, 2, 2, 1, 0, 2, 1, 3])\ntensor([40, 27, 46, 46, 37, 28, 19, 34, 35, 21])\n\n\n\n\nCode\n\"\"\"\nPlot the sequence length distribution of the batches in the train dataloader.\nMake sure that all batches have difference sequence lengths.\n\nDo NOT modify.\n\"\"\"\n\nbatch_len = []\nfor batch_x, _, _ in train_dataloader:\n    seq_len = batch_x.size(0)\n    batch_len.append(seq_len)\nplt.hist(batch_len)\nplt.show()\n\n\n\n\n\n\n\nModel\n\n\nCode\nclass RNN(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        input_size: int,\n        hidden_size: int,\n        num_class: int,\n        dropout_ratio: float,\n    ):\n        \"\"\"\n        Define the model weight parameters and initialize the weights.\n\n        Question (b)\n        - Complete the dimension and shape of the weights and biases.\n        - Use the model parameters (vocab_size, input_size, hidden_size, num_class).\n\n        Args:\n          vocab_size: size of dictionary of vocabularies.\n          input_size: size of each embedding vector.\n          hidden_size: size of hidden dimension. \n          num_class: size of output classes.\n          dropout_ratio: probability of an element to be zeroed.\n        \"\"\"\n        super(RNN, self).__init__()\n\n        ##### YOUR CODE #####\n        whh_size = (hidden_size, hidden_size)\n        wxh_size = (hidden_size, input_size)\n        why_size = (num_class, hidden_size)\n        bhh_size = (hidden_size ,1)\n        bxh_size = (hidden_size, 1)\n        bhy_size = (num_class, 1)\n        #####################\n\n        kwargs = {'device': DEVICE, 'dtype': torch.float}\n        self.dropout = dropout_ratio\n        self.hidden = hidden_size\n        self.num_class = num_class\n        self.embedding = nn.Embedding(vocab_size, input_size)\n        self.W_hh = nn.parameter.Parameter(torch.empty(whh_size, **kwargs))\n        self.W_xh = nn.parameter.Parameter(torch.empty(wxh_size, **kwargs))\n        self.W_hy = nn.parameter.Parameter(torch.empty(why_size, **kwargs))\n        self.b_hh = nn.parameter.Parameter(torch.empty(bhh_size, **kwargs))\n        self.b_xh = nn.parameter.Parameter(torch.empty(bxh_size, **kwargs))\n        self.b_hy = nn.parameter.Parameter(torch.empty(bhy_size, **kwargs))\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        \"\"\"\n        Initialize the parameters with Kaiming uniform initialization.\n\n        Do NOT modify this method.\n        \"\"\"\n        nn.init.kaiming_uniform_(self.W_hh, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hh)\n        bound = 1 / math.sqrt(fan_in)\n        nn.init.uniform_(self.b_hh, -bound, bound)\n        nn.init.kaiming_uniform_(self.W_xh, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_xh)\n        bound = 1 / math.sqrt(fan_in)\n        nn.init.uniform_(self.b_xh, -bound, bound)\n        nn.init.kaiming_uniform_(self.W_hy, a=math.sqrt(5))\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_hy)\n        bound = 1 / math.sqrt(fan_in)\n        nn.init.uniform_(self.b_hy, -bound, bound)\n\n    def forward(self, inputs: torch.Tensor, length: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Question (c)\n        - Randomly initialize h_0 with appropriate shape.\n        - Pass a sequence of tokens into the recurrent network.\n        - Implement dropout to embedded tokens with the given probability (self.dropout).\n          For example, if self.dropout is 0.3, 30% of the embedded tokens will be dropped out.\n        - We do not want to use a hidden cell of a zero-padded token for classification!\n        - Index the hidden cell of the last valid token (excluding the zero-padding)\n          based on the token length of each example in the batch.\n        - Do NOT use pre-defined PyTorch layers for this question. (e.g. nn.RNN, nn.Dropout)\n\n        Args:\n          inputs: a batch of encoded token sequences with shape (SEQ_LEN, BATCH_SIZE)\n          length: a batch of token lengths with shape (BATCH_SIZE)\n          \n        Returns:\n          Softmax probabilites for each class with shape (BATCH_SIZE, NUM_CLASS)\n        \"\"\"\n\n        ##### YOUR CODE #####\n        \n        seq_len = inputs.size(0)\n        batch_size = length.size(0)\n        \n        x_embed = self.embedding(inputs) # (seq_len, batch_size, input_size)\n        \n        # dropout     \n        mask = (torch.rand(list(x_embed.size()))<self.dropout) / self.dropout \n        x_embed = mask * x_embed     \n                    \n        hiddens = [] \n        h_0 =torch.randn(self.hidden, batch_size).to(DEVICE)   \n        hidden = h_0\n        hiddens.append(hidden)\n        \n        # forward 1        \n        for t in np.arange(seq_len): \n          hidden = torch.tanh((torch.matmul(self.W_hh, hidden) + self.b_xh) + \n                              (torch.matmul(self.W_xh, x_embed[t,:,:].T) + self.b_hh)).to(DEVICE)\n          hiddens.append(hidden)         \n\n        hiddens_temp = torch.zeros((self.hidden, batch_size)).to(DEVICE)      \n        for i in np.arange(batch_size):\n          if length[i] < seq_len:\n            hidden_length = hiddens[int(length[i])]\n            hiddens_temp[:,i] = hidden_length[:,i]\n          else:\n            hidden_length = hiddens[seq_len]\n            hiddens_temp[:,i] = hidden_length[:,i]   \n            \n        # Y_pred           \n        softmax = nn.Softmax(dim=0)\n        softmax_probs = softmax(torch.matmul(self.W_hy, hiddens_temp) + self.b_hy).permute(1,0)\n        \n        return softmax_probs \n        #####################\n\n    def compute_loss(\n        self,\n        prediction: torch.Tensor, \n        label: torch.Tensor) -> Tuple[torch.Tensor, int]:\n        \"\"\"\n        Question (d)\n        - Compute the cross entropy loss and the number of correct predictions\n        - Do NOT use loss function in torch.nn library (e.g. nn.CrossEntropyLoss())\n\n        Args:\n          prediction: output(softmax probabilities) from self.forward function with shape (BATCH_SIZE, NUM_CLASS)\n          label: integer labels of the batch inputs with shape (BATCH_SIZE)\n        \n        Returns:\n          cross entropy loss of the batch (float tensor) and the number of correct predictions (integer)\n        \"\"\"\n\n        ##### YOUR CODE #####\n        loss = 0\n        correct = 0\n        \n        batch_size = label.size(0)\n                \n        label_enc = F.one_hot(label, self.num_class)\n        label_enc = label_enc.to(DEVICE)\n\n        loss = - torch.sum(label_enc * torch.log(prediction))\n        correct = int((label == prediction.argmax(1)).sum())\n        loss /= batch_size \n        \n        return (loss, correct)\n        #####################\n\n\n\n\nTraining Modules\n\n\nCode\nclass ScheduledOptim():\n    \"\"\"\n    Learning rate scheduler.\n\n    Do NOT modify.\n    \"\"\"\n\n    def __init__(self, optimizer, n_warmup_steps, decay_rate):\n        self._optimizer = optimizer\n        self.n_warmup_steps = n_warmup_steps\n        self.decay = decay_rate\n        self.n_steps = 0\n        self.initial_lr = optimizer.param_groups[0]['lr']\n        self.current_lr = optimizer.param_groups[0]['lr']\n\n    def zero_grad(self):\n        self._optimizer.zero_grad()\n    \n    def step(self):\n        self._optimizer.step()\n    \n    def get_lr(self):\n        return self.current_lr\n    \n    def update(self):\n        if self.n_steps < self.n_warmup_steps:\n            lr = self.n_steps / self.n_warmup_steps * self.initial_lr\n        elif self.n_steps == self.n_warmup_steps:\n            lr = self.initial_lr\n        else:\n            lr = self.current_lr * self.decay\n        \n        self.current_lr = lr\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr\n\n        self.n_steps += 1\n\n\n\n\nCode\n\"\"\"\nFunctions for training and evaluating the model.\n\nQuestion (e)\n- There has been minor changes with the model forward operation and loss computation.\n  Check what has been changed from the train and evaluate functions that we have used previously,\n  and complete the train and evaluate function that works for the current model architecture.\n- Use the methods of the ScheduledOptim class above to perform necessary operations on the optimizer.\n- Do NOT change the arguments given to the train, evaluate functions.\n\"\"\"\n\ndef train(model, train_loader, scheduler):\n    ##### YOUR CODE #####\n    model.train()\n    train_loss = 0\n    correct = 0\n\n    tqdm_bar = tqdm(train_loader)\n\n    for text, label, length in tqdm_bar:\n        text = text.to(DEVICE)\n        label = label.to(DEVICE)\n        length = length.to(DEVICE)\n                \n        scheduler.zero_grad()        \n        class_pred = model(text, length)\n        loss_temp, correct_temp = model.compute_loss(class_pred, label)\n        loss_temp.backward()\n        scheduler.step()\n        train_loss += loss_temp\n        correct += correct_temp \n    \n    scheduler.update()\n    train_loss /= len(train_loader.dataset)\n    train_acc = 100. * correct / len(train_loader.dataset)\n\n    return train_loss, train_acc\n    #####################\n\n\ndef evaluate(model, test_loader):\n    ##### YOUR CODE #####\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    tqdm_bar = tqdm(test_loader)\n\n    with torch.no_grad():\n        for text, label, length in tqdm_bar:\n            text = text.to(DEVICE)\n            label = label.to(DEVICE)\n            length = length.to(DEVICE)\n    \n            class_pred = model(text, length)\n            loss_temp, correct_temp = model.compute_loss(class_pred, label) \n            test_loss += loss_temp\n            correct += correct_temp         \n\n    test_loss /= len(test_loader.dataset)\n    test_acc = 100. * correct / len(test_loader.dataset)\n\n    return test_loss, test_acc\n    #####################\n\n\n\n\nModel Training\n\n\nCode\n\"\"\"\nQuestion (f)\n- Train your RNN model and obtain the test accuracy of 70%.\n- Select the input size, hidden size of your choice\n- Try various optimizer type, learning rate and scheduler options for the best performance.\n- Visualize your experiments with Tensorboard.\n- Your TensorBoard results should include Train/Validation Loss and Accuracy.\n\"\"\"\n\n##### YOUR CODE #####\nwriter = SummaryWriter(log_dir=\"./logs\")\nEPOCHS = 15\nBATCH_SIZE = 512\nvocab_size = len(encoder) # 95811\ninput_size = 45\nhidden_size = 128\nnum_class = 4\ndropout_ratio = 0.3\nlearning_rate = 0.001\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                              shuffle=False, collate_fn=collate_batch)\n\nmodel = RNN(vocab_size, input_size, hidden_size, num_class,dropout_ratio)\nmodel = model.to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = ScheduledOptim(optimizer, 5, 0.8)\n\nfor epoch in range(1, EPOCHS + 1):\n    loss_train, accu_train = train(model, train_dataloader, scheduler)\n    writer.add_scalar(\"Loss/train\", loss_train,epoch)\n    writer.add_scalar(\"Accuracy/train\", accu_train,epoch)\n    loss_val, accu_val = evaluate(model, valid_dataloader)\n    writer.add_scalar(\"Loss/test\", loss_val,epoch)\n    writer.add_scalar(\"Accuracy/test\", accu_val,epoch)\n\n    lr = scheduler.get_lr()\n    print('-' * 83)\n    print('| end of epoch {:2d} | lr: {:5.4f} | train loss: {:8.3f} | train accuracy: {:8.3f} | '\n          'valid accuracy {:8.3f} '.format(epoch, lr, loss_train, accu_train, accu_val))\n    print('-' * 83)\n\nwriter.flush()\nwriter.close()\n#####################\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  1 | lr: 0.0000 | train loss:    0.003 | train accuracy:   34.801 | valid accuracy   43.211 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  2 | lr: 0.0002 | train loss:    0.002 | train accuracy:   42.688 | valid accuracy   43.171 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  3 | lr: 0.0004 | train loss:    0.002 | train accuracy:   44.939 | valid accuracy   46.316 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  4 | lr: 0.0006 | train loss:    0.002 | train accuracy:   49.015 | valid accuracy   50.842 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  5 | lr: 0.0008 | train loss:    0.002 | train accuracy:   54.191 | valid accuracy   56.053 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  6 | lr: 0.0010 | train loss:    0.002 | train accuracy:   59.338 | valid accuracy   61.447 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  7 | lr: 0.0008 | train loss:    0.002 | train accuracy:   64.619 | valid accuracy   67.066 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  8 | lr: 0.0006 | train loss:    0.001 | train accuracy:   69.592 | valid accuracy   72.197 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch  9 | lr: 0.0005 | train loss:    0.001 | train accuracy:   73.544 | valid accuracy   72.592 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch 10 | lr: 0.0004 | train loss:    0.001 | train accuracy:   75.653 | valid accuracy   74.908 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch 11 | lr: 0.0003 | train loss:    0.001 | train accuracy:   77.224 | valid accuracy   76.961 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch 12 | lr: 0.0003 | train loss:    0.001 | train accuracy:   78.388 | valid accuracy   77.250 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch 13 | lr: 0.0002 | train loss:    0.001 | train accuracy:   79.117 | valid accuracy   78.066 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch 14 | lr: 0.0002 | train loss:    0.001 | train accuracy:   79.552 | valid accuracy   78.882 \n-----------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n-----------------------------------------------------------------------------------\n| end of epoch 15 | lr: 0.0001 | train loss:    0.001 | train accuracy:   79.949 | valid accuracy   79.342 \n-----------------------------------------------------------------------------------\n\n\n\n\nCode\n%tensorboard --logdir ./logs\n\n\nReusing TensorBoard on port 6006 (pid 20996), started 1 day, 0:41:02 ago. (Use '!kill 20996' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\n\nCode\n# Run this cell to upload the result to TensorBoard.dev\n# Then you will get the shared link.\n\n# tensorboard dev upload --logdir 'e:\\OneDrive - SNU\\(B) 대학원\\수업\\2022 2학기\\데이터사이언스를위한머신러닝과딥러닝\\과제4\\HW4' --name \"MLDL1 HW4\" --description \"Training results from HW4 RNN Problem\" --one_shot\n\n\n\n\nCode\n# Write down the shared link in the below cell.\n\n\nWrite down the TensorBoard link in this cell.\nhttps://tensorboard.dev/experiment/1816CBEmQJm0J3ZCKYEE6A/"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html",
    "href": "posts/MLDL IV2/hw4_translation.html",
    "title": "Assignment 4-2",
    "section": "",
    "text": "Attention and LSTM, Seq2Seq, Transformer (Score: 84/100)"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#util",
    "href": "posts/MLDL IV2/hw4_translation.html#util",
    "title": "Assignment 4-2",
    "section": "Util",
    "text": "Util\nDo NOT Modify code blocks in this section\n\n\nCode\nSEED = 1234\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\nrandom.seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\n\n\nCode\ndef train(model, iterator, optimizer, loss_fn, clip):    \n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        src = batch[0].to(DEVICE)\n        trg = batch[1].to(DEVICE)        \n        optimizer.zero_grad()\n        output = model(src, trg)\n        loss = loss_fn(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) \n        optimizer.step()        \n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)\n\n\n\n\nCode\ndef evaluate(model, iterator, loss_fn):    \n    model.eval()\n    epoch_loss = 0\n    \n    with torch.no_grad():    \n        for i, batch in enumerate(iterator):\n            src = batch[0].to(DEVICE)\n            trg = batch[1].to(DEVICE)\n            output = model(src, trg)\n            loss = loss_fn(output, trg)\n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)\n\n\n\n\nCode\ndef plot_history(history):\n    plt.figure(figsize=(2 * 13, 4))\n    plt.subplot(1, 5, 1)\n    plt.title(\"Training and Validation Loss\")\n    plt.plot(history['train_PPL'], label=\"train_PPL\")\n    plt.plot(history['val_PPL'], label=\"val_PPL\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"PPL\")\n    plt.legend()\n    plt.subplot(1, 5, 2)\n    plt.title(\"Learning Rate\")\n    plt.plot(history['lr'], label=\"learning rate\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"LR\")\n    plt.show()"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#dataset-dataloader",
    "href": "posts/MLDL IV2/hw4_translation.html#dataset-dataloader",
    "title": "Assignment 4-2",
    "section": "Dataset & Dataloader",
    "text": "Dataset & Dataloader\nDo NOT Modify code blocks in this section\n\n\nCode\nMAX_LENGTH = 10\nBATCH_SIZE = 64\n\nTRAIN_RATIO = 0.7 # train dataset ratio, should be a float in (0, 0.8]\nVALID_RATIO = 0.8 - TRAIN_RATIO\n\nSOS_token = 0\nEOS_token = 1\n\n\n\n\nCode\nclass TranslateDataset(Dataset):\n    def __init__(self, max_length=10, fra2eng=True):\n        self.input_lang, self.output_lang, self.pairs = prepareData('eng', 'fra', max_length=max_length, reverse=fra2eng)\n        self.max_length = max_length\n        self.input_lang.addWord('PAD')\n        self.output_lang.addWord('PAD')\n        self.input_lang_pad = self.input_lang.word2index['PAD']\n        self.output_lang_pad = self.output_lang.word2index['PAD']\n        \n        print(\"\\n\")\n        print(\"This is data example\")\n        print(random.choice(self.pairs))\n\n        print(\"\\n\")\n        print(\"This is index of PAD token for each language\")\n        print(f\"fra {self.output_lang.word2index['PAD']}\")\n        print(f\"eng {self.input_lang.word2index['PAD']}\")\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        pair = self.pairs[idx]\n        x, y = self._tensorsFromPair(pair)\n        return x, y\n\n    def _tensorFromSentence(self, lang, sentence):\n        indexes = [lang.word2index[word] for word in sentence.split(' ')]\n        indexes.append(EOS_token)\n        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n\n    def _tensorsFromPair(self, pair):\n        input_tensor = self._tensorFromSentence(self.input_lang, pair[0])\n        target_tensor = self._tensorFromSentence(self.output_lang, pair[1])\n        return (input_tensor, target_tensor)\n    \n    def collate_fn(self, data):\n        x_batch = []; y_batch = []\n        \n        for x, y in data:\n            if x.shape[0] < self.max_length-1:\n                x = torch.cat([x, self.input_lang_pad*torch.ones((self.max_length-1 - x.shape[0], 1), dtype=x.dtype)])\n            elif x.shape[0] > self.max_length-1:\n                x = x[:self.max_length-1]\n            if y.shape[0] < self.max_length-1:\n                y = torch.cat([y, self.output_lang_pad*torch.ones((self.max_length-1 - y.shape[0], 1), dtype=y.dtype)])\n            elif y.shape[0] > self.max_length-1:\n                y = y[:self.max_length-1]\n\n            x_batch.append(torch.cat([torch.tensor([SOS_token]), x.squeeze(1)]))\n            y_batch.append(torch.cat([torch.tensor([SOS_token]), y.squeeze(1)]))\n        \n        return torch.stack(x_batch), torch.stack(y_batch)\n\ndataset = TranslateDataset(max_length=MAX_LENGTH)\n\ntrain_size = int(len(dataset)*TRAIN_RATIO)\nvalid_size = int(len(dataset)*VALID_RATIO)\ntrain_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, len(dataset)-(train_size+valid_size)],)\nprint(\"\\n\")\nprint(f\"This is dataset_size: {len(dataset)}\")\nprint(f\"train_size: {train_size}\")\nprint(f\"valid_data: {valid_size}\")\nprint(f\"test_data: {len(test_data)}\")\n\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\nvalid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n\n\nReading lines...\nRead 135842 sentence pairs\nTrimmed to 10599 sentence pairs\nCounting words...\nCounted words:\nfra 4345\neng 2803\n\n\nThis is data example\n['tu me fais de l ombre .', 'you re blocking my light .']\n\n\nThis is index of PAD token for each language\nfra 2803\neng 4345\n\n\nThis is dataset_size: 10599\ntrain_size: 7419\nvalid_data: 1059\ntest_data: 2121"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#implement-lstm-seq2seq-model",
    "href": "posts/MLDL IV2/hw4_translation.html#implement-lstm-seq2seq-model",
    "title": "Assignment 4-2",
    "section": "Implement LSTM Seq2Seq Model",
    "text": "Implement LSTM Seq2Seq Model\n\n\nCode\nclass LSTMEncoder(nn.Module):\n    \n    def __init__(self, in_dim, emb_dim, hid_dim):\n        super(LSTMEncoder, self).__init__()\n        \n        self.embedding = nn.Embedding(in_dim, emb_dim)\n        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n\n    def forward(self, input, hidden, cell):\n        '''\n        Q2 - (a)\n        Implement forward method of LSTM Encoder Module\n\n        INPUT\n        - input: input sentence, (B, max_len)\n        - hidden: initialized hidden state, (1, B, hid_dim)\n        - cell: initialized cell state, (1, B, hid_dim)\n\n        OUTPUT\n        What to be returned depends on your implementation of LSTMSeq2Seq. (Q2 - (b))\n        Feel free to return outputs you need. (e.g. hidden states of encoder, etc.)\n        '''\n        ################### YOUR CODE ###################\n        input_embed = self.embedding(input) # (batch, max_len, emb_dim)\n\n        enc_hidden, (hidden_state, cell_state) = self.lstm(input_embed,(hidden, cell))\n        hidden_state = hidden_state[0]\n  \n        return enc_hidden, hidden_state, cell_state  \n        #################################################\n\n\n\n\nCode\nclass AttnLSTMDecoder(nn.Module):\n\n    def __init__(self, emb_dim, hid_dim, out_dim, dropout, enc_hiddens=None):\n        super(AttnLSTMDecoder, self).__init__()\n        self.enc_hiddens = enc_hiddens # encoder output\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding(out_dim, emb_dim)\n        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim, batch_first=True)\n        self.fc = nn.Linear(hid_dim + hid_dim, hid_dim)\n        self.tanh = nn.Tanh()\n        self.classifier = nn.Linear(hid_dim, out_dim)\n\n    def forward(self, input, hidden, cell):\n        \n        '''\n        Q2 - (a)\n        Implement forward method of LSTM Decoder Module with dot-product attention\n        Before implementing LSTM layer, make sure to feed the concatenated input into Linear and tanh activation layer.\n        This will allow the concatenated input to be resized from (B, hid_dim + hid_dim) into (B, hid_dim) \n\n        INPUT\n        - input: a token of input sentence (B, 1)\n        - hidden: previous hidden state (B, hid_dim)\n        - cell: previous cell state (1, B, hid_dim)\n\n        OUTPUT\n        What to be returned depends on your implementation of LSTMSeq2Seq. (Q2 - (b))\n        Feel free to return outputs you need.\n        Some examples below\n        - predicted token embedding (N, emb_dim)\n        - current hidden state\n        - current cell state\n        '''\n\n        ################### YOUR CODE ###################\n        query = hidden # set query to calculate attention\n        query = torch.unsqueeze(query, dim=2) # (Batch, hidden, 1)\n        key = self.enc_hiddens # (Batch, max_len, hid_dim)\n        val = key.permute(0,2,1) # (Batch, hid_dim,max_len)\n        \n        attn_coef = F.softmax(torch.bmm(key, query),dim=1) # (Batch, max_len, 1)\n        attn_val = torch.bmm(val, attn_coef).permute(0,2,1) # (Batch, 1, hid_dim)           \n        input_embed = self.embedding(input)   \n        \n        # input: (Batch, 1, emb_dim + hid_dim)        \n        hidden_cat = torch.cat([input_embed, attn_val], dim=2)\n        \n        # FC: (Batch, 1, hid_dim)  \n        fc=self.fc\n        tanh=self.tanh\n        hidden_fc = fc(hidden_cat)\n        hidden_act = tanh(hidden_fc)\n        hidden_act = nn.Dropout(p=self.dropout)(hidden_act)                      \n        \n        # hiddens (Batch, 1, hid_dim), hidden_state (1, Batch, hid_dim), cell_state (1, Batch, hid_dim)\n        hidden = torch.unsqueeze(hidden, dim = 0)     \n        hiddens, (hidden_state, cell_state) = self.lstm(hidden_act, (hidden, cell))  \n        hiddens = hiddens.permute(1,0,2)\n        hidden_state = hidden_state[0]\n        \n        emb_pred = F.log_softmax(self.classifier(hiddens[0]),dim = 1) # (Batch, out_dim)\n        \n        return emb_pred, hidden_state, cell_state\n        #################################################\n\n\n\n\nCode\nclass LSTMSeq2Seq(nn.Module):\n    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n        super(LSTMSeq2Seq, self).__init__()\n\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.device = device\n        self.dropout = nn.Dropout(dropout)\n        \n        self.encoder = LSTMEncoder(in_dim, emb_dim, hid_dim)\n        self.decoder = AttnLSTMDecoder(emb_dim, hid_dim, out_dim, dropout)\n        \n    def forward(self, src, trg):\n        '''\n        Q2 - (b)\n        Implement forward method of LSTM Seq2Seq Module\n        (Decoder module should attend encoder's outputs using dot product.)\n        \n        INPUT\n        - src: source language batched data (B, max_len)\n        - trg: target language batched data (B, max_len)\n\n        OUTPUT\n        - output of one-hot prediction (B, out_dim, max_len)\n        '''\n        ################### YOUR CODE ###################\n        batch_size, mx_len = src.shape\n        DEVICE = self.device\n\n        # Encoder (start from zero-hidden & zero-cell states)        \n        hidden_state_0 = torch.zeros(1, batch_size, self.hid_dim).to(DEVICE)\n        cell_state_0 = torch.zeros(1, batch_size, self.hid_dim).to(DEVICE)\n        enc_hidden, hidden_state, cell_state = self.encoder(src, hidden_state_0, cell_state_0)\n\n        # Decoder\n        self.decoder.enc_hiddens = enc_hidden # set encoder's hidden states\n        outputs = torch.zeros(mx_len, batch_size, dataset.output_lang.n_words).to(DEVICE) # to store each decoder's output\n        decodes = trg[:,[0]].to(DEVICE)        \n        \n        for t in np.arange(1, mx_len): # for each t'th token, get decoder outputs\n            output, hidden_state, cell_state = self.decoder(decodes, hidden_state, cell_state)\n            outputs[t] = output \n            decodes = trg[:,[t]].to(DEVICE)\n                                           \n        outputs = torch.permute(outputs,dims = (1,2,0))           \n        return outputs\n        #################################################"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#training",
    "href": "posts/MLDL IV2/hw4_translation.html#training",
    "title": "Assignment 4-2",
    "section": "Training",
    "text": "Training\n\n\nCode\n'''\nQ2 - (c)\nTrain your Seq2Seq model and plot perplexities and learning rates. \nUpon successful training, the test perplexity should be less than 7. \nBriefly report your hyperparameters and results on test dataset. \nMake sure your results are printed in your submitted file.\n'''\n\n\n\n\nCode\n# experiment various methods for better performance\n# you can modify the codes in this block\nin_dim = dataset.input_lang.n_words\nout_dim = dataset.output_lang.n_words\nhid_dim = 256\nemb_dim = 256\ndropout = 0.3\nlearning_rate = 0.0005\nN_EPOCHS = 30\nvalid_every= 5\nbest_valid_loss = float('inf')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.NLLLoss(ignore_index = dataset.output_lang_pad)\n\n\n\n\nCode\n# Train your model \n# you can modify the codes in this block\nhistory = {'train_PPL':[], 'val_PPL':[], 'lr':[]}\n\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model, train_dataloader, optimizer, loss_fn, 1)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    \n    if epoch%valid_every==0:\n        print(\"==========================\")\n        valid_loss = evaluate(model, valid_dataloader, loss_fn)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            model.decoder.t=0\n            torch.save(model.state_dict(), 'lstm-attn-model.pt')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\n        history['train_PPL'].append(math.exp(train_loss))\n        history['val_PPL'].append(math.exp(valid_loss))\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n\nplot_history(history) \n\n\nEpoch: 01\n    Train Loss: 3.576 | Train PPL:  35.739\n==========================\n     Val. Loss: 2.674 |  Val. PPL:  14.498\nEpoch: 02\n    Train Loss: 2.445 | Train PPL:  11.525\nEpoch: 03\n    Train Loss: 2.156 | Train PPL:   8.633\nEpoch: 04\n    Train Loss: 1.967 | Train PPL:   7.149\nEpoch: 05\n    Train Loss: 1.818 | Train PPL:   6.158\nEpoch: 06\n    Train Loss: 1.689 | Train PPL:   5.414\n==========================\n     Val. Loss: 1.848 |  Val. PPL:   6.349\nEpoch: 07\n    Train Loss: 1.574 | Train PPL:   4.825\nEpoch: 08\n    Train Loss: 1.465 | Train PPL:   4.328\nEpoch: 09\n    Train Loss: 1.366 | Train PPL:   3.919\nEpoch: 10\n    Train Loss: 1.271 | Train PPL:   3.566\nEpoch: 11\n    Train Loss: 1.183 | Train PPL:   3.264\n==========================\n     Val. Loss: 1.575 |  Val. PPL:   4.829\nEpoch: 12\n    Train Loss: 1.098 | Train PPL:   2.998\nEpoch: 13\n    Train Loss: 1.017 | Train PPL:   2.764\nEpoch: 14\n    Train Loss: 0.942 | Train PPL:   2.565\nEpoch: 15\n    Train Loss: 0.868 | Train PPL:   2.382\nEpoch: 16\n    Train Loss: 0.800 | Train PPL:   2.226\n==========================\n     Val. Loss: 1.418 |  Val. PPL:   4.128\nEpoch: 17\n    Train Loss: 0.736 | Train PPL:   2.087\nEpoch: 18\n    Train Loss: 0.675 | Train PPL:   1.963\nEpoch: 19\n    Train Loss: 0.616 | Train PPL:   1.852\nEpoch: 20\n    Train Loss: 0.562 | Train PPL:   1.754\nEpoch: 21\n    Train Loss: 0.511 | Train PPL:   1.666\n==========================\n     Val. Loss: 1.327 |  Val. PPL:   3.772\nEpoch: 22\n    Train Loss: 0.464 | Train PPL:   1.591\nEpoch: 23\n    Train Loss: 0.420 | Train PPL:   1.523\nEpoch: 24\n    Train Loss: 0.377 | Train PPL:   1.458\nEpoch: 25\n    Train Loss: 0.340 | Train PPL:   1.406\nEpoch: 26\n    Train Loss: 0.307 | Train PPL:   1.359\n==========================\n     Val. Loss: 1.290 |  Val. PPL:   3.634\nEpoch: 27\n    Train Loss: 0.274 | Train PPL:   1.316\nEpoch: 28\n    Train Loss: 0.246 | Train PPL:   1.279\nEpoch: 29\n    Train Loss: 0.220 | Train PPL:   1.247\nEpoch: 30\n    Train Loss: 0.199 | Train PPL:   1.220\n\n\n\n\n\n\n\nCode\n# Test your model\ntorch.save(model.state_dict(), 'lstm-attn-model.pt') \nloaded_model = LSTMSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\nloaded_model.load_state_dict(torch.load('lstm-attn-model.pt'))\n\ntest_loss = evaluate(loaded_model, test_dataloader, loss_fn)\nprint(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')\n\n\n     Test. Loss: 1.290 |  Test. PPL:   3.634"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#bonus-implement-gru-seq2seq-model",
    "href": "posts/MLDL IV2/hw4_translation.html#bonus-implement-gru-seq2seq-model",
    "title": "Assignment 4-2",
    "section": "[Bonus] Implement GRU Seq2Seq Model",
    "text": "[Bonus] Implement GRU Seq2Seq Model\n\n\nCode\n'''\nQ2 - (d)\nChange the modules(encoder, decoder) in Seq2Seq model to GRU, and repeat (a)~(c).\n\n'''\n\n\n\n\nCode\nclass GRUEncoder(nn.Module):\n    def __init__(self, in_dim, emb_dim, hid_dim):\n        super(GRUEncoder, self).__init__()\n        ################### YOUR CODE ###################\n        self.dropout_layer = nn.Dropout(p=dropout)\n        self.embedding = nn.Embedding(in_dim, emb_dim)\n        self.gru = nn.GRU(input_size=emb_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n        #################################################\n\n    def forward(self, input, hidden):\n        ################### YOUR CODE ###################\n        input_embed = self.embedding(input) # (batch, max_len, emb_dim)\n        input_embed = self.dropout_layer(input_embed)\n        \n        enc_hidden, hidden_state = self.gru(input_embed,hidden)\n        hidden_state = hidden_state[0]\n  \n        return enc_hidden, hidden_state\n        #################################################\n\n\n\n\nCode\nclass AttnGRUDecoder(nn.Module):\n    def __init__(self, emb_dim, hid_dim, out_dim, dropout, enc_hiddens=None):\n        super(AttnGRUDecoder, self).__init__()\n        ################### YOUR CODE ###################\n        self.enc_hiddens = enc_hiddens # encoder output\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding(out_dim, emb_dim)\n        self.gru = nn.GRU(input_size=hid_dim, hidden_size=hid_dim, num_layers=1, batch_first=True)\n        self.fc = nn.Linear(hid_dim + hid_dim, hid_dim)\n        self.tanh = nn.Tanh()\n        self.classifier = nn.Linear(hid_dim, out_dim)\n        #################################################\n\n    def forward(self, input, hidden):\n\n        ################### YOUR CODE ###################\n        query = hidden # set query to calculate attention\n        query = torch.unsqueeze(query, dim=2) # (Batch, hidden, 1)\n        key = self.enc_hiddens # (Batch, max_len, hid_dim)\n        val = key.permute(0,2,1) # (Batch, hid_dim,max_len)\n        \n        attn_coef = F.softmax(torch.bmm(key, query),dim=1) # (Batch, max_len, 1)\n        attn_val = torch.bmm(val, attn_coef).permute(0,2,1) # (Batch, 1, hid_dim)           \n        input_embed = self.embedding(input) \n        \n        # input: (Batch, 1, emb_dim + hid_dim)        \n        hidden_cat = torch.cat([input_embed, attn_val], dim=-1)\n                \n        # FC: (Batch, 1, hid_dim)  \n        fc=self.fc\n        tanh=self.tanh\n        hidden_fc = fc(hidden_cat)\n        hidden_act = tanh(hidden_fc)\n        \n        hidden_act = nn.Dropout(p=self.dropout)(hidden_act) \n        \n        # hiddens (Batch, 1, hid_dim), hidden_state (1, Batch, hid_dim)\n        hidden = torch.unsqueeze(hidden, dim = 0)   \n        hiddens, hidden_state = self.gru(hidden_act, hidden)  \n        hiddens = hiddens.permute(1,0,2)\n        hidden_state = hidden_state[0]\n        \n        emb_pred = F.log_softmax(self.classifier(hiddens[0]),dim = 1) # (Batch, out_dim)\n\n        return emb_pred, hidden_state\n        #################################################\n\n\n\n\nCode\nclass GRUSeq2Seq(nn.Module):\n    def __init__(self, in_dim, out_dim, emb_dim, hid_dim, device, dropout):\n        super(GRUSeq2Seq, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.device = device\n        self.dropout = nn.Dropout(dropout)\n        \n        self.encoder = GRUEncoder(in_dim, emb_dim, hid_dim)\n        self.decoder = AttnGRUDecoder(emb_dim, hid_dim, out_dim, dropout)\n        \n    def forward(self, src, trg):\n        batch_size, mx_len = src.shape\n        DEVICE = self.device\n        \n        ################### YOUR CODE ###################\n        # Encoder\n        hidden_state_0 = torch.zeros(1, batch_size, self.hid_dim).to(DEVICE)\n        enc_hidden, hidden_state = self.encoder(src, hidden_state_0)\n                \n        # Decoder\n        self.decoder.enc_hiddens = enc_hidden # set encoder's hidden states\n        outputs = torch.zeros(mx_len, batch_size, dataset.output_lang.n_words).to(DEVICE) # to store each decoder's output\n        decodes = trg[:,[0]].to(DEVICE)   \n        \n        for t in np.arange(1, mx_len): # for each t'th token, get decoder outputs\n            output, hidden_state = self.decoder(decodes, hidden_state)\n            outputs[t] = output \n            decodes = trg[:,[t]].to(DEVICE)\n                                           \n        outputs = torch.permute(outputs,dims = (1,2,0))           \n        return outputs\n        #################################################\n\n\n\n\nCode\n# experiment various methods for better performance\n# you can modify the codes in this block\nin_dim = dataset.input_lang.n_words\nout_dim = dataset.output_lang.n_words\n\nhid_dim = 256\nemb_dim = 256\ndropout = 0.3\nlearning_rate = 0.001\nN_EPOCHS = 30\nvalid_every = 3\nbest_valid_loss = float('inf')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ngru_model = GRUSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\n\noptimizer = torch.optim.Adam(gru_model.parameters(), lr=learning_rate)\nloss_fn = nn.NLLLoss(ignore_index = dataset.output_lang_pad)\n\n\n\n\nCode\n# Train your model \n# you can modify the codes in this block\nhistory = {'train_PPL':[], 'val_PPL':[], 'lr':[]}\n\nfor epoch in range(N_EPOCHS):\n    train_loss = train(gru_model, train_dataloader, optimizer, loss_fn, 1)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    \n    if epoch%valid_every==0:\n        print(\"==========================\")\n        valid_loss = evaluate(gru_model, valid_dataloader, loss_fn)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            gru_model.decoder.t=0\n            torch.save(gru_model.state_dict(), 'gru-attn-model.pt')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\n        history['train_PPL'].append(math.exp(train_loss))\n        history['val_PPL'].append(math.exp(valid_loss))\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n\nplot_history(history) \n\n\nEpoch: 01\n    Train Loss: 3.076 | Train PPL:  21.672\n==========================\n     Val. Loss: 2.423 |  Val. PPL:  11.282\nEpoch: 02\n    Train Loss: 2.149 | Train PPL:   8.579\nEpoch: 03\n    Train Loss: 1.831 | Train PPL:   6.238\nEpoch: 04\n    Train Loss: 1.626 | Train PPL:   5.084\n==========================\n     Val. Loss: 1.786 |  Val. PPL:   5.963\nEpoch: 05\n    Train Loss: 1.462 | Train PPL:   4.316\nEpoch: 06\n    Train Loss: 1.317 | Train PPL:   3.734\nEpoch: 07\n    Train Loss: 1.195 | Train PPL:   3.303\n==========================\n     Val. Loss: 1.571 |  Val. PPL:   4.812\nEpoch: 08\n    Train Loss: 1.089 | Train PPL:   2.971\nEpoch: 09\n    Train Loss: 0.995 | Train PPL:   2.705\nEpoch: 10\n    Train Loss: 0.914 | Train PPL:   2.493\n==========================\n     Val. Loss: 1.477 |  Val. PPL:   4.382\nEpoch: 11\n    Train Loss: 0.843 | Train PPL:   2.324\nEpoch: 12\n    Train Loss: 0.777 | Train PPL:   2.176\nEpoch: 13\n    Train Loss: 0.717 | Train PPL:   2.049\n==========================\n     Val. Loss: 1.423 |  Val. PPL:   4.149\nEpoch: 14\n    Train Loss: 0.663 | Train PPL:   1.941\nEpoch: 15\n    Train Loss: 0.617 | Train PPL:   1.853\nEpoch: 16\n    Train Loss: 0.571 | Train PPL:   1.770\n==========================\n     Val. Loss: 1.384 |  Val. PPL:   3.991\nEpoch: 17\n    Train Loss: 0.531 | Train PPL:   1.700\nEpoch: 18\n    Train Loss: 0.497 | Train PPL:   1.644\nEpoch: 19\n    Train Loss: 0.463 | Train PPL:   1.590\n==========================\n     Val. Loss: 1.374 |  Val. PPL:   3.951\nEpoch: 20\n    Train Loss: 0.433 | Train PPL:   1.542\nEpoch: 21\n    Train Loss: 0.402 | Train PPL:   1.495\nEpoch: 22\n    Train Loss: 0.376 | Train PPL:   1.457\n==========================\n     Val. Loss: 1.364 |  Val. PPL:   3.912\nEpoch: 23\n    Train Loss: 0.352 | Train PPL:   1.422\nEpoch: 24\n    Train Loss: 0.327 | Train PPL:   1.386\nEpoch: 25\n    Train Loss: 0.309 | Train PPL:   1.362\n==========================\n     Val. Loss: 1.398 |  Val. PPL:   4.048\nEpoch: 26\n    Train Loss: 0.291 | Train PPL:   1.338\nEpoch: 27\n    Train Loss: 0.272 | Train PPL:   1.313\nEpoch: 28\n    Train Loss: 0.256 | Train PPL:   1.292\n==========================\n     Val. Loss: 1.370 |  Val. PPL:   3.937\nEpoch: 29\n    Train Loss: 0.239 | Train PPL:   1.270\nEpoch: 30\n    Train Loss: 0.222 | Train PPL:   1.249\n\n\n\n\n\n\n\nCode\n# Test your model\ntorch.save(gru_model.state_dict(), 'gru-attn-model.pt') \nloaded_model = GRUSeq2Seq(in_dim, out_dim, emb_dim, hid_dim, device, dropout).to(device)\ngru_model.load_state_dict(torch.load('gru-attn-model.pt'))\n\ntest_loss = evaluate(gru_model, test_dataloader, loss_fn)\nprint(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')\n\n\n     Test. Loss: 1.370 |  Test. PPL:   3.937"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#implement-transformer-seq2seq-model",
    "href": "posts/MLDL IV2/hw4_translation.html#implement-transformer-seq2seq-model",
    "title": "Assignment 4-2",
    "section": "Implement Transformer Seq2Seq Model",
    "text": "Implement Transformer Seq2Seq Model\n\n\nCode\nclass TransEncoder(nn.Module):\n    def __init__(self, input_dim, hid_dim, n_layers, n_heads, ff_dim, dropout, device, max_length = MAX_LENGTH):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.max_length = max_length\n        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n        encoder_layer = TransformerEncoderLayer(hid_dim, n_heads, ff_dim, dropout, batch_first=True)\n        self.encoder = TransformerEncoder(encoder_layer, n_layers)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.tensor([hid_dim], device = device, dtype=torch.float32))\n        \n    def forward(self, src, pos_emb, src_mask):\n        '''\n        Q3 - (c)\n        Implement forward method of TransEncoder Module\n        (Use torch.nn.TransformerEncoder, torch.nn.TransformerEncoderLayer)\n        \n        INPUT\n        - src: source language batched data (B, max_len)\n        - pos_emb: positional embedding (1, max_len, hid_dim)\n        - src_mask: padding mask tensor for source sentences (B, max_len)\n\n        OUTPUT\n        What to be returned depends on your implementation of TransSeq2Seq.\n        Feel free to return outputs you need.\n        Some examples below,\n\n        - encoder output (B, max_len, hid_dim)\n        '''\n        #################### YOUR CODE ####################\n        batch_size = src.size(0)\n        src_len = src.size(1)\n        output = self.dropout(self.tok_embedding(src) * self.scale + pos_emb)\n        enc_output =  self.encoder(output, src_key_padding_mask = src_mask)     \n        # https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html  \n        return enc_output\n        ###################################################\n        \n\n\n\n\nCode\nclass TransDecoder(nn.Module):\n    def __init__(self, out_dim, hid_dim, n_layers, n_heads, ff_dim, dropout, device, max_length = MAX_LENGTH):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.max_length = max_length\n        self.tok_embedding = nn.Embedding(out_dim, hid_dim)\n        decoder_layer = TransformerDecoderLayer(hid_dim, n_heads, ff_dim, dropout, batch_first=True)\n        self.decoder = TransformerDecoder(decoder_layer, n_layers)\n        self.fc_out = nn.Linear(hid_dim, out_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.tensor([hid_dim], device = device, dtype=torch.float32))\n        \n    def forward(self, trg, pos_emb, enc_src, trg_mask, trg_sub_mask, src_mask):\n        '''\n        Q3 - (c)\n        Implement forward method of TransDecoder Module\n        (Use torch.nn.TransformerDecoder, torch.nn.TransformerDecoderLayer)\n        \n        INPUT\n        - trg: target language batched data (B, max_len)\n        - pos_emb: positional embedding (1, max_len, hid_dim)\n        - enc_src: encoder outputs (B, max_len, hid_dim)\n        - trg_mask: padding mask tensor for target sentences (B, max_len)\n        - trg_sub_mask: subsequent mask for target sentences (max_len, max_len)\n        - src_mask: padding mask tensor for source sentences (B, max_len)\n\n        OUTPUT\n        What to be returned depends on your implementation of TransSeq2Seq.\n        Feel free to return outputs you need.\n        Some examples below,\n\n        - decoder output (B, max_len, out_dim)\n        '''\n        #################### YOUR CODE ####################\n        batch_size = trg.size(0)\n        trg_len = trg.size(1)\n        output = self.tok_embedding(trg) * self.scale + pos_emb\n        dec_output = self.decoder(tgt=output, memory = enc_src, tgt_mask = trg_sub_mask, tgt_key_padding_mask = trg_mask, memory_key_padding_mask = src_mask)\n        # https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html\n        dec_output_fc = self.fc_out(dec_output)\n        dec_output_fc = self.dropout(dec_output_fc)\n\n        return dec_output_fc\n        ###################################################\n\n\n\n\nCode\nclass TransSeq2Seq(nn.Module):\n    def __init__(self, in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout_p, device, max_length=MAX_LENGTH):\n        super().__init__()\n        \n        self.device = device\n        self.hid_dim = hid_dim\n        self.max_length = max_length\n\n        self.encoder = TransEncoder(in_dim, hid_dim, n_layers[0], n_heads, ff_dim, dropout_p, device)\n        self.decoder = TransDecoder(out_dim, hid_dim, n_layers[1], n_heads, ff_dim, dropout_p, device)\n        \n    def make_src_mask(self, src):\n        '''\n        Q3 - (b)\n        Implement mask generating function\n\n        INPUT\n        - src: batched input sentences (B, max_len)\n\n        OUTPUT\n        - Boolean padding mask tensor (B, max_len)\n        '''\n        #################### YOUR CODE ####################\n        DEVICE = self.device\n        \n        source_mask = (src == dataset.input_lang_pad)\n        source_mask = source_mask.to(DEVICE)\n        \n        return source_mask\n        ###################################################\n\n    def make_trg_mask(self, trg):\n        '''\n        Q3 - (b)\n        Implement mask generating function\n\n        INPUT\n        - trg: batched target sentences (B, max_len)\n\n        OUTPUT\n        - A tuple of a padding mask tensor and a subsequent mask tensor ((B, max_len), (max_len, max_len))\n        '''\n        #################### YOUR CODE ####################\n        DEVICE = self.device\n        max_length = self.max_length\n        \n        target_mask = (trg == dataset.input_lang_pad)\n        target_mask = target_mask.to(DEVICE)\n        \n        sub_mask = torch.triu(torch.ones((max_length,max_length)), diagonal=1).bool()\n        sub_mask = sub_mask.to(DEVICE)\n\n        return target_mask, sub_mask\n        ###################################################\n\n    def forward(self, src, trg):\n        '''\n        Q3 - (c)\n        Implement forward method of TransSeq2Seq Module\n        \n        INPUT\n        - src: source language batched data (B, max_len)\n        - trg: target language batched data (B, max_len)\n\n        OUTPUT\n        - decoder output (B, out_dim, max_dim)\n        \n        '''\n        #################### YOUR CODE ####################\n        src_mask = self.make_src_mask(src)\n        trg_mask, trg_sub_mask = self.make_trg_mask(trg)\n        pos_emb = self.get_pos_emb()\n        enc_output = self.encoder(src=src, pos_emb=pos_emb, src_mask=src_mask)\n        dec_output = self.decoder(trg=trg, pos_emb=pos_emb, enc_src=enc_output, trg_mask=trg_mask, trg_sub_mask=trg_sub_mask, src_mask=src_mask)\n        dec_output = dec_output.permute((0, 2, 1))\n        \n        return dec_output\n        ###################################################  \n    \n    def get_pos_emb(self):\n        '''\n        Q3 - (a)\n        Implement absolute positional embedding\n\n        OUTPUT\n        - positional embedding tensor (max_len, hid_dim)\n        '''\n        #################### YOUR CODE ####################\n        hid_dim = self.hid_dim \n        max_len = self.max_length \n        DEVICE = self.device\n                \n        embed_temp = torch.zeros((max_len, hid_dim))\n        \n        for k in np.arange(max_len):\n            for i in np.arange(int(hid_dim/2)): \n                embed_temp[k, 2*i] = np.sin(k/np.power(10000, 2*i/hid_dim))\n                embed_temp[k, 2*i+1] = np.cos(k/np.power(10000, 2*i/hid_dim))\n                \n        pos_emb = embed_temp.to(DEVICE)\n        return pos_emb\n        ###################################################"
  },
  {
    "objectID": "posts/MLDL IV2/hw4_translation.html#training-1",
    "href": "posts/MLDL IV2/hw4_translation.html#training-1",
    "title": "Assignment 4-2",
    "section": "Training",
    "text": "Training\n\n\nCode\n'''\nQ3 - (d)\nTrain your Seq2Seq model and plot perplexities and learning rates. \nUpon successful training, the test perplexity should be less than 2. \nBriefly report your hyperparmeters and results on test dataset. \nMake sure your results are printed in your submitted file.\n'''\n\n\n\n\nCode\n# experiment various methods for better performance\n# you can modify the codes in this block\nin_dim = dataset.input_lang.n_words\nout_dim = dataset.output_lang.n_words\nhid_dim = 256\nff_dim = 256\nn_enc_layers = 4\nn_dec_layers = 4\nn_layers = [n_enc_layers, n_dec_layers]\nn_heads = 8\ndropout = 0.3\n\nlearning_rate=0.001\nN_EPOCHS =5\nvalid_every=5\nbest_valid_loss = float('inf')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrans_model = TransSeq2Seq(in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout, device).to(device)\noptimizer = torch.optim.Adam(trans_model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss(ignore_index = dataset.output_lang_pad)\n\n\n\n\nCode\n# Train your model\n# you can modify the codes in this block\nhistory = {'train_PPL':[], 'val_PPL':[], 'lr':[]}\n\nfor epoch in range(N_EPOCHS):\n    train_loss = train(trans_model, train_dataloader, optimizer, loss_fn, 1)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    \n    if epoch%valid_every==0:\n        print(\"==========================\")\n        valid_loss = evaluate(trans_model, valid_dataloader, loss_fn)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            trans_model.decoder.t=0\n            torch.save(trans_model.state_dict(), 'transformer-model.pt')\n\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\n        history['train_PPL'].append(math.exp(train_loss))\n        history['val_PPL'].append(math.exp(valid_loss))\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n\nplot_history(history)\n\n\nEpoch: 01\n    Train Loss: 1.969 | Train PPL:   7.161\n==========================\n     Val. Loss: 0.718 |  Val. PPL:   2.049\nEpoch: 02\n    Train Loss: 0.594 | Train PPL:   1.812\nEpoch: 03\n    Train Loss: 0.267 | Train PPL:   1.306\nEpoch: 04\n    Train Loss: 0.127 | Train PPL:   1.135\nEpoch: 05\n    Train Loss: 0.061 | Train PPL:   1.063\n\n\n\n\n\n\n\nCode\n# Test your model\ntorch.save(trans_model.state_dict(), 'transformer-model.pt') \nloaded_model = TransSeq2Seq(in_dim, out_dim, hid_dim, ff_dim, n_layers, n_heads, dropout, device).to(device)\nloaded_model.load_state_dict(torch.load('transformer-model.pt'))\n\ntest_loss = evaluate(loaded_model, test_dataloader, loss_fn)\nprint(f'\\t Test. Loss: {valid_loss:.3f} |  Test. PPL: {math.exp(valid_loss):7.3f}')\n\n\n     Test. Loss: 0.718 |  Test. PPL:   2.049"
  },
  {
    "objectID": "posts/Public Data 1/index.html",
    "href": "posts/Public Data 1/index.html",
    "title": "Analysis 1: Number of local government employees",
    "section": "",
    "text": "The number of local government employees in 2020 decreased by 9.5% (compared to 2018) and 13.32 % (compared to 2019) by on average. This decrease was transitory, since it was affected by The Transition of Fire Officials to National Position (2020)."
  },
  {
    "objectID": "posts/Public Data 1/index.html#package-load",
    "href": "posts/Public Data 1/index.html#package-load",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Package Load",
    "text": "- Package Load\n\n\nCode\npacman::p_load(\"jsonlite\",\n               \"tidyverse\",\n               \"forecast\",\n               \"ggfortify\",\n               \"forecast\",\n               \"httr\",\n               \"sleekts\",\n               \"lubridate\",\n               \"stats\",\n               \"smooth\",\n               \"ghibli\",\n               \"plyr\",\n               \"scales\",\n               \"formattable\",\n               \"knitr\",\n               \"showtext\",\n               \"kableExtra\",\n               \"IRdisplay\",\n               \"glue\",\n               \"echarts4r\",\n               \"plotly\")"
  },
  {
    "objectID": "posts/Public Data 1/index.html#adding-fonts-for-ggplot2",
    "href": "posts/Public Data 1/index.html#adding-fonts-for-ggplot2",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Adding fonts for ggplot2",
    "text": "- Adding fonts for ggplot2\n\n\nCode\nfont_add_google(name=\"Lato\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/Public Data 1/index.html#importing-data",
    "href": "posts/Public Data 1/index.html#importing-data",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Importing data",
    "text": "- Importing data\n\n\nCode\n# auto inporting function\nkosis1 = function (a,b){\n    library(\"jsonlite\")\n    library(\"dplyr\")\n    i1=a\n    i2=b    \n    years=seq(i1+1,i2,1)\n    data= fromJSON(glue::glue(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1YL2101E/2/1/20211111002533&prdSe=Y&startPrdDe={i1}&endPrdDe={i1}\"))\n    for(i in years) {\n        assign(paste(\"employee_\",i,sep=\"\"), \n               fromJSON(glue::glue(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1YL2101E/2/1/20211111002533&prdSe=Y&startPrdDe={i}&endPrdDe={i}\")))\n        data %>% \n            full_join(get(paste(\"employee_\",i,sep=\"\"))) -> data\n        print(paste(\"employee_\",i,\" added\",sep=\"\"))\n    }\n    return(data)\n}\n\n# Importing Data\nemployee_data=kosis1(2008,2020)\n\n\n\n\nCode\nemployee_data %>% \n    glimpse()\n\n\nRows: 230\nColumns: 14\n$ TBL_NM      <chr> \"지방자치단체공무원 현원(시도)\", \"지방자치단체공무원 현원(…\n$ PRD_DE      <chr> \"2008\", \"2008\", \"2008\", \"2008\", \"2008\", \"2008\", \"2008\", \"2…\n$ TBL_ID      <chr> \"DT_1YL2101E\", \"DT_1YL2101E\", \"DT_1YL2101E\", \"DT_1YL2101E\"…\n$ ITM_NM      <chr> \"지방자치단체공무원 현원\", \"지방자치단체공무원 현원\", \"지…\n$ ITM_ID      <chr> \"16110T2009_036\", \"16110T2009_036\", \"16110T2009_036\", \"161…\n$ ORG_ID      <chr> \"101\", \"101\", \"101\", \"101\", \"101\", \"101\", \"101\", \"101\", \"1…\n$ UNIT_NM     <chr> \"명\", \"명\", \"명\", \"명\", \"명\", \"명\", \"명\", \"명\", \"명\", \"명\"…\n$ UNIT_NM_ENG <chr> \"In person\", \"In person\", \"In person\", \"In person\", \"In pe…\n$ C1_OBJ_NM   <chr> \"행정구역별\", \"행정구역별\", \"행정구역별\", \"행정구역별\", \"…\n$ DT          <chr> \"275231\", \"46270\", \"15752\", \"10799\", \"12964\", \"6473\", \"676…\n$ PRD_SE      <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\"…\n$ C1          <chr> \"15110SDB000\", \"15110SDB001\", \"15110SDB002\", \"15110SDB003\"…\n$ C1_NM       <chr> \"전국\", \"서울특별시\", \"부산광역시\", \"대구광역시\", \"인천광…\n$ C1_NM_ENG   <chr> \"Whole country\", \"Seoul\", \"Busan\", \"Daegu\", \"Incheon\", \"Gw…\n\n\n\n\nCode\n# Selected Provinces \nemployee_data$C1_NM_ENG %>% \n    unique() -> level\nlevel\n\n\n [1] \"Whole country\"     \"Seoul\"             \"Busan\"            \n [4] \"Daegu\"             \"Incheon\"           \"Gwangju\"          \n [7] \"Daejeon\"           \"Ulsan\"             \"Gyeonggi-do\"      \n[10] \"Gangwon-do\"        \"Chungcheongbuk-do\" \"Chungcheongnam-do\"\n[13] \"Jeollabuk-do\"      \"Jeollanam-do\"      \"Gyeongsangbuk-do\" \n[16] \"Gyeongsangnam-do\"  \"Jeju\"              \"Sejong\"           \n\n\nSouth Korea is made up of 17 first-tier administrative divisions: 6 metropolitan cities (gwangyeoksi 광역시/廣域市), 1 special city (teukbyeolsi 특별시/特別市), 1 special self-governing city (teukbyeol-jachisi 특별자치시/特別自治市), and 9 provinces (do 도/道), including one special self-governing province (teukbyeol jachido 특별자치도/特別自治道). (cited from Wikipedia)"
  },
  {
    "objectID": "posts/Public Data 1/index.html#clensing-data",
    "href": "posts/Public Data 1/index.html#clensing-data",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Clensing data",
    "text": "- Clensing data\n\n\nCode\nemployee_data$DT=as.numeric(employee_data$DT)\nemployee_data=employee_data%>%\n    dplyr::rename(ad_division=C1_NM_ENG) \n\n\nemployee_data %>% \n    filter(PRD_DE==2018) %>% \n    select(ad_division,DT) %>% \n    dplyr::rename(employee_2018=\"DT\") -> employee_2018\n\nemployee_data %>% \n    filter(PRD_DE==2019) %>% \n    select(ad_division,DT) %>% \n    dplyr::rename(employee_2019=\"DT\") -> employee_2019\n\nemployee_data %>% \n    filter(PRD_DE==2020) %>% \n    select(ad_division,DT) %>% \n    dplyr::rename(employee_2020=\"DT\") -> employee_2020"
  },
  {
    "objectID": "posts/Public Data 1/index.html#the-decrease-rate-of-the-number-of-employees-in-2020-compared-to-2018",
    "href": "posts/Public Data 1/index.html#the-decrease-rate-of-the-number-of-employees-in-2020-compared-to-2018",
    "title": "Analysis 1: Number of local government employees",
    "section": "- The decrease rate of the number of employees in 2020 compared to 2018",
    "text": "- The decrease rate of the number of employees in 2020 compared to 2018\n\n\nCode\nemployee_2018 %>% \n    full_join(employee_2019) %>% \n    full_join(employee_2020) %>% \n    dplyr::rename(Local_Divisions=\"ad_division\") -> emp_num\n\n\nJoining, by = \"ad_division\"\nJoining, by = \"ad_division\"\n\n\nCode\nemp_num$Local_Divisions=factor(emp_num$Local_Divisions,levels=rev(level))\n\nknitr::kable(emp_num,format.args = list(big.mark = \",\"),\n            caption = \"The Number of local government employees (2018, 2019, 2020)\",\n            col.names = gsub(\"[_]\", \" \", names(emp_num))) %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nThe Number of local government employees (2018, 2019, 2020)\n \n  \n    Local Divisions \n    employee 2018 \n    employee 2019 \n    employee 2020 \n  \n \n\n  \n    Whole country \n    322,862 \n    337,084 \n    292,182 \n  \n  \n    Seoul \n    50,599 \n    52,081 \n    45,826 \n  \n  \n    Busan \n    19,088 \n    19,294 \n    16,509 \n  \n  \n    Daegu \n    12,912 \n    13,321 \n    10,975 \n  \n  \n    Incheon \n    14,515 \n    15,662 \n    13,478 \n  \n  \n    Gwangju \n    7,956 \n    8,409 \n    7,377 \n  \n  \n    Daejeon \n    7,704 \n    7,981 \n    6,704 \n  \n  \n    Ulsan \n    6,409 \n    6,684 \n    5,644 \n  \n  \n    Sejong \n    1,906 \n    2,132 \n    1,767 \n  \n  \n    Gyeonggi-do \n    54,864 \n    58,293 \n    51,147 \n  \n  \n    Gangwon-do \n    18,599 \n    19,215 \n    16,047 \n  \n  \n    Chungcheongbuk-do \n    13,947 \n    14,661 \n    13,040 \n  \n  \n    Chungcheongnam-do \n    18,180 \n    19,313 \n    16,522 \n  \n  \n    Jeollabuk-do \n    17,042 \n    17,811 \n    15,639 \n  \n  \n    Jeollanam-do \n    21,698 \n    22,685 \n    20,247 \n  \n  \n    Gyeongsangbuk-do \n    26,745 \n    27,829 \n    24,047 \n  \n  \n    Gyeongsangnam-do \n    24,836 \n    25,690 \n    22,056 \n  \n  \n    Jeju \n    5,862 \n    6,023 \n    5,157 \n  \n\n\n\n\n\n\n\nCode\nemp_num %>% \n    mutate(Increase_Rate=formattable::percent((employee_2020-employee_2018)/employee_2018)) %>%\n    select(Local_Divisions,Increase_Rate) %>% \n    kbl(col.names = gsub(\"[_]\", \" \", names(.)),\n        caption = \"The decrease rate of the number of employees in 2020 compared to 2018\") %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nThe decrease rate of the number of employees in 2020 compared to 2018\n \n  \n    Local Divisions \n    Increase Rate \n  \n \n\n  \n    Whole country \n    -9.50% \n  \n  \n    Seoul \n    -9.43% \n  \n  \n    Busan \n    -13.51% \n  \n  \n    Daegu \n    -15.00% \n  \n  \n    Incheon \n    -7.14% \n  \n  \n    Gwangju \n    -7.28% \n  \n  \n    Daejeon \n    -12.98% \n  \n  \n    Ulsan \n    -11.94% \n  \n  \n    Sejong \n    -7.29% \n  \n  \n    Gyeonggi-do \n    -6.77% \n  \n  \n    Gangwon-do \n    -13.72% \n  \n  \n    Chungcheongbuk-do \n    -6.50% \n  \n  \n    Chungcheongnam-do \n    -9.12% \n  \n  \n    Jeollabuk-do \n    -8.23% \n  \n  \n    Jeollanam-do \n    -6.69% \n  \n  \n    Gyeongsangbuk-do \n    -10.09% \n  \n  \n    Gyeongsangnam-do \n    -11.19% \n  \n  \n    Jeju \n    -12.03% \n  \n\n\n\n\n\n\nOverall, the number of employees in every local divisions decreased by around 6 to 15 percentage points."
  },
  {
    "objectID": "posts/Public Data 1/index.html#increase-rate-of-the-number-of-employees-in-2019-compared-to-2018",
    "href": "posts/Public Data 1/index.html#increase-rate-of-the-number-of-employees-in-2019-compared-to-2018",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Increase rate of the number of employees in 2019 compared to 2018",
    "text": "- Increase rate of the number of employees in 2019 compared to 2018\n\n\nCode\nemp_num %>% \n    mutate(increase=round(((employee_2019-employee_2018)/employee_2018)*100,2)) %>%\n    select(Local_Divisions,increase) %>% \n    ggplot(aes(x=Local_Divisions,y=increase)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_classic() +\n    geom_text(aes(label= paste(sprintf(\"%2.2f\", increase),\"%\",sep=\"\")), size=3, position = position_dodge(width = 1))+\n  labs(x=\"17 first-tier administrative divisions\",\n       y=\"Increase Rate(%)\")+\n  scale_y_continuous(breaks=seq(0, 15, 1)) + \n  theme(text=element_text(family=\"Lato\",size=10),\n        plot.title = element_text(size = 5))+\n  coord_flip() -> p \n\nplotly::ggplotly(p, tooltip=\"text\") %>% \n    style(textposition = \"right\") %>% \n    layout(title = list(text = paste0(\n        'Increase rate of employees in 2019 (compared to 2018)',\n        '<br>',\n        '<sup>',\n        'Source: Ministry of Public Administration and Security (KOSIS)','</sup>'),\n        x = 0.1,\n        font=list(size=15)))"
  },
  {
    "objectID": "posts/Public Data 1/index.html#increase-rate-of-the-number-of-employees-in-2020-compared-to-2018",
    "href": "posts/Public Data 1/index.html#increase-rate-of-the-number-of-employees-in-2020-compared-to-2018",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Increase rate of the number of employees in 2020 compared to 2018",
    "text": "- Increase rate of the number of employees in 2020 compared to 2018\n\n\nCode\nemp_num %>% \n    mutate(increase=round(((employee_2020-employee_2018)/employee_2018)*100,2)) %>%\n    select(Local_Divisions,increase) %>% \n    arrange(increase) %>% \n    ggplot(aes(x=Local_Divisions,y=increase)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_classic() +\n    geom_text(aes(label= paste(sprintf(\"%2.2f\", increase),\"%\",sep=\"\")), size=3, position = position_dodge(width = 1))+\n  labs(x=\"17 first-tier administrative divisions\",\n       y=\"Increase Rate(%)\")+\n  scale_y_continuous(breaks=seq(0, 15, 1)) + \n  theme(text=element_text(family=\"Lato\",size=10),\n        plot.title = element_text(size = 5)) +\n  coord_flip() -> p \n\nplotly::ggplotly(p, tooltip=\"text\") %>% \n    style(textposition = \"left\") %>% \n    layout(title = list(text = paste0(\n        'Decrease rate of employees in 2020 (compared to 2018)',\n        '<br>',\n        '<sup>',\n        'Source: Ministry of Public Administration and Security (KOSIS)','</sup>'),\n        x = 0.1,\n        font=list(size=15)))\n\n\n\n\n\n\n\n\nCode\nemp_num %>% \n    mutate(increase=round(((employee_2020-employee_2018)/employee_2018)*100,2)) %>%\n    select(Local_Divisions,increase) %>% \n    arrange(increase) %>% \n    ggplot(aes(x=factor(Local_Divisions,levels=rev(.$Local_Divisions)),y=increase)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_classic() +\n    geom_text(aes(label= paste(sprintf(\"%2.2f\", increase),\"%\",sep=\"\")), size=3, position = position_dodge(width = 1))+\n  labs(x=\"17 first-tier administrative divisions\",\n       y=\"Increase Rate(%)\")+\n  scale_y_continuous(breaks=seq(0, 15, 1)) + \n  theme(text=element_text(family=\"Lato\",size=10),\n        plot.title = element_text(size = 5)) +\n  coord_flip() -> p \n\nplotly::ggplotly(p, tooltip=\"text\") %>% \n    style(textposition = \"left\") %>% \n    layout(title = list(text = paste0(\n        'Decrease rate of employees in 2020 (compared to 2018)',\n        '<br>',\n        '<sup>',\n        'Source: Ministry of Public Administration and Security (KOSIS)','</sup>'),\n        x = 0.1,\n        font=list(size=15)))\n\n\nWarning: Use of `.$Local_Divisions` is discouraged. Use `Local_Divisions` instead.\nUse of `.$Local_Divisions` is discouraged. Use `Local_Divisions` instead."
  },
  {
    "objectID": "posts/Public Data 1/index.html#trends-of-the-number-of-local-government-employees-from-2009-to-2020",
    "href": "posts/Public Data 1/index.html#trends-of-the-number-of-local-government-employees-from-2009-to-2020",
    "title": "Analysis 1: Number of local government employees",
    "section": "- Trends of the number of local government employees from 2009 to 2020",
    "text": "- Trends of the number of local government employees from 2009 to 2020\n\n\nCode\nemployee_data %>% \n    filter(ad_division==\"Whole country\") %>%  \n    arrange(PRD_DE) %>% \n    transmute(year=(PRD_DE),number=DT) %>%\n    mutate_if(is.character,as.numeric) %>%\n    dplyr::mutate(Diff_growth = number - lag(number), \n    # Difference in route between years\n     Rate_percent = round((Diff_growth)/lag(number) * 100,2)) %>% \n    # growth rate in percent\n    filter(year>2008) %>% \n    mutate(year=factor(year)) -> employee3\n\n employee3 %>% \n    kbl(col.names = gsub(\"[_]\", \" \", names(.)),\n        caption = \"The decrease rate of the number of employees in 2020 compared to 2018\",\n        format.args = list(big.mark = \",\")) %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nThe decrease rate of the number of employees in 2020 compared to 2018\n \n  \n    year \n    number \n    Diff growth \n    Rate percent \n  \n \n\n  \n    2009 \n    278,303 \n    3,072 \n    1.12 \n  \n  \n    2010 \n    279,636 \n    1,333 \n    0.48 \n  \n  \n    2011 \n    281,035 \n    1,399 \n    0.50 \n  \n  \n    2012 \n    284,355 \n    3,320 \n    1.18 \n  \n  \n    2013 \n    287,299 \n    2,944 \n    1.04 \n  \n  \n    2014 \n    289,914 \n    2,615 \n    0.91 \n  \n  \n    2015 \n    296,273 \n    6,359 \n    2.19 \n  \n  \n    2016 \n    303,401 \n    7,128 \n    2.41 \n  \n  \n    2017 \n    310,654 \n    7,253 \n    2.39 \n  \n  \n    2018 \n    322,862 \n    12,208 \n    3.93 \n  \n  \n    2019 \n    337,084 \n    14,222 \n    4.40 \n  \n  \n    2020 \n    292,182 \n    -44,902 \n    -13.32 \n  \n\n\n\n\n\n\n\nCode\nfont_add_google(name=\"Noto Serif\")\nemployee3  %>% \n  ggplot(aes(x=as.factor(year))) + \n  geom_bar(aes(fill=\"pink\",y=Diff_growth),stat = \"identity\") + \n  theme_classic()+\n  geom_text(aes(y=Diff_growth+sign(Diff_growth)*-15,label= scales::comma(Diff_growth)), size=5,position = position_dodge(width = 1))+\n  theme(text=element_text(family=\"Noto Serif\",size=12),legend.position = \"none\")+\n  scale_x_discrete(labels = c(2009:2020))+\n  scale_y_continuous(breaks =seq(-45000,15000,by=5000)) +\n  labs(x=\"17 first-tier administrative divisions\",\n       y=\"Increase Rate(%)\")-> p1\n\nplotly::ggplotly(p1, tooltip=\"text\") %>% \n    style(textposition = \"center\") %>% \n    layout(title = list(text = paste0(\n        'Trends of the number of local government employees from 2009 to 2020',\n        '<br>',\n        '<sup>',\n        'Source: Ministry of Public Administration and Security (KOSIS)','</sup>'),\n        x = 0.01,\n        font=list(size=16)))\n\n\n\n\n\n\n\n\nCode\nemployee3  %>% \n  ggplot(aes(x=as.factor(year))) + \n  geom_bar(aes(x = as.factor(year), y = Diff_growth), stat = \"identity\",fill=\"gray\",alpha=0.3)+\n  geom_line(aes(x = as.integer(1:12), y = Rate_percent*3500),color=\"orange\",size=1)+\n  geom_point(aes(x = as.factor(year), y = Rate_percent*3500),color=\"red\",size=2, alpha=0.7) +\n  geom_text(aes(x = as.factor(year),y=Rate_percent*3500,label= paste(sprintf(\"%2.2f\", Rate_percent),\"%\",sep=\"\")),color=\"black\",hjust=-0.1,vjust=-2.3, size=8,position = position_dodge(width = 2))+\n  theme_classic()+\n  scale_y_continuous(sec.axis = sec_axis(~./3500, name = paste(\"Increase Rate  (%)\",\"\\n\"),breaks = seq(-15,10,5)),breaks =comma(seq(-50000,25000,by=5000),format = \"d\") )+\n  scale_x_discrete(labels = as.factor(c(2009:2020)))+\n  coord_cartesian(ylim=c(-50000,25000))+\n  labs(x=\"17 first-tier administrative divisions\",\n       y=\"Change in # of employees\",\n       title=\"Trends of the number of local government employees from 2009 to 2020\",\n       subtitle=\"Source: Ministry of Public Administration and Security (KOSIS)\") +\n  theme(text=element_text(family=\"Noto Serif\",size=25))\n\n\nWarning: position_dodge requires non-overlapping x intervals"
  },
  {
    "objectID": "posts/Public Data 2/index.html",
    "href": "posts/Public Data 2/index.html",
    "title": "Analysis 2: Unemployment index",
    "section": "",
    "text": "Most important part in the unemployment index analysis is how specific figures were calculated, how the definition of the ‘unemployment’ was constructed, and how the data were surveyed and acquired."
  },
  {
    "objectID": "posts/Public Data 2/index.html#package-load",
    "href": "posts/Public Data 2/index.html#package-load",
    "title": "Analysis 2: Unemployment index",
    "section": "- Package Load",
    "text": "- Package Load\n\n\nCode\npacman::p_load(\"jsonlite\",\n               \"tidyverse\",\n               \"forecast\",\n               \"ggfortify\",\n               \"forecast\",\n               \"httr\",\n               \"sleekts\",\n               \"lubridate\",\n               \"stats\",\n               \"smooth\",\n               \"ghibli\",\n               \"plyr\",\n               \"scales\",\n               \"formattable\",\n               \"knitr\",\n               \"showtext\",\n               \"kableExtra\",\n               \"IRdisplay\",\n               \"glue\",\n               \"echarts4r\",\n               \"plotly\",\n               \"magrittr\")"
  },
  {
    "objectID": "posts/Public Data 2/index.html#adding-fonts-for-ggplot2",
    "href": "posts/Public Data 2/index.html#adding-fonts-for-ggplot2",
    "title": "Analysis 2: Unemployment index",
    "section": "- Adding fonts for ggplot2",
    "text": "- Adding fonts for ggplot2\n\n\nCode\nfont_add_google(name=\"Noto Serif\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/Public Data 2/index.html#importing-data",
    "href": "posts/Public Data 2/index.html#importing-data",
    "title": "Analysis 2: Unemployment index",
    "section": "- Importing data",
    "text": "- Importing data\n\n\nCode\n# auto inporting function\ntotal=fromJSON(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DA7001S/2/1/20211110114643_1&prdSe=M&newEstPrdCnt=142\")\nmale=fromJSON(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DA7001S/2/1/20211110114643_2&prdSe=M&newEstPrdCnt=142\")\nfemale=fromJSON(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DA7001S/2/1/20211110114643_3&prdSe=M&newEstPrdCnt=142\")\n\ntotal %>% \n    full_join(male) %>% \n    full_join(female) -> unemployment_data\n\n\n\n\nCode\nunemployment_data %T>% \n    glimpse() %>% \n    head(5) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\nRows: 426\nColumns: 16\n$ TBL_NM        <chr> \"성별 경제활동인구 총괄\", \"성별 경제활동인구 총괄\", \"성…\n$ PRD_DE        <chr> \"201010\", \"201011\", \"201012\", \"201101\", \"201102\", \"20110…\n$ TBL_ID        <chr> \"DT_1DA7001S\", \"DT_1DA7001S\", \"DT_1DA7001S\", \"DT_1DA7001…\n$ ITM_NM        <chr> \"실업률\", \"실업률\", \"실업률\", \"실업률\", \"실업률\", \"실업…\n$ ITM_NM_ENG    <chr> \"Unemployment rate\", \"Unemployment rate\", \"Unemployment …\n$ ITM_ID        <chr> \"T80\", \"T80\", \"T80\", \"T80\", \"T80\", \"T80\", \"T80\", \"T80\", …\n$ UNIT_NM       <chr> \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"…\n$ ORG_ID        <chr> \"101\", \"101\", \"101\", \"101\", \"101\", \"101\", \"101\", \"101\", …\n$ UNIT_NM_ENG   <chr> \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"%\", \"…\n$ C1_OBJ_NM     <chr> \"성별\", \"성별\", \"성별\", \"성별\", \"성별\", \"성별\", \"성별\", …\n$ C1_OBJ_NM_ENG <chr> \"By gender\", \"By gender\", \"By gender\", \"By gender\", \"By …\n$ DT            <chr> \"3.3\", \"3\", \"3.5\", \"3.8\", \"4.5\", \"4.3\", \"3.7\", \"3.2\", \"3…\n$ PRD_SE        <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"…\n$ C1            <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ C1_NM         <chr> \"계\", \"계\", \"계\", \"계\", \"계\", \"계\", \"계\", \"계\", \"계\", \"…\n$ C1_NM_ENG     <chr> \"Total\", \"Total\", \"Total\", \"Total\", \"Total\", \"Total\", \"T…\n\n\n\n\n \n  \n    TBL_NM \n    PRD_DE \n    TBL_ID \n    ITM_NM \n    ITM_NM_ENG \n    ITM_ID \n    UNIT_NM \n    ORG_ID \n    UNIT_NM_ENG \n    C1_OBJ_NM \n    C1_OBJ_NM_ENG \n    DT \n    PRD_SE \n    C1 \n    C1_NM \n    C1_NM_ENG \n  \n \n\n  \n    성별 경제활동인구 총괄 \n    201010 \n    DT_1DA7001S \n    실업률 \n    Unemployment rate \n    T80 \n    % \n    101 \n    % \n    성별 \n    By gender \n    3.3 \n    M \n    0 \n    계 \n    Total \n  \n  \n    성별 경제활동인구 총괄 \n    201011 \n    DT_1DA7001S \n    실업률 \n    Unemployment rate \n    T80 \n    % \n    101 \n    % \n    성별 \n    By gender \n    3 \n    M \n    0 \n    계 \n    Total \n  \n  \n    성별 경제활동인구 총괄 \n    201012 \n    DT_1DA7001S \n    실업률 \n    Unemployment rate \n    T80 \n    % \n    101 \n    % \n    성별 \n    By gender \n    3.5 \n    M \n    0 \n    계 \n    Total \n  \n  \n    성별 경제활동인구 총괄 \n    201101 \n    DT_1DA7001S \n    실업률 \n    Unemployment rate \n    T80 \n    % \n    101 \n    % \n    성별 \n    By gender \n    3.8 \n    M \n    0 \n    계 \n    Total \n  \n  \n    성별 경제활동인구 총괄 \n    201102 \n    DT_1DA7001S \n    실업률 \n    Unemployment rate \n    T80 \n    % \n    101 \n    % \n    성별 \n    By gender \n    4.5 \n    M \n    0 \n    계 \n    Total"
  },
  {
    "objectID": "posts/Public Data 2/index.html#clensing-data",
    "href": "posts/Public Data 2/index.html#clensing-data",
    "title": "Analysis 2: Unemployment index",
    "section": "- Clensing data",
    "text": "- Clensing data\n\n\nCode\nunemployment_data$C1_NM_ENG %>% table()\n\n\n.\nFemale   Male  Total \n   142    142    142 \n\n\nCode\nunemployment_data %>% \n    transmute(Month=PRD_DE,Gender=ordered(C1_NM_ENG,levels=c(\"Total\",\"Male\",\"Female\")),                                          Unemployment_Rate=DT) %>% \n    tidyr::spread(key=\"Month\",value=\"Unemployment_Rate\") %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    Gender \n    201010 \n    201011 \n    201012 \n    201101 \n    201102 \n    201103 \n    201104 \n    201105 \n    201106 \n    201107 \n    201108 \n    201109 \n    201110 \n    201111 \n    201112 \n    201201 \n    201202 \n    201203 \n    201204 \n    201205 \n    201206 \n    201207 \n    201208 \n    201209 \n    201210 \n    201211 \n    201212 \n    201301 \n    201302 \n    201303 \n    201304 \n    201305 \n    201306 \n    201307 \n    201308 \n    201309 \n    201310 \n    201311 \n    201312 \n    201401 \n    201402 \n    201403 \n    201404 \n    201405 \n    201406 \n    201407 \n    201408 \n    201409 \n    201410 \n    201411 \n    201412 \n    201501 \n    201502 \n    201503 \n    201504 \n    201505 \n    201506 \n    201507 \n    201508 \n    201509 \n    201510 \n    201511 \n    201512 \n    201601 \n    201602 \n    201603 \n    201604 \n    201605 \n    201606 \n    201607 \n    201608 \n    201609 \n    201610 \n    201611 \n    201612 \n    201701 \n    201702 \n    201703 \n    201704 \n    201705 \n    201706 \n    201707 \n    201708 \n    201709 \n    201710 \n    201711 \n    201712 \n    201801 \n    201802 \n    201803 \n    201804 \n    201805 \n    201806 \n    201807 \n    201808 \n    201809 \n    201810 \n    201811 \n    201812 \n    201901 \n    201902 \n    201903 \n    201904 \n    201905 \n    201906 \n    201907 \n    201908 \n    201909 \n    201910 \n    201911 \n    201912 \n    202001 \n    202002 \n    202003 \n    202004 \n    202005 \n    202006 \n    202007 \n    202008 \n    202009 \n    202010 \n    202011 \n    202012 \n    202101 \n    202102 \n    202103 \n    202104 \n    202105 \n    202106 \n    202107 \n    202108 \n    202109 \n    202110 \n    202111 \n    202112 \n    202201 \n    202202 \n    202203 \n    202204 \n    202205 \n    202206 \n    202207 \n  \n \n\n  \n    Total \n    3.3 \n    3 \n    3.5 \n    3.8 \n    4.5 \n    4.3 \n    3.7 \n    3.2 \n    3.3 \n    3.3 \n    3 \n    3 \n    2.9 \n    2.9 \n    3 \n    3.5 \n    4.2 \n    3.7 \n    3.5 \n    3.1 \n    3.2 \n    3.1 \n    3 \n    2.9 \n    2.8 \n    2.8 \n    2.9 \n    3.4 \n    3.9 \n    3.5 \n    3.2 \n    3 \n    3.1 \n    3.1 \n    3 \n    2.7 \n    2.7 \n    2.6 \n    3 \n    3.4 \n    4.5 \n    3.9 \n    3.8 \n    3.5 \n    3.5 \n    3.4 \n    3.3 \n    3.1 \n    3.2 \n    3 \n    3.3 \n    3.7 \n    4.5 \n    4 \n    3.9 \n    3.7 \n    3.8 \n    3.6 \n    3.4 \n    3.2 \n    3.1 \n    3 \n    3.2 \n    3.7 \n    4.9 \n    4.2 \n    3.9 \n    3.6 \n    3.6 \n    3.5 \n    3.6 \n    3.5 \n    3.3 \n    3.1 \n    3.2 \n    3.7 \n    4.9 \n    4.1 \n    4.2 \n    3.6 \n    3.8 \n    3.4 \n    3.6 \n    3.3 \n    3.2 \n    3.1 \n    3.3 \n    3.7 \n    4.6 \n    4.5 \n    4.1 \n    4 \n    3.7 \n    3.7 \n    4 \n    3.6 \n    3.5 \n    3.2 \n    3.4 \n    4.5 \n    4.7 \n    4.3 \n    4.4 \n    4 \n    4 \n    3.9 \n    3 \n    3.1 \n    3 \n    3.1 \n    3.4 \n    4.1 \n    4.1 \n    4.2 \n    4.2 \n    4.5 \n    4.3 \n    4 \n    3.1 \n    3.6 \n    3.7 \n    3.4 \n    4.1 \n    5.7 \n    4.9 \n    4.3 \n    4 \n    4 \n    3.8 \n    3.2 \n    2.6 \n    2.7 \n    2.8 \n    2.6 \n    3.5 \n    4.1 \n    3.4 \n    3 \n    3 \n    3 \n    3 \n    2.9 \n  \n  \n    Male \n    3.5 \n    3.3 \n    3.7 \n    3.9 \n    4.5 \n    4.4 \n    4 \n    3.4 \n    3.4 \n    3.6 \n    3.3 \n    3.3 \n    3.2 \n    3.1 \n    3.2 \n    3.6 \n    4.1 \n    3.9 \n    3.6 \n    3.2 \n    3.4 \n    3.3 \n    3.2 \n    3.1 \n    2.9 \n    3 \n    3.3 \n    3.6 \n    4.1 \n    3.6 \n    3.3 \n    3.1 \n    3.2 \n    3.4 \n    3.3 \n    2.9 \n    2.9 \n    2.8 \n    3.1 \n    3.5 \n    4.2 \n    3.9 \n    3.8 \n    3.4 \n    3.5 \n    3.3 \n    3.5 \n    3.3 \n    3.3 \n    3.1 \n    3.4 \n    3.8 \n    4.3 \n    4 \n    3.8 \n    3.7 \n    3.9 \n    3.8 \n    3.5 \n    3.4 \n    3.1 \n    3.2 \n    3.3 \n    3.7 \n    4.7 \n    4.4 \n    4 \n    3.7 \n    3.8 \n    3.7 \n    3.8 \n    3.7 \n    3.3 \n    3.1 \n    3.1 \n    3.7 \n    4.7 \n    4.1 \n    4.3 \n    3.7 \n    3.9 \n    3.6 \n    3.8 \n    3.6 \n    3.4 \n    3.3 \n    3.5 \n    3.7 \n    4.3 \n    4.6 \n    4.2 \n    4.2 \n    3.8 \n    3.8 \n    4.2 \n    3.8 \n    3.6 \n    3.4 \n    3.7 \n    4.4 \n    4.5 \n    4.4 \n    4.6 \n    4.3 \n    4.2 \n    3.9 \n    3.2 \n    3.4 \n    3.2 \n    3.3 \n    3.3 \n    3.9 \n    4 \n    4.2 \n    4.2 \n    4.6 \n    4.3 \n    4 \n    3 \n    3.7 \n    3.8 \n    3.4 \n    3.7 \n    5 \n    4.7 \n    4.2 \n    4 \n    4 \n    3.8 \n    3.3 \n    2.7 \n    2.8 \n    2.8 \n    2.6 \n    3.1 \n    3.5 \n    3.2 \n    2.9 \n    2.8 \n    2.9 \n    2.9 \n    2.9 \n  \n  \n    Female \n    3 \n    2.5 \n    3.2 \n    3.6 \n    4.5 \n    4.1 \n    3.3 \n    2.9 \n    3.1 \n    2.9 \n    2.6 \n    2.7 \n    2.5 \n    2.5 \n    2.8 \n    3.3 \n    4.3 \n    3.5 \n    3.2 \n    2.9 \n    2.9 \n    2.7 \n    2.7 \n    2.6 \n    2.6 \n    2.4 \n    2.4 \n    3.1 \n    3.6 \n    3.3 \n    3 \n    2.9 \n    2.8 \n    2.7 \n    2.6 \n    2.4 \n    2.4 \n    2.4 \n    2.8 \n    3.4 \n    4.8 \n    3.8 \n    3.8 \n    3.7 \n    3.6 \n    3.4 \n    3.1 \n    2.9 \n    3 \n    2.9 \n    3.3 \n    3.7 \n    4.8 \n    4.1 \n    4 \n    3.8 \n    3.7 \n    3.4 \n    3.2 \n    2.9 \n    3 \n    2.8 \n    3.1 \n    3.6 \n    5 \n    4 \n    3.9 \n    3.5 \n    3.4 \n    3.2 \n    3.3 \n    3.4 \n    3.3 \n    3.1 \n    3.3 \n    3.8 \n    5.2 \n    4.2 \n    4 \n    3.4 \n    3.6 \n    3.2 \n    3.3 \n    2.9 \n    2.9 \n    2.9 \n    3 \n    3.8 \n    5 \n    4.4 \n    4 \n    3.7 \n    3.6 \n    3.6 \n    3.8 \n    3.5 \n    3.3 \n    3 \n    3.1 \n    4.5 \n    5 \n    4.1 \n    4.1 \n    3.7 \n    3.7 \n    3.7 \n    2.8 \n    2.8 \n    2.8 \n    2.7 \n    3.4 \n    4.4 \n    4.2 \n    4.4 \n    4.2 \n    4.5 \n    4.4 \n    4.1 \n    3.2 \n    3.4 \n    3.5 \n    3.5 \n    4.6 \n    6.7 \n    5.2 \n    4.4 \n    4.1 \n    4 \n    3.9 \n    3.1 \n    2.5 \n    2.5 \n    2.7 \n    2.6 \n    4 \n    4.9 \n    3.6 \n    3.2 \n    3.2 \n    3.2 \n    3.2 \n    2.8"
  },
  {
    "objectID": "posts/Public Data 2/index.html#unemployment-rate-of-total-population",
    "href": "posts/Public Data 2/index.html#unemployment-rate-of-total-population",
    "title": "Analysis 2: Unemployment index",
    "section": "- Unemployment rate of total population",
    "text": "- Unemployment rate of total population\n\n\nCode\nunemp_total %>% \n    autoplot() + \n    theme_classic()+\n    theme(text=element_text(family=\"Noto Serif\",size=12)) -> p\n\nggplotly(p) %>% \n    layout(title = list(text = paste0(\n        'Unemployment rate of economically active population (2010~2022)',\n        '<br>',\n        '<sup>',\n        'Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)','</sup>'),\n        x = 0,\n        font=list(size=16)))\n\n\n\n\n\n\n\n- Time-series smoothing\n\n\nCode\n# 3RSSH Twice. 4253H Twice smoothing\nsmooth_3RSSH=function(data){ \n  smooth3RSS=smooth(data, kind=\"3RSS\") \n  n=length(data) \n  smooth3RSSH=smooth3RSS \n  \n  for (i in 2:(n-1)) {\n    smooth3RSSH[i] <- smooth3RSS[i-1]/4 + smooth3RSS[i]/2 + smooth3RSS[i+1]/4}\n    \n    smooth3RSSH[1] <- smooth3RSS[1]; \n    smooth3RSSH[n] <- smooth3RSS[n] \n    rough=data-smooth3RSSH \n    roughH=rough \n  \n    smooth3RSS2=smooth(rough,kind=\"3RSS\") \n  \n  for (i in 2:(n-1)) {\n    roughH[i] <- smooth3RSS2[i-1]/4 + smooth3RSS2[i]/2 + smooth3RSS2[i+1]/4}\n  \n    roughH[1] <- smooth3RSS2[1]; \n    roughH[n] <- smooth3RSS2[n] \n    out=smooth3RSSH+roughH \n    out=as.vector(out) \n    return(out)} \n\n\n\n\nCode\n# Data clensing\nsmooth=tibble(\n    ym(total$PRD_DE),as.numeric(total$DT),smooth_3RSSH(as.numeric(total$DT)),\n    sleek(as.numeric(total$DT)))\n\nnames(smooth)=c(\"year\",\"Default\",\"3RSSH Twice\",\"4253H Twice\")\n\ndatebreaks <- seq(as.Date(\"2010-01-01\"), as.Date(\"2022-12-01\"), by=\"6 month\")\n\n\n\n\nCode\nsmooth %>% \n    tidyr::gather(\"Default\",\n                  \"3RSSH Twice\",\n                  \"4253H Twice\",\n                  key = \"variable\",\n                  value=\"value\") %>%\n    mutate(type=factor(variable,\n                           levels=c(\"Default\",\n                                    \"3RSSH Twice\",\n                                    \"4253H Twice\"))) %>% \n    ggplot(aes(x=year,y=value,color=type, group = 1,\n               text=paste('Smoothing Type:', type,\n                     '<br>Time: ', format(zoo::as.yearmon(year),\"%Y/%m\"),\n                     '<br>Rate (%): ', formattable::percent(value/100)))) +\n    geom_line(aes(x=year,y=value),size=1) +\n    geom_point(size=2)+\n    theme_classic() +\n    labs(color=\"Smoothing Methods\",\n         y=\"Unimployment Index(%)\")+\n    scale_x_date(breaks=datebreaks,labels=date_format(\"%Y/%m\"))+\n    theme(text=element_text(family=\"Noto Serif\",size=12),\n          axis.text.x = element_text(angle=30, hjust=1),\n          axis.title.x=element_blank()) -> p1\n\nggplotly(p1,tooltip=\"text\") %>% \n    layout(title = list(text = paste0(\n        'Unemployment Rate of economically active population with smoothing',\n        '<br>',\n        '<sup>',\n        'Source: Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)','</sup>'),\n        x = 0,\n        font=list(size=16)))"
  },
  {
    "objectID": "posts/Public Data 2/index.html#comparison-of-the-unemployment-rate-among-presidential-regimes",
    "href": "posts/Public Data 2/index.html#comparison-of-the-unemployment-rate-among-presidential-regimes",
    "title": "Analysis 2: Unemployment index",
    "section": "- Comparison of the unemployment rate among presidential regimes",
    "text": "- Comparison of the unemployment rate among presidential regimes\n\n\nCode\nregime=data.frame(president=factor(c(\"Lee Myung-bak\",\"Park Geun-hye\",\"Hwang Kyo-ahn\",\"Moon Jae-in\",\"Yoon Suk-yeol\"),\n                              levels=c(\"Lee Myung-bak\",\"Park Geun-hye\",\"Hwang Kyo-ahn\",\"Moon Jae-in\",\"Yoon Suk-yeol\")),\n             start_date=ymd(c(20100101,20130201,20170301,20170510,20220510)), \n             end_date=ymd(c(20130131,20170228,20170509,20220509,20230701)))\n\nregime %>% \n    kbl(col.names = gsub(\"[_]\", \" \", names(.)) %>% str_to_title()) %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n \n  \n    President \n    Start Date \n    End Date \n  \n \n\n  \n    Lee Myung-bak \n    2010-01-01 \n    2013-01-31 \n  \n  \n    Park Geun-hye \n    2013-02-01 \n    2017-02-28 \n  \n  \n    Hwang Kyo-ahn \n    2017-03-01 \n    2017-05-09 \n  \n  \n    Moon Jae-in \n    2017-05-10 \n    2022-05-09 \n  \n  \n    Yoon Suk-yeol \n    2022-05-10 \n    2023-07-01 \n  \n\n\n\n\n\n\n\nCode\ndatebreaks <- seq(as.Date(\"2010-01-01\"), as.Date(\"2022-12-01\"), by=\"6 month\")\nsmooth %>% \n    tidyr::gather(\"Default\",\n                  \"3RSSH Twice\",\n                  \"4253H Twice\",\n                  key = \"variable\",\n                  value=\"value\") %>% \n    dplyr::filter(variable==\"Default\") %>% \n    ggplot(aes(group=1,text=paste('Time: ', format(zoo::as.yearmon(year),\"%Y/%m\"),\n                     '<br>Rate (%): ', formattable::percent(value/100))))+\n    geom_rect(data=regime,aes(xmin=start_date,\n                              xmax=end_date, \n                              ymin=2,\n                              ymax=6, \n                              fill=president,text=\"\"), alpha=0.2)+\n    geom_line(aes(x=year,y=value),color=\"black\",size=1)+\n    geom_point(aes(x=year,y=value),color=\"black\",size=1.5)+\n    theme_classic() +\n    labs(fill=\"Presidential Regime\",\n         y=\"Unimployment Index(%)\")+\n    scale_x_date(breaks=datebreaks,labels=date_format(\"%Y/%m\"))+\n    theme(text=element_text(family=\"Noto Serif\",size=12),\n          axis.text.x = element_text(angle=30, hjust=1),\n          axis.title.x=element_blank())+\n    geom_smooth(aes(x=year,y=value),\n                method='loess',\n                color=\"orange\",\n                linetype=\"dashed\",\n                fill = \"orange\",\n                alpha=0.1) -> p2\n\n\nWarning: Ignoring unknown aesthetics: text\n\n\nCode\nggplotly(p2,tooltip=c(\"text\") )%>% \n    layout(title = list(text = paste0(\n        'Unemployment Rate of economically active population with smoothing',\n        '<br>',\n        '<sup>',\n        'Source: Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)','</sup>'),\n        x = 0,\n        font=list(size=16)))\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/Public Data 2/index.html#comparison-of-the-unemployment-rate-between-male-and-female",
    "href": "posts/Public Data 2/index.html#comparison-of-the-unemployment-rate-between-male-and-female",
    "title": "Analysis 2: Unemployment index",
    "section": "- Comparison of the unemployment rate between male and female",
    "text": "- Comparison of the unemployment rate between male and female\n\n\nCode\nunemp=data.frame(year=ym(total$PRD_DE),\n                 total_unemp=total$DT,\n                 male_unemp=male$DT,\n                 female_unemp=female$DT)\n\nunemp %>% \n    mutate_if(is.character,as.numeric) %>%\n    mutate(month=format(as.Date(year,format=\"%Y%m\"), \"%b\")) -> unemp\nunemp$month=ordered(month.abb[as.numeric(unemp$month)],levels=month.abb)\n\n\nunemp %>% \n    select(-year) %>% \n    gather(\"variable\",\"value\",1:3) %>% \n    group_by(month,variable,.add = TRUE) %>% \n    mutate(Mean_value= mean(value),Min_value=min(value),Max_value=max(value))%>% \n    mutate(variable=factor(variable,\n                           levels=c(\"total_unemp\",\"male_unemp\",\"female_unemp\"))) %>%\n    ggplot(aes(x=month,y=Mean_value,fill=variable))+ \n    facet_wrap(~variable,\n               labeller = labeller(variable = c(\"total_unemp\" = \"Total Unemployment\",   \n                                                \"male_unemp\" = \"Male Unemployment\",\n                                            \"female_unemp\" = \"Female Unemployment\")))+\n    theme_classic()+\n    geom_bar(stat = \"identity\", position = \"dodge\")+\n    scale_fill_manual(labels=c(\"Total Unemployment\", \n                               \"Male Unemployment\",\n                               \"Female Unemployment\"),\n                      name = \"Genders\",values =c(\"gray\",\"#619CFF\",\"#F8766D\"))+\n    labs(y=\"Unemployment Rate\\n\",\n        x=\"\\n\")+\n    geom_errorbar(aes(ymin = Min_value, \n                      ymax = Max_value), \n                      width = 0.05, position = position_dodge(0.9))+\n    theme(text=element_text(family=\"Noto Serif\",size=12),\n          axis.text.x = element_text(angle=45, hjust=1),\n          axis.title.x=element_blank())+\n    coord_cartesian(ylim=c(1.5,7))+\n    geom_text(aes(label= paste(sprintf(\"%2.2f\", Mean_value),\"%\",sep=\"\")),\n              vjust=-0.4, size=3,position = position_dodge(width = 1)) -> p3\n    \n\nmrg <- list(l = 100, r = 50 ,\n          b = 30, t = 150)\n\nggplotly(p3, tooltip=\"text\")%>% \n    style(textposition = \"top\") %>% \n    layout(title = list(text = paste0(\n        'Unemployment Rate of economically active population',\n        '<br>',\n        '<sup>',\n        'Source: Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)<br>',\n        'Error Bar: Min-Max Rate Per Month',\n        '</sup>'),\n        x = 0.05,\n        font=list(size=16)),\n        margin = mrg) \n\n\n\n\n\n\n\n\nCode\nunemp=data.frame(year=ym(total$PRD_DE),\n                 total_unemp=total$DT,\n                 male_unemp=male$DT,\n                 female_unemp=female$DT)\n\nunemp %>% \n    mutate_if(is.character,as.numeric) %>%\n    mutate(month=format(as.Date(year,format=\"%Y%m\"), \"%b\")) -> unemp\nunemp$month=ordered(month.abb[as.numeric(unemp$month)],levels=month.abb)\n\n\nunemp %>% \n    select(-year) %>% \n    gather(\"variable\",\"value\",1:3) %>% \n    group_by(month,variable,.add = TRUE) %>% \n    mutate(Mean_value= mean(value),Min_value=min(value),Max_value=max(value))%>% \n    mutate(variable=factor(variable,\n                           levels=c(\"total_unemp\",\"male_unemp\",\"female_unemp\"))) %>%\n    ggplot(aes(x=month,y=Mean_value,fill=variable))+ \n    theme_classic()+\n    geom_bar(stat = \"identity\", position = \"dodge\")+\n    scale_fill_manual(labels=c(\"Total Unemployment\", \n                               \"Male Unemployment\",\n                               \"Female Unemployment\"),\n                      name = \"Genders\",values =c(\"gray\",\"#619CFF\",\"#F8766D\"))+\n    labs(y=\"Unemployment Rate\\n\")+\n    geom_errorbar(aes(ymin = Min_value, \n                      ymax = Max_value), \n                      width = 0.05, position = position_dodge(0.9))+\n    theme(text=element_text(family=\"Noto Serif\",size=12),\n          axis.text.x = element_text(angle=45, hjust=1),\n          axis.title.x=element_blank())+\n    coord_cartesian(ylim=c(1.5,7)) -> p4\n    \n\nmrg <- list(l = 100, r = 50 ,\n          b = 30, t = 150)\n\nggplotly(p4, tooltip=c(\"value\",\"year\"))%>% \n    style(textposition = \"top\") %>% \n    layout(title = list(text = paste0(\n        'Unemployment Rate of economically active population',\n        '<br>',\n        '<sup>',\n        'Source: Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)<br>',\n        'Error Bar: Min-Max Rate Per Month',\n        '</sup>'),\n        x = 0.05,\n        font=list(size=16)),\n        margin = mrg)"
  },
  {
    "objectID": "posts/Public Data 2/index.html#seasonal-decomposition-of-the-unemployment-rate",
    "href": "posts/Public Data 2/index.html#seasonal-decomposition-of-the-unemployment-rate",
    "title": "Analysis 2: Unemployment index",
    "section": "- Seasonal decomposition of the unemployment rate",
    "text": "- Seasonal decomposition of the unemployment rate\n\n\nCode\nunemp_total.decompose=decompose(unemp_total)\nunemp_male.decompose=decompose(unemp_male)\nunemp_female.decompose=decompose(unemp_female)\n\n\n\n\nCode\n# Total\nplot(unemp_total.decompose,cex.main=2, cex.lab=2, cex.axis=2)\n\n\n\n\n\n\n\n\n\nCode\n# Male\nplot(unemp_male.decompose,cex.main=2, cex.lab=2, cex.axis=2)\n\n\n\n\n\n\n\n\n\nCode\n# Female\nplot(unemp_female.decompose,cex.main=2, cex.lab=2, cex.axis=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(unemp_total.decompose$seasonal,main=\"The Plot of Seasoal Decomposition\",cex.main=2, cex.lab=2, cex.axis=2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Box-Cox Transformation(To visualize seasonality and to improve normality)\nlambda <- forecast::BoxCox.lambda(unemp_total)\nunemp_total_bc <- forecast::BoxCox(unemp_total, lambda)\n\n# Boxcox transforamtion is only applicable when data only contain positive value.\nplot(unemp_total_bc, main = \"Box-Cox : Unemployment Rate\",,cex.main=2, cex.lab=2, cex.axis=2)\n\n\n\n\n\n\n\n\n\n\n- Visualize seasonality and comparing trends per year\n\n\nCode\nunemp %>%\n    mutate(time=year,\n        year=factor(format(as.Date(year), \"%Y\"))) -> unemp2\n\n\n\n\nCode\nunemp2 %>% \n    select(time,month,year,total_unemp,male_unemp,female_unemp) %>%\n    gather(\"variable\",\"value\", 4:6) %>% \n    mutate(variable=factor(variable,levels= c(\"total_unemp\",\n                                              \"male_unemp\",\n                                          \"female_unemp\"))) %>% \n    ggplot(aes(x=as.factor(month),\n               y=value,\n               group=year,\n               color=as.factor(year),\n               text=paste('Time: ', format(zoo::as.yearmon(time),\"%Y/%m\"),\n                     '<br>Rate (%): ', formattable::percent(value/100),\n                     '<br>Gender: ',variable))) +\n    geom_line() + \n    geom_point()+\n    facet_wrap(~variable,\n               labeller = labeller(variable = \n                                       c(\"total_unemp\" = \"Total Unemployment\",   \n                                         \"male_unemp\" = \"Male Unemployment\",\n                                         \"female_unemp\" = \"Female Unemployment\")))+\n    theme_classic()+\n    scale_fill_manual(labels=c(\"Total Unemployment\", \n                               \"Male Unemployment\",\n                               \"Female Unemployment\"),\n                      name = \"Genders\",values =c(\"gray\",\"#619CFF\",\"#F8766D\"))+\n    labs(y=\"Unemployment Rate\\n\",color = \"Year\")+\n    theme(text=element_text(family=\"Noto Serif\",size=12),\n          axis.text.x = element_text(angle=45, hjust=1),\n          axis.title.x=element_blank()) +\n    coord_cartesian(ylim=c(1.5,7)) -> p5\n    \n\nmrg <- list(l = 100, r = 50 ,\n          b = 30, t = 100)\n\ngp<- ggplotly(p5, tooltip=\"text\")%>% \n    style(textposition = \"top\") %>% \n    layout(title = list(text = paste0(\n        'Unemployment Rate of economically active population',\n        '<br>',\n        '<sup>',\n        'Source: Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)',\n        '</sup>'),\n        x = 0.05,\n        font=list(size=16)),\n        margin = mrg) \n\nfor (i in seq_along(gp$x$data)) {\n  # Is the layer the first entry of the group?\n  is_first <- grepl(\"^\\\\(.*?,1\\\\)\", gp$x$data[[i]]$name)\n  # Extract the group identifier and assign it to the name and legendgroup arguments\n  gp$x$data[[i]]$name <- gsub(\"^\\\\((.*?),\\\\d+\\\\)\", \"\\\\1\", gp$x$data[[i]]$name)\n  gp$x$data[[i]]$legendgroup <- gp$x$data[[i]]$name\n  # Show the legend only for the first layer of the group \n  if (!is_first) gp$x$data[[i]]$showlegend <- FALSE\n}\n\ngp\n\n\n\n\n\n\n\n\nCode\nunemp2 %>% \n    select(time,month,year,total_unemp,male_unemp,female_unemp) %>%\n    gather(\"variable\",\"value\", 4:6) %>% \n    mutate(variable=factor(variable,levels= c(\"total_unemp\",\n                                              \"male_unemp\",\n                                          \"female_unemp\"))) %>% \n    ggplot(aes(x=as.factor(month),\n               y=sleek(value),\n               group=year,\n               color=as.factor(year),\n               text=paste('Time: ', format(zoo::as.yearmon(time),\"%Y/%m\"),\n                     '<br>Rate (%): ', formattable::percent(value/100),\n                     '<br>Gender: ',variable))) +\n    geom_line() + \n    geom_point()+\n    facet_wrap(~variable,\n               labeller = labeller(variable = \n                                       c(\"total_unemp\" = \"Total Unemployment\",   \n                                         \"male_unemp\" = \"Male Unemployment\",\n                                         \"female_unemp\" = \"Female Unemployment\")))+\n    theme_classic()+\n    scale_fill_manual(labels=c(\"Total Unemployment\", \n                               \"Male Unemployment\",\n                               \"Female Unemployment\"),\n                      name = \"Genders\",values =c(\"gray\",\"#619CFF\",\"#F8766D\"))+\n    labs(y=\"Unemployment Rate\\n\",color = \"Year\")+\n    theme(text=element_text(family=\"Noto Serif\",size=12),\n          axis.text.x = element_text(angle=45, hjust=1),\n          axis.title.x=element_blank()) +\n    coord_cartesian(ylim=c(1.5,7)) -> p6\n    \n\nmrg <- list(l = 100, r = 50 ,\n          b = 30, t = 100)\n\ngp1<- ggplotly(p6, tooltip=\"text\")%>% \n    style(textposition = \"top\") %>% \n    layout(title = list(text = paste0(\n        'Unemployment Rate of economically active population with 4253H Twice smoothing',\n        '<br>',\n        '<sup>',\n        'Source: Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)',\n        '</sup>'),\n        x = 0.05,\n        font=list(size=16)),\n        margin = mrg) \n\nfor (i in seq_along(gp1$x$data)) {\n  # Is the layer the first entry of the group?\n  is_first <- grepl(\"^\\\\(.*?,1\\\\)\", gp1$x$data[[i]]$name)\n  # Extract the group identifier and assign it to the name and legendgroup arguments\n  gp1$x$data[[i]]$name <- gsub(\"^\\\\((.*?),\\\\d+\\\\)\", \"\\\\1\", gp1$x$data[[i]]$name)\n  gp1$x$data[[i]]$legendgroup <- gp1$x$data[[i]]$name\n  # Show the legend only for the first layer of the group \n  if (!is_first) gp1$x$data[[i]]$showlegend <- FALSE\n}\n\n\ngp1"
  },
  {
    "objectID": "posts/Public Data 2/index.html#predicting-next-years-unemployment-rate-using-arima-algorithm.",
    "href": "posts/Public Data 2/index.html#predicting-next-years-unemployment-rate-using-arima-algorithm.",
    "title": "Analysis 2: Unemployment index",
    "section": "- Predicting next year’s unemployment rate using ARIMA algorithm.",
    "text": "- Predicting next year’s unemployment rate using ARIMA algorithm.\n\n\nCode\n# ARIMA\naTSA::adf.test(unemp_total, nlag = NULL, output = TRUE) \n\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -0.770   0.403\n[2,]   1 -0.769   0.403\n[3,]   2 -0.778   0.400\n[4,]   3 -0.709   0.425\n[5,]   4 -0.840   0.378\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -4.74    0.01\n[2,]   1 -5.76    0.01\n[3,]   2 -5.52    0.01\n[4,]   3 -4.80    0.01\n[5,]   4 -5.17    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -4.79    0.01\n[2,]   1 -5.85    0.01\n[3,]   2 -5.72    0.01\n[4,]   3 -5.04    0.01\n[5,]   4 -5.69    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nCode\n# p-value<=0.01 Reject null-hypothesis. (Stationary)\n\n\n\n\nCode\nfit=auto.arima(unemp_total,trace=T)\n\n\n\n ARIMA(2,1,2)(1,1,1)[12]                    : 39.27135\n ARIMA(0,1,0)(0,1,0)[12]                    : 70.98412\n ARIMA(1,1,0)(1,1,0)[12]                    : 40.28337\n ARIMA(0,1,1)(0,1,1)[12]                    : 41.94204\n ARIMA(2,1,2)(0,1,1)[12]                    : 44.64906\n ARIMA(2,1,2)(1,1,0)[12]                    : 38.27577\n ARIMA(2,1,2)(0,1,0)[12]                    : 64.85645\n ARIMA(2,1,2)(2,1,0)[12]                    : 39.29379\n ARIMA(2,1,2)(2,1,1)[12]                    : 41.50791\n ARIMA(1,1,2)(1,1,0)[12]                    : 37.939\n ARIMA(1,1,2)(0,1,0)[12]                    : Inf\n ARIMA(1,1,2)(2,1,0)[12]                    : 38.74243\n ARIMA(1,1,2)(1,1,1)[12]                    : 38.71915\n ARIMA(1,1,2)(0,1,1)[12]                    : 43.72291\n ARIMA(1,1,2)(2,1,1)[12]                    : 40.90068\n ARIMA(0,1,2)(1,1,0)[12]                    : 36.58976\n ARIMA(0,1,2)(0,1,0)[12]                    : 60.81371\n ARIMA(0,1,2)(2,1,0)[12]                    : 37.46671\n ARIMA(0,1,2)(1,1,1)[12]                    : 37.47204\n ARIMA(0,1,2)(0,1,1)[12]                    : 42.11681\n ARIMA(0,1,2)(2,1,1)[12]                    : 39.61018\n ARIMA(0,1,1)(1,1,0)[12]                    : 36.29592\n ARIMA(0,1,1)(0,1,0)[12]                    : 59.53748\n ARIMA(0,1,1)(2,1,0)[12]                    : 36.66919\n ARIMA(0,1,1)(1,1,1)[12]                    : 36.76048\n ARIMA(0,1,1)(2,1,1)[12]                    : 38.77932\n ARIMA(0,1,0)(1,1,0)[12]                    : 49.85337\n ARIMA(1,1,1)(1,1,0)[12]                    : 35.85001\n ARIMA(1,1,1)(0,1,0)[12]                    : 60.64694\n ARIMA(1,1,1)(2,1,0)[12]                    : 36.72877\n ARIMA(1,1,1)(1,1,1)[12]                    : 36.70104\n ARIMA(1,1,1)(0,1,1)[12]                    : 41.57715\n ARIMA(1,1,1)(2,1,1)[12]                    : 38.85781\n ARIMA(2,1,1)(1,1,0)[12]                    : 37.87742\n ARIMA(2,1,0)(1,1,0)[12]                    : 40.68781\n\n Best model: ARIMA(1,1,1)(1,1,0)[12]                    \n\n\n\n\nCode\nforecast::forecast(fit, level=c(75, 95), h=12) %>% \n    autoplot() + \n    theme_classic()+\n    theme(text=element_text(family=\"Noto Serif\",size=12)) -> p7\n\nggplotly(p7) %>% \n    layout(title = list(text = paste0(\n        'Prediction of unemployment rate of economically active population (for next 12 months)',\n        '<br>',\n        '<sup>',\n        'Korean Population and Housing Census (retrieved from KOSIS) (Jan, 2010 ~ June, 2022)','</sup>'),\n        x = 0,\n        font=list(size=16)))"
  },
  {
    "objectID": "posts/Public Data 3/FWA.html",
    "href": "posts/Public Data 3/FWA.html",
    "title": "Analysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]",
    "section": "",
    "text": "Sophisticated policy design is required for the public sector flexible work arrangement(FWA) to achieve the original purpose of work-family balance. In addition, the use of FWA due to external pressure (such as department evaluation of penalizing for not choosing to use FWA) rather reduces the adoption rate in a long-term perspective (i.e. stigmatization or psychological issue). Therefore, it is necessary to design a meticulous personnel system that facilitates the adoption through both psychological and material incentives."
  },
  {
    "objectID": "posts/Public Data 3/FWA.html#텍스트-마이닝을-활용한-빅데이터-텍스트-분석",
    "href": "posts/Public Data 3/FWA.html#텍스트-마이닝을-활용한-빅데이터-텍스트-분석",
    "title": "Analysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]",
    "section": "- 텍스트 마이닝을 활용한 빅데이터 텍스트 분석",
    "text": "- 텍스트 마이닝을 활용한 빅데이터 텍스트 분석\n\n기술통계 분석\n1. 빈도표 계산 및 말뭉치 등을 활용한 시각화 단어가 몇번 출현했는 지수(Count)를 기반으로 하여 텍스트를 표현 - BoW: 단어의 순서를 고려하지 않고 단어 출현 빈도에만 집중 - DTM (Document-Term Matrix): 문서단어행렬, 다수의 문서에서 등장하는 단어들의 빈도를 행렬로 표현하는 방식 - TF-IDF (Term Frequency - Inverse Document Frequency): 단어 빈도 - 역 문서 빈도  총 문서 수 대비 적게 등장한 단어가 중요한 단어  특정 문서 안에서 많이 등장한다고 해도 중요도가 올라가지는 않음  > TF: 특정 문서 D에서 특정 단어 T의 등장 횟수  DF: 특정 단어 T가 등장한 문서의 수  IDF: DF에 반비례 하는 수. ln(총문서수/(DF))  TF-IDF: TF * IDF 희귀하면서도 특정 텍스트에서 자주 사용된 단어 (TF)는 그 텍스트에서 중요함 2. 단어간 혹은 기사간 상관관계 - 동시 출현(Co-Occurrence) 단어 분석 문장 혹은 기사에 함께 사용된 단어는 어떤 단어일지 분석하는 것. 단어의 “맥락”을 파악하기 위하여 어떤 단어들이 함께 쓰였는지를 알아야 함. 의미를 가진 단어(명사, 동사, 형용사)등을 추출하여 어떤 단어들이 함께 빈번하게 쓰였는지 분석해보는게 필요함. 3. 연관 규칙 분석 (Association Rules) - 장바구니 분석 Apriori Algorithm(Agrawal et al., 1993): 어떤 단어가 다른 단어들과의 연관규칙을 가지는지를 추출 하는 방식\n\n\n토픽 모델링\n토픽 모델링은 문서와 단어로 구성된 행렬(DTM)을 기반으로 문서에 잠재되어 (Latent)있다고 가정된 토픽의 등장확률을 추정하는 일련의 통계적 텍스트 처리기법을 일컫는다. (Blei, 2014; Blei and Lafferty, 2007;Blei, Ng and Jordan, 2003) DTM을 활용하여 주제-확률 분포, 단어-확률 분포를 구한뒤 잠재 주제를 찾는 LDA나 Singular Value Decomposition을 통해 차원 축소를 하는 방법이 있다.\n\n\nLDA(Latent Dirichlet Allocation): 이 문서에서는 어떤 주제들이 오가고 있을까?  PLSA를 조건부 확률로 확장시킨 기법으로 잠재 주제의 확률적 분포에 대한 PLSA의 한계점을 보완한 모델이다. LDA모델은 무작위로 섞여있는 대량의 문서에서 단어들의 패턴을 추론하여 각 토픽의 특성을 도출하는데 용이하며, 텍스트 데이터의 의미구조를 파악하기에 적합한 방법 중 하나이다.  한 문서는 여러가지 토픽으로 이루어지고, 토픽은 여러 단어를 혼합하여 구성된다.   1개의 토픽은 여러 단어(서로 다른 확률을 가진)로 구성. 1개의 단어는 여러 토픽에서 서로 다른 확률을 가짐.  delta는 문장이 각 토픽에 등장할 확률, beta는 단어가 각 토픽에 등장할 확률\n\nfyi) 디리클레 분포(Dirichlet distribution):  베타분포를 다변량으로 확장한 것으로 다변량 베타분포라고 볼 수 있음. (K가 2일때 베타분포) LDA 토픽 모델링으로 예를 들면, 한 문서에 대한 토픽의 분포는 k개의 토픽의 확률 (\\(p_k\\))로 표현할 수 있음.  문서에서 각 단어에 대한 토픽을 샘플을 할때 이 토픽의 분포는 Multinomial 분포를 가정하게 됨. dirichlet 분포의 샘플링 된 k차원 벡터는 합이 1을 만족하기 때문에, multinomial 분포의 모수(\\(p_k\\))에 사용될 수 있음 -> 분포의 분포를 표현.\n\\[f(x_1, x_2, \\cdots, x_K) = \\frac{1}{\\mathrm{B}(\\boldsymbol\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1} \\]\n\\[ \\mathrm{B}(\\boldsymbol\\alpha) = \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)} {\\Gamma\\bigl(\\sum_{i=1}^K \\alpha_i\\bigr)}\\] 제한조건: \\[ \\sum_{i=1}^{K} x_i = 1 \\]\nhttps://donghwa-kim.github.io/distributions.html (참고)"
  },
  {
    "objectID": "posts/Public Data 3/FWA.html#자료수집",
    "href": "posts/Public Data 3/FWA.html#자료수집",
    "title": "Analysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]",
    "section": "자료수집 ",
    "text": "자료수집 \n우리나라 COVID-19 최초 발생월인 2020년 2월부터 2021년 12월까지의 ‘유연근무’혹은 ’탄력근무’ 혹은 ’원격근무’를 포함하고 있는 정치부, 사회부 언론기사 제목 11,334건을 한국언론진흥재단(BIGKINDS)에서 수집. (경제부 기사들은 특정기업이 유연근무를 사용하기 시작하였음을 홍보하는 기사들이 많아 제외하였음)\n\na. 데이터 불러오기\n\n\nCode\nworksheet = gc.open('NewsResult_20200201-20211202 (1)').sheet1\nrows = worksheet.get_all_values()\n\n\n\n\nCode\nbigkinds=pd.DataFrame.from_records(rows)\nbigkinds.columns=bigkinds.iloc[0]\nbigkinds=bigkinds.drop(bigkinds.index[0])\n\n\n\n\nCode\nimport pandas_profiling\nbigkinds.profile_report()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb. 기술 통계 분석\n\n시기 별 기사의 수\n\n\n\nCode\nfrom datetime import datetime \nnumber=bigkinds >> group_by(X.일자) >> summarize(count=n(X.일자))\nnumber.head(10)\n\n\n\n\n\n\n  \n    \n      \n      일자\n      count\n    \n  \n  \n    \n      0\n      20200201\n      1\n    \n    \n      1\n      20200202\n      1\n    \n    \n      2\n      20200203\n      5\n    \n    \n      3\n      20200204\n      8\n    \n    \n      4\n      20200205\n      7\n    \n    \n      5\n      20200206\n      48\n    \n    \n      6\n      20200207\n      15\n    \n    \n      7\n      20200208\n      2\n    \n    \n      8\n      20200209\n      3\n    \n    \n      9\n      20200210\n      3\n    \n  \n\n\n\n\n\n\nCode\n# list 내포\nkdate = [ datetime.strptime(d, '%Y%m%d') for d in number[\"일자\"] ]\n\nhead(kdate)\nnumber['date'] = kdate\n\n\n\n\nCode\nkdate1 = [datetime.strftime(d, '%Y%m') for d in number[\"date\"] ]\n\nnumber['date1'] = kdate1\nnumber.head(10)\nnumber2=number[number.일자.astype(\"int64\")<20211131] >> select([\"date1\",\"count\"])  >> rename(num=\"count\") >> group_by('date1') >> summarise(num=X.num.sum())\n\n\n\n\nCode\nfig=plt.figure(figsize = (10,7))\n\nplt.plot(number[\"date\"], number[\"count\"],color='blue', label=str(\"기사\"))\nplt.title(\"일별 유연근무제 관련 기사 수\",fontsize=20)\nplt.style.use(\"default\")\nplt.rc('font', family='NanumBarunGothic') \n\nplt.legend(fontsize=20)\nplt.xticks(rotation=75,fontsize=10)\nplt.yticks(fontsize=10)\n\nax = fig.add_subplot(1, 1, 1)\nax.set_xlabel(\"일별\", fontsize=\"15\")\nax.set_ylabel(\"기사 건수\", fontsize=\"15\")\nplt.show()\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n  if sys.path[0] == '':\n\n\n\n\n\n\n\nCode\nfig2=plt.figure(figsize = (10,7))\nax = fig2.add_subplot(1, 1, 1)\nax.set_xlabel(\"기사 건수\", fontsize=\"15\")\nax.set_ylabel(\"월별\", fontsize=\"15\")\n\nplt.plot(number2[\"date1\"], number2[\"num\"],color='red', label=str(\"기사\"))\nplt.title(\"월별 유연근무제 관련 기사 수\",fontsize=20)\nplt.style.use(\"default\")\nplt.rc('font', family='NanumBarunGothic') \n\nplt.legend(fontsize=20)\nplt.xticks(rotation=75,fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()\n\n\n\n\n\n\n텍스트 전처리\n\n\n\nCode\n# 토큰화\narticle=bigkinds >> select(\"제목\") >> rename(title=\"제목\")\nprint(article[\"title\"].head(10))\narticle[\"title\"]=article[\"title\"].astype(str)\narticle.shape #11334개의 기사\n\n\n1                                     동화약품, 가족친화 기업 재인증\n2               집단감염 취약한 콜센터 찾은 안경덕 고용부 장관 \"방역수칙 준수\" 당부\n3                  환풍기 타고 확산? 전파력 5배 오미크론, 재택치료 빈틈 파고드나\n4                                 [팀장시각]서로 위로하는 코로나 분투기\n5                        새 판 짜는 완성차 노조 MZ세대 품을까 [비즈360]\n6                           “재택치료? 가족들 번갈아가며 확진되란 건가요?”\n7                    5000명대 확진에 자영업자들 “다시 거리두기 격상되나” 한숨\n8                   전면등교 후 인천 학생 자가격리자 5천명 폭등 돌봄공백 '비상'\n9     \"가구를 통해 건강한 세상 만들겠다\" 유해물질 소음 진동 걱정없는 책상 [환경표지 ...\n10               김인호 서울시의회 의장, 코로나 확산세 관련 서울시에 적극 대응 요청\nName: title, dtype: object\n\n\n(11334, 1)\n\n\n\n\nCode\ndef text_preprocess(text):\n    \"\"\"\n    텍스트 전처리\n    1. span tag 삭재\n    2. br tag 삭제\n    3. 영어, 한글, 숫자, 온점 제외 삭제\n    4. 온점을 구분으로 문장 구분\n    \"\"\"\n    text = re.sub(\"(<span class='quot[0-9]'>|</span>|<br/>|<br />|([^0-9가-힣A-Za-z. ]))\",\"\",text)\n    return [sen.strip() for sen in text.split('.') if sen.strip()]\n\n\n\n\nCode\ndf = pd.DataFrame(index=np.arange(1,article.shape[0]+1), columns=['title'])\n\n\n\n\nCode\n# 정규표현식으로 불필요한 부분 제거\nfor i in np.arange(1,article.shape[0]+1):\n  df[\"title\"][i]=text_preprocess(article[\"title\"][i])\n\n\n\n\nCode\ndf\n\n\n\n\n\n\n  \n    \n      \n      title\n    \n  \n  \n    \n      1\n      [동화약품 가족친화 기업 재인증]\n    \n    \n      2\n      [집단감염 취약한 콜센터 찾은 안경덕 고용부 장관 방역수칙 준수 당부]\n    \n    \n      3\n      [환풍기 타고 확산 전파력 5배 오미크론 재택치료 빈틈 파고드나]\n    \n    \n      4\n      [팀장시각서로 위로하는 코로나 분투기]\n    \n    \n      5\n      [새 판 짜는 완성차 노조 MZ세대 품을까 비즈360]\n    \n    \n      ...\n      ...\n    \n    \n      11330\n      [정부 신종코로나 사망자 치사율 축소 논란 글로벌 대유행 가능성도]\n    \n    \n      11331\n      [신종코로나 확산 사태로 가장 먼저 재택근무 시행하는 업계는]\n    \n    \n      11332\n      [지옥철서 신종코로나 옮길라 문지방 출퇴근 지시한 제약회사]\n    \n    \n      11333\n      [여주시 신종 코로나 선제적 대응위해 보건소 선별진료소 운영]\n    \n    \n      11334\n      [태안군 6번 확진자 접촉 관내 2인 음성 판정]\n    \n  \n\n11334 rows × 1 columns\n\n\n\n\n\nCode\nsentence_arr = []\nfor i in np.arange(1,article.shape[0]):\n    text=df.title[i][0] \n    sentence_arr.insert(0,text)\n\n\n\n\nCode\npprint.pprint(sentence_arr[1:10])\n\n\n['지옥철서 신종코로나 옮길라 문지방 출퇴근 지시한 제약회사',\n '신종코로나 확산 사태로 가장 먼저 재택근무 시행하는 업계는',\n '정부 신종코로나 사망자 치사율 축소 논란 글로벌 대유행 가능성도',\n '경찰인재개발원 필수 인력만 남고 근무 장소 변경 허용',\n '격리 시설 직원들 하소연 유치원서 자녀 보내지 말라네요',\n '오늘은 이런 경향2월4일 재탕 후퇴 미흡  20대 총선보다 부실한 주거공약',\n '신종 코로나 16번째 확진자 발생 태국 여행한 42세 한국인 여성',\n '신종 코로나 확진 1명 추가 태국 여행 뒤 증상 발현',\n '우한 폐렴 비상오락가락하는 정부 기준 자가격리도 잇단 혼선']\n\n\n\n\nCode\ncheck = ['탄력', '재택', '유연']\nmatching1 = [s for s in sentence_arr if \"유연\" in s] \nmatching2 = [s for s in sentence_arr if \"재택\" in s] \nmatching3 = [s for s in sentence_arr if \"탄력\" in s] \nmatching =matching1 + matching2 + matching3 \nmatching=list(set(matching))\n\n\n\n\nCode\nprint(len(matching))\ntype(matching)\n# 1226개의 기사 제목\n\n\n1226\n\n\nlist\n\n\n한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)를 수행하게 됨을 뜻한다. 여기선 이 중에서 Okt와 꼬꼬마를 통해서 토큰화를 수행한다.  참고: https://wikidocs.net/21698\n\n\nCode\nfrom konlpy.tag import *\n\n\n\n\nCode\nkkma = Kkma()  \nokt = Okt()  \n\n\n\nOKT\n\n\n\nCode\nokt_list=[]\nfor title in matching:\n  tokenized3=okt.pos(title)\n  okt_list.insert(len(okt_list)+1,tokenized3)\n\n\n\n\nCode\nokt_list[:3]\n\n\n[[('코로나', 'Noun'),\n  ('19', 'Number'),\n  ('우려', 'Noun'),\n  ('에', 'Josa'),\n  ('유엔', 'Noun'),\n  ('본부', 'Noun'),\n  ('직원', 'Noun'),\n  ('3000', 'Number'),\n  ('명', 'Noun'),\n  ('3', 'Number'),\n  ('주간', 'Noun'),\n  ('재택근무', 'Noun'),\n  ('돌입', 'Noun')],\n [('재택근무', 'Noun'),\n  ('중', 'Noun'),\n  ('아이', 'Noun'),\n  ('도', 'Josa'),\n  ('보육', 'Noun'),\n  ('하', 'Suffix'),\n  ('라', 'Josa'),\n  ('고요', 'Noun'),\n  ('긴급', 'Noun'),\n  ('보육', 'Noun'),\n  ('청원', 'Noun'),\n  ('갑론', 'Noun'),\n  ('을', 'Josa'),\n  ('박', 'Noun')],\n [('KBS', 'Alpha'),\n  ('본관', 'Noun'),\n  ('3', 'Number'),\n  ('층', 'Noun'),\n  ('직원', 'Noun'),\n  ('1', 'Number'),\n  ('명', 'Noun'),\n  ('확진', 'Noun'),\n  ('필수', 'Noun'),\n  ('인력', 'Noun'),\n  ('제외', 'Noun'),\n  ('재택근무', 'Noun')]]\n\n\n\n\nCode\nokt_morphs_list=[]\nfor title in matching:\n  tokenized4=okt.morphs(title)\n  okt_morphs_list.insert(len(okt_morphs_list)+1,tokenized4)\n\n\n\n\nCode\nokt_morphs_list[:3]\n\n\n[['코로나',\n  '19',\n  '우려',\n  '에',\n  '유엔',\n  '본부',\n  '직원',\n  '3000',\n  '명',\n  '3',\n  '주간',\n  '재택근무',\n  '돌입'],\n ['재택근무',\n  '중',\n  '아이',\n  '도',\n  '보육',\n  '하',\n  '라',\n  '고요',\n  '긴급',\n  '보육',\n  '청원',\n  '갑론',\n  '을',\n  '박'],\n ['KBS', '본관', '3', '층', '직원', '1', '명', '확진', '필수', '인력', '제외', '재택근무']]\n\n\n\n꼬꼬마\n\n\n\nCode\nkkma_list=[]\nfor title in matching:\n  tokenized=kkma.pos(title)\n  kkma_list.insert(len(kkma_list)+1,tokenized)\n\n\n\n\nCode\nkkma_list[:5]\n\n\n[[('코로나', 'NNG'),\n  ('19', 'NR'),\n  ('우려', 'NNG'),\n  ('에', 'JKM'),\n  ('유엔', 'NNG'),\n  ('본부', 'NNG'),\n  ('직원', 'NNG'),\n  ('3000', 'NR'),\n  ('명', 'NNM'),\n  ('3', 'NR'),\n  ('주간', 'NNG'),\n  ('재택근무', 'NNG'),\n  ('돌입', 'NNG')],\n [('재택근무', 'NNG'),\n  ('중', 'NNB'),\n  ('아이', 'NNG'),\n  ('도', 'JX'),\n  ('보육', 'NNG'),\n  ('하', 'XSV'),\n  ('라', 'ECD'),\n  ('고요', 'NNG'),\n  ('긴급', 'NNG'),\n  ('보육', 'NNG'),\n  ('청원', 'NNG'),\n  ('갑론을박', 'NNG')],\n [('KBS', 'OL'),\n  ('본관', 'NNG'),\n  ('3', 'NR'),\n  ('층', 'NNG'),\n  ('직원', 'NNG'),\n  ('1', 'NR'),\n  ('명', 'NNM'),\n  ('확', 'MAG'),\n  ('질', 'VV'),\n  ('ㄴ', 'ETD'),\n  ('필수', 'NNG'),\n  ('인력', 'NNG'),\n  ('제외', 'NNG'),\n  ('재택근무', 'NNG')],\n [('서울', 'NNG'),\n  ('LS', 'OL'),\n  ('용', 'NNG'),\n  ('산', 'NNG'),\n  ('타워', 'NNG'),\n  ('직장인', 'NNG'),\n  ('확', 'MAG'),\n  ('진', 'NNG'),\n  ('판정', 'NNG'),\n  ('임직원', 'NNG'),\n  ('재택근무', 'NNG')],\n [('코로나', 'NNG'),\n  ('시대', 'NNG'),\n  ('재택근무', 'NNG'),\n  ('개인', 'NNG'),\n  ('시간', 'NNG'),\n  ('늘', 'VV'),\n  ('었', 'EPT'),\n  ('지만', 'ECE'),\n  ('일', 'NNG'),\n  ('생활', 'NNG'),\n  ('분리', 'NNG'),\n  ('어렵', 'VV'),\n  ('어', 'ECS')]]\n\n\n\n\nCode\nkkma_morphs_list=[]\nfor title in matching:\n  tokenized1=kkma.morphs(title)\n  kkma_morphs_list.insert(len(kkma_morphs_list)+1,tokenized1)\n\n\n\n\nCode\nkkma_morphs_list[:5]\n\n\n[['코로나',\n  '19',\n  '우려',\n  '에',\n  '유엔',\n  '본부',\n  '직원',\n  '3000',\n  '명',\n  '3',\n  '주간',\n  '재택근무',\n  '돌입'],\n ['재택근무', '중', '아이', '도', '보육', '하', '라', '고요', '긴급', '보육', '청원', '갑론을박'],\n ['KBS',\n  '본관',\n  '3',\n  '층',\n  '직원',\n  '1',\n  '명',\n  '확',\n  '질',\n  'ㄴ',\n  '필수',\n  '인력',\n  '제외',\n  '재택근무'],\n ['서울', 'LS', '용', '산', '타워', '직장인', '확', '진', '판정', '임직원', '재택근무'],\n ['코로나', '시대', '재택근무', '개인', '시간', '늘', '었', '지만', '일', '생활', '분리', '어렵', '어']]\n\n\n딱히 차이가 없으므로 OKT로만 분석 진행.|\n\n불용어 제거  불용어 리스트: https://www.ranks.nl/stopwords/korean\n\n\n\nCode\nworksheet2 = gc.open('stopwords').sheet1\nrows2 = worksheet2.get_all_values()\nstopwords=pd.DataFrame.from_records(rows2)\n\n\n\n\nCode\nstopwords=pd.Series.tolist(stopwords[0])\n\n\n\n\nCode\nokt_morphs_list_stop=[]\nfor words in okt_morphs_list:\n  tokenized=[]\n  for word in words:\n    if not word in stopwords:\n      tokenized.insert(len(tokenized)+1,word)\n  okt_morphs_list_stop.insert(len(okt_morphs_list_stop)+1,tokenized)\n\n\n\n\nCode\nokt_morphs_list_stop[:5]\n\n\n[['코로나', '19', '우려', '유엔', '본부', '직원', '3000', '명', '3', '주간', '재택근무', '돌입'],\n ['재택근무', '중', '도', '보육', '라', '고요', '긴급', '보육', '청원', '갑론', '박'],\n ['KBS', '본관', '3', '층', '직원', '1', '명', '확진', '필수', '인력', '제외', '재택근무'],\n ['서울', 'LS', '용산', '타워', '직장인', '확진', '판정', '임', '직원', '재택근무'],\n ['코로나', '시대', '재택근무', '개인', '늘었지만', '생활', '분리', '어려워']]\n\n\n\n\nCode\nkkma_morphs_list_stop=[]\nfor words in kkma_morphs_list:\n  tokenized=[]\n  for word in words:\n    if not word in stopwords:\n      tokenized.insert(len(tokenized)+1,word)\n  kkma_morphs_list_stop.insert(len(kkma_morphs_list_stop)+1,tokenized)\n\n\n\n\nCode\nkkma_morphs_list_stop[:5]\n\n\n[['코로나', '19', '우려', '유엔', '본부', '직원', '3000', '명', '3', '주간', '재택근무', '돌입'],\n ['재택근무', '중', '도', '보육', '라', '고요', '긴급', '보육', '청원', '갑론을박'],\n ['KBS',\n  '본관',\n  '3',\n  '층',\n  '직원',\n  '1',\n  '명',\n  '확',\n  '질',\n  'ㄴ',\n  '필수',\n  '인력',\n  '제외',\n  '재택근무'],\n ['서울', 'LS', '용', '산', '타워', '직장인', '확', '진', '판정', '임직원', '재택근무'],\n ['코로나', '시대', '재택근무', '개인', '늘', '었', '생활', '분리', '어렵']]\n\n\n\nWord Cloud\n\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk as nltk\n\n\n\n\nCode\ntotal_okt= []\nfor element in okt_morphs_list_stop:\n  total_okt+=element\n\n\n\n\nCode\ntotal_okt_over2=pd.Series(total_okt)[pd.Series(total_okt).str.len()>=2]\n\n\n\n\nCode\ncount_okt=pd.DataFrame(total_okt).value_counts().rename_axis('unique_values').reset_index(name='counts')\ncount_okt.columns=[\"word\",\"counts\"]\n\n\n\n\nCode\ncount_okt2=count_okt >> mask(X.word.str.len() >=2)\ncount_okt2=count_okt2.reset_index().iloc[:,1:]\ncount_okt2.head(60)\n\n\n\n\n\n\n  \n    \n      \n      word\n      counts\n    \n  \n  \n    \n      0\n      재택근무\n      968\n    \n    \n      1\n      코로나\n      365\n    \n    \n      2\n      19\n      191\n    \n    \n      3\n      재택\n      163\n    \n    \n      4\n      직원\n      148\n    \n    \n      5\n      확산\n      86\n    \n    \n      6\n      근무\n      85\n    \n    \n      7\n      기업\n      83\n    \n    \n      8\n      확진\n      75\n    \n    \n      9\n      출근\n      57\n    \n    \n      10\n      확대\n      53\n    \n    \n      11\n      시행\n      46\n    \n    \n      12\n      진자\n      41\n    \n    \n      13\n      유연근무제\n      41\n    \n    \n      14\n      유연\n      41\n    \n    \n      15\n      직장\n      40\n    \n    \n      16\n      콜센터\n      40\n    \n    \n      17\n      공무원\n      38\n    \n    \n      18\n      연장\n      38\n    \n    \n      19\n      실시\n      34\n    \n    \n      20\n      거리\n      33\n    \n    \n      21\n      감염\n      33\n    \n    \n      22\n      전환\n      33\n    \n    \n      23\n      정부\n      32\n    \n    \n      24\n      포스코\n      32\n    \n    \n      25\n      두기\n      31\n    \n    \n      26\n      공공기관\n      31\n    \n    \n      27\n      권고\n      30\n    \n    \n      28\n      도입\n      30\n    \n    \n      29\n      폐쇄\n      30\n    \n    \n      30\n      직장인\n      29\n    \n    \n      31\n      단계\n      28\n    \n    \n      32\n      회식\n      27\n    \n    \n      33\n      회사\n      27\n    \n    \n      34\n      삼성\n      27\n    \n    \n      35\n      출퇴근\n      27\n    \n    \n      36\n      돌입\n      26\n    \n    \n      37\n      해야\n      26\n    \n    \n      38\n      LG\n      26\n    \n    \n      39\n      사업\n      25\n    \n    \n      40\n      국회\n      25\n    \n    \n      41\n      하는\n      25\n    \n    \n      42\n      서울\n      24\n    \n    \n      43\n      대응\n      23\n    \n    \n      44\n      금지\n      23\n    \n    \n      45\n      비상\n      23\n    \n    \n      46\n      분산\n      22\n    \n    \n      47\n      그룹\n      21\n    \n    \n      48\n      방역\n      20\n    \n    \n      49\n      지원\n      20\n    \n    \n      50\n      에도\n      20\n    \n    \n      51\n      활용\n      20\n    \n    \n      52\n      전원\n      19\n    \n    \n      53\n      강화\n      19\n    \n    \n      54\n      업무\n      19\n    \n    \n      55\n      임산부\n      19\n    \n    \n      56\n      본사\n      19\n    \n    \n      57\n      KT\n      19\n    \n    \n      58\n      신청\n      18\n    \n    \n      59\n      비대\n      18\n    \n  \n\n\n\n\n\n\nCode\ntotal_okt_over2 # 9467개\ntotal_okt_over3=[]\nfor element in total_okt_over2:\n  if element not in [\"재택근무\",\"유연근무제\",\"19\",\"재택\",\"해야\",\"진자\",\"비대\"]:\n    total_okt_over3.insert(0,element)\n\n\n\n\nCode\noktplot=nltk.Text(total_okt_over3,name=\"Test\")\nfig=plt.figure(figsize = (10,5))\nax = fig.add_subplot(1, 1, 1)\nplt.title(\"키워드 빈도별 그래프\",fontsize=20)\noktplot.plot(50)\nplt.show(ax)\n\n\n\n\n\n\n\nCode\ndata=oktplot.vocab().most_common(50)\nplt.figure(figsize = (10,5))\npath=\"/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf\"\nwc=WordCloud(font_path=path,relative_scaling=0.2,background_color=\"white\",width=1200, height=800).generate_from_frequencies(dict(data))\n\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()\n\n\n\n\n\n\n분석의 문제점  제목기반 분석이라 의미있는 내용을 뽑아낼 수 없었음. 맥락이 제거된 빈도기반 분석으로 어떤 맥락에서 사용되었는지 알수가 없었음. 의미있는 인사이트를 찾을수는 없었으나 공무원, 공공기관, 포스코, LG 등 공공영역이나 대기업 위주로 유연근무제가 시행된다는 사실을 확인할 수 있었음. 콜센터와 같은 밀집된 곳에서 시행하는 업무의 경우, 유연근무제를 통해 감염을 확산을 저지하려는 시도가 있었지 않았나 추측해 보았음.\nTF-IDF\n\n\n\nCode\nfrom collections import defaultdict\n\nvectorizer = TfidfVectorizer()\ntdm = vectorizer.fit_transform(sentence_arr)\n\n\n\n\nCode\nword_count = pd.DataFrame({\n    '단어': vectorizer.get_feature_names(),\n    '빈도': tdm.sum(axis=0).flat\n})\n\n\n/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\nCode\nword_count.sort_values(\"빈도\",ascending=False).reset_index(drop=True).head(30)\n\n\n\n\n\n\n  \n    \n      \n      단어\n      빈도\n    \n  \n  \n    \n      0\n      코로나19\n      248.149943\n    \n    \n      1\n      코로나\n      225.877234\n    \n    \n      2\n      확진\n      189.862035\n    \n    \n      3\n      재택근무\n      166.912066\n    \n    \n      4\n      확진자\n      141.297835\n    \n    \n      5\n      거리두기\n      134.722720\n    \n    \n      6\n      직원\n      116.244045\n    \n    \n      7\n      폐쇄\n      82.505198\n    \n    \n      8\n      수도권\n      79.311515\n    \n    \n      9\n      사회적\n      77.250924\n    \n    \n      10\n      방역\n      64.115543\n    \n    \n      11\n      콜센터\n      63.758580\n    \n    \n      12\n      감염\n      60.624583\n    \n    \n      13\n      발생\n      59.834789\n    \n    \n      14\n      3단계\n      54.067897\n    \n    \n      15\n      정부\n      53.735398\n    \n    \n      16\n      확산\n      51.893703\n    \n    \n      17\n      서울\n      51.615911\n    \n    \n      18\n      비상\n      48.585535\n    \n    \n      19\n      1명\n      47.710836\n    \n    \n      20\n      국회\n      45.337426\n    \n    \n      21\n      백신\n      44.223253\n    \n    \n      22\n      신규\n      43.779173\n    \n    \n      23\n      추가\n      43.749493\n    \n    \n      24\n      집단감염\n      43.617216\n    \n    \n      25\n      검사\n      40.211961\n    \n    \n      26\n      마스크\n      38.943360\n    \n    \n      27\n      음성\n      37.046574\n    \n    \n      28\n      격상\n      35.974340\n    \n    \n      29\n      재택\n      35.499295\n    \n  \n\n\n\n\n\n\nc. 동시 출현(Co-Occurrence) \n참고: https://bab2min.tistory.com/598\n\n\nCode\n# 2글자 이상의 단어로 한정 / 의미없는 숫자 제거\ncooccur=[]\nfor elements in okt_morphs_list_stop:\n  new_elements=[]\n  for i in elements:\n    if i not in [\"재택근무\",\"유연\"]:\n      text = re.sub(r'[a-zA-Z0-9]',' ',i).strip()\n      if len(text)>=2:\n        new_elements.append(text)\n  cooccur.append(new_elements)\n\n\n\n\nCode\ncount = {}   #동시출현 빈도가 저장될 dict\nfor line in cooccur:\n    words = line \n    for i, a in enumerate(words):\n        for b in words[i+1:]:\n            if a == b: continue   #같은 단어의 경우는 세지 않음\n            if a > b: \n              a, b = b, a   #A, B와 B, A가 다르게 세어지는것을 막기 위해 항상 a < b로 순서 고정\n            count[a, b] = count.get((a, b), 0) + 1   #실제로 센다\n\n\n\n\nCode\ndef dict_to_df(df):\n        data_df = pd.DataFrame({'keys': df.keys(), 'features': df.values()})\n        data_df['word1'] = data_df['keys'].apply(lambda x: x[0])\n        data_df['word2'] = data_df['keys'].apply(lambda x: x[1])\n        return data_df[['word1','word2','features']]\n\n\n\n\nCode\ncooccur_df.sort_values(\"features\",ascending=False).head(10)\n\n\n\n\n\n\n  \n    \n      \n      word1\n      word2\n      features\n    \n  \n  \n    \n      884\n      거리\n      두기\n      89\n    \n    \n      98\n      코로나\n      확산\n      62\n    \n    \n      24\n      직원\n      확진\n      52\n    \n    \n      65\n      근무\n      재택\n      46\n    \n    \n      94\n      근무\n      유연\n      39\n    \n    \n      770\n      직원\n      코로나\n      36\n    \n    \n      174\n      코로나\n      확진\n      34\n    \n    \n      585\n      근무\n      분산\n      32\n    \n    \n      1238\n      건물\n      폐쇄\n      30\n    \n    \n      745\n      감염\n      직원\n      27\n    \n  \n\n\n\n\n\n\nCode\ncooccur_df=dict_to_df(count)\ncooccur_df.features.describe()\n\n\ncount    9559.000000\nmean        2.456219\nstd         2.635716\nmin         1.000000\n25%         1.000000\n50%         2.000000\n75%         3.000000\nmax        89.000000\nName: features, dtype: float64\n\n\n\n\nd. 네트워크 그래프\n\n\nCode\nimport networkx as nx\nimport operator\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cm\n\n\n\n\nCode\ncooccur_df_major=cooccur_df >> mask(X.features>=13)\n\n\n\n\nCode\n# generate sample graph\nplt.figure(figsize = (9,9),facecolor='k')\nplt.rcParams['font.sans-serif'] = ['NanumBarunGothic'] \ng = nx.from_pandas_edgelist(cooccur_df_major, 'word1', 'word2')\n\n\n# centrality\ndeg_centrality = nx.degree_centrality(g)\ncentrality = np.fromiter(deg_centrality.values(), float)\n# plot\npos = nx.kamada_kawai_layout(g,scale=3)\nnx.draw(g, pos, node_color=centrality,with_labels=True)\nplt.title(\"유연근무제 기사제목 분석\",size=15)\nplt.cool()\n\n\nsizes = centrality / np.max(centrality) * 200\nnormalize = mcolors.Normalize(vmin=centrality.min(), vmax=centrality.max())\ncolormap = cm.cool\n\nscalarmappaple = cm.ScalarMappable(norm=normalize, cmap=colormap)\nscalarmappaple.set_array(centrality)\n\nplt.colorbar(scalarmappaple,shrink=0.3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\ndef dict_to_df_1(df):\n        data_df = pd.DataFrame({'keys': df.keys(), 'features': df.values()})\n        data_df['word1'] = data_df['keys']\n        return data_df[['word1','features']]\n\n\n\n\nCode\ndict_to_df_1(deg_centrality).sort_values(\"features\",ascending=False)[1:20]\n\n\n\n\n\n\n  \n    \n      \n      word1\n      features\n    \n  \n  \n    \n      5\n      코로나\n      0.101266\n    \n    \n      10\n      기업\n      0.088608\n    \n    \n      34\n      본사\n      0.075949\n    \n    \n      44\n      경력\n      0.075949\n    \n    \n      6\n      근무\n      0.075949\n    \n    \n      1\n      확진\n      0.050633\n    \n    \n      48\n      금지\n      0.050633\n    \n    \n      57\n      단계\n      0.037975\n    \n    \n      7\n      재택\n      0.037975\n    \n    \n      16\n      시행\n      0.037975\n    \n    \n      36\n      거리\n      0.037975\n    \n    \n      18\n      아기\n      0.037975\n    \n    \n      73\n      국회\n      0.037975\n    \n    \n      33\n      감염\n      0.037975\n    \n    \n      55\n      구로\n      0.025316\n    \n    \n      26\n      전환\n      0.025316\n    \n    \n      66\n      구글\n      0.025316\n    \n    \n      42\n      시차\n      0.025316\n    \n    \n      41\n      폐쇄\n      0.025316"
  },
  {
    "objectID": "posts/Public Data 3/FWA.html#분석-실패-요인-및-느낀점",
    "href": "posts/Public Data 3/FWA.html#분석-실패-요인-및-느낀점",
    "title": "Analysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]",
    "section": "분석 실패 요인 및 느낀점",
    "text": "분석 실패 요인 및 느낀점\n\n(진행하더라도 유의미한 분석결과가 나오지 않을 것으로 예상되어) 토픽 모델링은 진행하지 않았음\n빈도기반 텍스트 마이닝 분석은 그러한 단어가 어떤 맥락에서 형성되었는지 알 수 없고 어떠한 의미를 가지는지를 이해하는데 있어서 어려움이 있다.\n신문 기사 제목들이 서로 응집성을 가지지 않고 산발적으로 나타나는 경우가 다수였기 때문에 적절한 분석이 진행되지 못한 것으로 보임.\n이후 보다 의미있는 분석을 위해서는 신문 기사 내용들을 활용을 하거나, 다른 텍스트 원천을 활용하는 것이 보다 효과적인 분석에 있어서 필요할 것으로 보임."
  },
  {
    "objectID": "posts/Public Data 3/FWA.html#경제활동인구조사-20152021",
    "href": "posts/Public Data 3/FWA.html#경제활동인구조사-20152021",
    "title": "Analysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]",
    "section": "1. 경제활동인구조사 (2015~2021) ",
    "text": "1. 경제활동인구조사 (2015~2021) \n\n\n경제활동인구조사: 통계종류지정통계/조사통계 -계속여부: 계속통계 -작성목적: 국민의 경제활동(취업, 실업, 노동력 등) 특성을 조사함으로써 거시경제 -분석과 인력자원의 개발정책 수립에 필요한 기초 자료를 제공 -작성주기:월  -작성체계: 조사원(면접조사)→지방통계청(사무소)→통계청  -공표범위: 시도 -공표주기: 월 -특이점: 2017년 9월 조사부터 복수응답가능\n\n\n\n\nCode\nfrom mizani.breaks import date_breaks, minor_breaks\nfrom mizani.formatters import date_format\nimport matplotlib.font_manager as fm\n\n\n\n\nCode\npath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf' \nfont = fm.FontProperties(fname=path, size=13)\n\n\n\na. 유연근무제 활용여부\n\n\nCode\nyes=pd.read_json(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DE7099S/2/1/20211204112805_1&prdSe=M&startPrdDe=201508&endPrdDe=202108\")\nno=pd.read_json(\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DE7099S/2/1/20211204112931_1&prdSe=M&startPrdDe=201508&endPrdDe=202108\")\n\n\n\n\nCode\npj_usege=yes >> bind_rows(no, join='outer') >> select(X.PRD_DE,X.C1_NM,X.DT)\npj_usege=pj_usege.reset_index(drop=True)\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (8,6)\n(ggplot(pj_usege, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(0,22001),breaks= np.arange(0,22001,2000))\n    + geom_line() \n    + geom_point(pj_usege,aes(x='factor(PRD_DE)',y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"인원 (천 명)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"참여 여부\")\n    + scale_color_manual(values=(\"blue\",\"red\"))\n    + geom_text(aes(label=\"DT\"),nudge_x=0, nudge_y=1000,size=13,color=\"black\")\n    + ggtitle('임금 근로자 중 유연근무제 참여 인원 수\\n(자료 출처: 통계청 경제활동인구조사)'))\n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762697391465)>\n\n\n\n\nb. 성별 유연근무제 활용현황\n\n\nCode\nlink_list=[]\nlink=\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DE7103S/2/1/20211204135633&prdSe=M&startPrdDe=\"\nfor i in range(201508,202109,100):\n  link_list.append(link+str(i))\n\n\n\n\nCode\npj_gender=pd.read_json(link_list[0])\nfor i in link_list[1:]:\n  pj_gender = pj_gender >> bind_rows(pd.read_json(i), join='outer')\n\n\n\n\nCode\npj_gender=pj_gender.reset_index(drop=True)\n\n\n\n\nCode\npj_gender2=pj_gender >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(~X.C1_NM==\"계\") >>  mask(~X.C2_NM==\"계\")\npj_gender2=pj_gender2.reset_index(drop=True)\n\n\n\n\nCode\npj_total=pj_gender >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT)  >> mask(~X.C1_NM==\"계\") >> mask(X.C2_NM==\"계\")\n\n\n\n\nCode\npj_total\n\n\n\n\n\n\n  \n    \n      \n      PRD_DE\n      C1_NM\n      C2_NM\n      DT\n    \n  \n  \n    \n      3\n      201508\n      남자\n      계\n      11006.6\n    \n    \n      6\n      201508\n      여자\n      계\n      8467.9\n    \n    \n      12\n      201608\n      남자\n      계\n      11085.6\n    \n    \n      15\n      201608\n      여자\n      계\n      8657.6\n    \n    \n      21\n      201708\n      남자\n      계\n      11188.2\n    \n    \n      24\n      201708\n      여자\n      계\n      8817.9\n    \n    \n      30\n      201808\n      남자\n      계\n      11171.3\n    \n    \n      33\n      201808\n      여자\n      계\n      8873.7\n    \n    \n      39\n      201908\n      남자\n      계\n      11395.7\n    \n    \n      42\n      201908\n      여자\n      계\n      9163.3\n    \n    \n      48\n      202008\n      남자\n      계\n      11361.3\n    \n    \n      51\n      202008\n      여자\n      계\n      9084.6\n    \n    \n      57\n      202108\n      남자\n      계\n      11516.6\n    \n    \n      60\n      202108\n      여자\n      계\n      9475.9\n    \n  \n\n\n\n\n\n\nCode\npj_total2=pj_total >> select(~X.C2_NM) >> spread(X.PRD_DE, X.DT) >> select(~X.C1_NM)\npj_total2.index=[\"남자\",\"여자\"]\npj_total2.transpose() >> mutate(ratio=np.round(X.남자/(X.남자+X.여자),2),배수 =np.round(X.남자/(X.여자),2) )\n# 총 경제활동 인원 대비 남성 비율 / 점차 줄어드는 중 \n\n\n\n\n\n\n  \n    \n      \n      남자\n      여자\n      ratio\n      배수\n    \n  \n  \n    \n      201508\n      11006.6\n      8467.9\n      0.57\n      1.30\n    \n    \n      201608\n      11085.6\n      8657.6\n      0.56\n      1.28\n    \n    \n      201708\n      11188.2\n      8817.9\n      0.56\n      1.27\n    \n    \n      201808\n      11171.3\n      8873.7\n      0.56\n      1.26\n    \n    \n      201908\n      11395.7\n      9163.3\n      0.55\n      1.24\n    \n    \n      202008\n      11361.3\n      9084.6\n      0.56\n      1.25\n    \n    \n      202108\n      11516.6\n      9475.9\n      0.55\n      1.22\n    \n  \n\n\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (8,6)\n(ggplot(pj_total, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(6000,15001),breaks= np.arange(0,22001,2000))\n    + geom_line() \n    + geom_point(pj_total,aes(x='factor(PRD_DE)',y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"인원 (천 명)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"성별\")\n    + scale_color_manual(values=(\"blue\",\"red\"))\n    + geom_text(aes(label=\"DT\"),nudge_x=0, nudge_y=500,size=13,color=\"black\")\n    + ggtitle('조사대상 성별 임금 근로자 수\\n(자료 출처: 통계청 경제활동인구조사)'))\n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762697419985)>\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (14,6)\n(ggplot(pj_gender2, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C2_NM\",color=\"C2_NM\")) \n    + facet_wrap('C1_NM')\n    + scale_y_continuous(limits=(0,14001),breaks= np.arange(0,22001,2000))\n    + geom_line() \n    + geom_point(pj_gender2, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C2_NM\",color=\"C2_NM\"))\n    + theme_classic() \n    + ylab(\"인원 (천 명)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"참여 여부\")\n    + scale_color_manual(values=(\"blue\",\"red\"))\n    + geom_text(aes(label=\"DT\"),nudge_x=0, nudge_y=1000,size=13,color=\"black\")\n    + ggtitle('임금 근로자 중 성별 유연근무제 참여 인원 수\\n(자료 출처: 통계청 경제활동인구조사)'))\n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762697654101)>\n\n\n\n\nCode\n# 성별 경제활동 참여 인구 대비 유연근무제 참여자 비율 \nratio_total=(pj_gender >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"계\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,3)*100,ratio_notuse=np.round(X.notused/X.계,3)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_male=(pj_gender >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"남자\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,3)*100,ratio_notuse=np.round(X.notused/X.계,3)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_female=(pj_gender >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"여자\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,3)*100,ratio_notuse=np.round(X.notused/X.계,3)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio=pd.concat([ratio_total,ratio_male,ratio_female], keys=[\"전체\",\"남성\",\"여성\"]).reset_index() >> select(~X.level_1) >> rename(성별=\"level_0\") \n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(ratio, aes(x='factor(PRD_DE)', y=\"ratio_use\",group=\"성별\",color=\"성별\")) \n    + scale_y_continuous(limits=(0,20),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(ratio, aes(x='factor(PRD_DE)', y=\"ratio_use\",group=\"성별\",color=\"성별\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"성별\")\n    + scale_color_manual(values=(\"#619CFF\",\"#F8766D\",\"gray\"))\n    + geom_text(aes(label=\"ratio_use\"),nudge_x=0, nudge_y=+0.5,size=10,color=\"black\",format_string='{}%')\n    + ggtitle('성별 총 임금근로자 대비 유연근무제 참여 인원 수\\n(자료 출처: 통계청 경제활동인구조사)'))    \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696383501)>\n\n\n\n\nCode\npj_age_2021=pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.PRD_DE==202108) >> mask(~X.C1_NM==\"계\") >>mask(~X.C2_NM==\"계\")\n\n\n\n\nCode\n# 2021년 기준 유연근무제 참여집단과 비참여집단 비교\n\n\n\n\nCode\npj_2021=pj_gender >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.PRD_DE==202108) \n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\ndodge_text = position_dodge(width=0.9)\n(ggplot(pj_2021,aes(x='C1_NM',y=\"DT\",fill=\"C1_NM\",group=\"C1_NM\")) \n    + scale_y_continuous(limits=(0,22000),breaks= np.arange(0,22001,2000))\n    + geom_bar(stat='identity',position = position_dodge(width = 0.9)) \n    + theme_classic() \n    + ylab(\"인원 (천 명)\")\n    + xlab(\"유형 별\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(fill=\"성별\")\n    + facet_wrap('C2_NM')\n    + scale_fill_manual(values=(\"gray\",\"#619CFF\",\"#F8766D\"))\n    + geom_text(aes(label='DT'), position=dodge_text,size=10, va='bottom', format_string='{}')\n    + ggtitle('임금 근로자 중 성별 유연근무제 참여 인원 수 (2021)\\n(자료 출처: 통계청 경제활동인구조사)'))\n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762697263013)>\n\n\n\n\nc. 연령 별 유연근무제 사용 유형\n\n\nCode\nlink_list=[]\nlink=\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DE7105S/2/1/20211204160618&prdSe=M&startPrdDe=\"\nfor i in range(201508,202109,100):\n  link_list.append(link+str(i))\n\n\n\n\nCode\npj_age=pd.read_json(link_list[0])\nfor i in link_list[1:]:\n  pj_age = pj_age >> bind_rows(pd.read_json(i), join='outer')\n\n\n\n\nCode\nratio_age_total=(pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"계\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_age_15=(pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"15 - 29세\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_age_30=(pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"30 - 39세\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_age_40=(pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"40 - 49세\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_age_50=(pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"50 - 59세\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_age_60=(pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"60세이상\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_age=pd.concat([ratio_age_total,ratio_age_15,ratio_age_30,ratio_age_40,ratio_age_50,ratio_age_60], keys=[\"전체\",\"15 - 29세\",\"30 - 39세\",\"40 - 49세\",\"50 - 59세\",\"60세이상\"]).reset_index() >> select(~X.level_1) >> rename(연령별=\"level_0\") \n\n\n\n\nCode\nratio_age[\"ratio_use\"]=np.round(ratio_age[\"ratio_use\"].astype(np.float64),2)\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(ratio_age, aes(x='factor(PRD_DE)', y=\"ratio_use\",group=\"연령별\",color=\"연령별\")) \n    + scale_y_continuous(limits=(0,25),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(ratio_age, aes(x='factor(PRD_DE)', y=\"ratio_use\",group=\"연령별\",color=\"연령별\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"연령 별\")\n    + geom_text(aes(label=\"ratio_use\"),nudge_x=0, nudge_y=+0.8,size=8,color=\"black\",format_string='{}%')\n    + ggtitle('연령별 총 임금근로자 대비 유연근무제 참여 인원 비율\\n(자료 출처: 통계청 경제활동인구조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8731672718093)>\n\n\n\n\nCode\npj_age_2021=pj_age >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.PRD_DE==202108) >> mask(~X.C1_NM==\"계\") >>mask(~X.C2_NM==\"계\")\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (12,6)\ndodge_text = position_dodge(width=0.9)\n(ggplot(pj_age_2021,aes(x='C1_NM',y=\"DT\",fill=\"C1_NM\",group=\"C1_NM\")) \n    + scale_y_continuous(limits=(0,6000),breaks= np.arange(0,22001,2000))\n    + geom_bar(stat='identity',position = position_dodge(width = 0.9)) \n    + theme_classic() \n    + ylab(\"인원 (천 명)\")\n    + xlab(\"유형 별\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(fill=\"연령별\")\n    + facet_wrap('C2_NM')\n    + geom_text(aes(label='DT'), position=dodge_text,size=10, va='bottom', format_string='{}') \n    + scale_fill_manual(values=(\"#4a4e4d\",\"#0e9aa7\" ,\"orange\",\"#f6cd61\",\"#f9caa7\"))\n    + ggtitle('임금 근로자 중 연령별 유연근무제 참여 인원 수 (2021)\\n(자료 출처: 통계청 경제활동인구조사)'))\n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696667145)>\n\n\n\n\nd. 혼인상태별 유연근무제 활용현황\n\n\nCode\nlink_marriage_list=[]\nlink=\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DE7104S/2/1/20211204164055&prdSe=M&startPrdDe=\"\nfor i in range(201508,202109,100):\n  link_marriage_list.append(link+str(i))\n\n\n\n\nCode\npj_marriage=pd.read_json(link_marriage_list[0])\nfor i in link_marriage_list[1:]:\n  pj_marriage = pj_marriage >> bind_rows(pd.read_json(i), join='outer')\n\n\n\n\nCode\nratio_marriage_total=(pj_marriage >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"계\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_marriage_no=(pj_marriage >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"미혼\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_marriage_yes=(pj_marriage >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.C1_NM==\"기혼\") >> select(~X.C1_NM) >> spread(X.C2_NM,X.DT) >> rename(used=\"활용하고 있음\",notused=\"활용하지 않음\")\n  >> mutate(ratio_use=np.round(X.used/X.계,4)*100,ratio_notuse=np.round(X.notused/X.계,4)*100) >> select(X.PRD_DE,X.ratio_use,X.ratio_notuse)) \n\n\n\n\nCode\nratio_marriage=pd.concat([ratio_marriage_total,ratio_marriage_yes,ratio_marriage_no], keys=[\"전체\",\"기혼\",\"미혼\"]).reset_index() >> select(~X.level_1) >> rename(결혼=\"level_0\") \n\n\n\n\nCode\nratio_marriage[\"ratio_use\"]=np.round(ratio_marriage[\"ratio_use\"].astype(np.float64),2)\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(ratio_marriage, aes(x='factor(PRD_DE)', y=\"ratio_use\",group=\"결혼\",color=\"결혼\")) \n    + scale_y_continuous(limits=(0,18),breaks= np.arange(0,101,3))\n    + geom_line() \n    + geom_point(ratio_marriage, aes(x='factor(PRD_DE)', y=\"ratio_use\",group=\"결혼\",color=\"결혼\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"혼인 여부\")\n    + scale_color_manual(values=(\"#619CFF\",\"#F8766D\",\"gray\"))\n    + annotate(\"text\", x=6.5, y=17.91, label=\"17.91%\", size=12,color=\"black\")\n    + annotate(\"text\", x=6.5, y=16.93, label=\"16.93%\", size=12,color=\"black\")\n    + annotate(\"text\", x=6.5, y=16, label=\"16.35%\", size=12,color=\"black\")\n    + ggtitle('혼인 유형 별 임금근로자 대비 유연근무제 참여 인원 수\\n(자료 출처: 통계청 경제활동인구조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696204773)>\n\n\n\n\nCode\npj_marriage_2021=pj_marriage >> select(X.PRD_DE,X.C1_NM,X.C2_NM,X.DT) >> mask(X.PRD_DE==202108) >> mask(~X.C1_NM==\"계\")\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\ndodge_text = position_dodge(width=0.9)\n(ggplot(pj_marriage_2021,aes(x='C1_NM',y=\"DT\",fill=\"C1_NM\",group=\"C1_NM\")) \n    + scale_y_continuous(limits=(0,15000),breaks= np.arange(0,22001,2000))\n    + geom_bar(stat='identity',position = position_dodge(width = 0.9)) \n    + theme_classic() \n    + ylab(\"인원 (천 명)\")\n    + xlab(\"유형 별\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(fill=\"연령별\")\n    + facet_wrap('C2_NM')\n    + geom_text(aes(label='DT'), position=dodge_text,size=10, va='bottom', format_string='{}')\n    + scale_fill_manual(values=(\"#619CFF\",\"#F8766D\"))\n    + ggtitle('임금 근로자 중 성별 유연근무제 참여 인원 수 (2021)\\n(자료 출처: 통계청 경제활동인구조사)'))\n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696952981)>\n\n\n\n\ne. 유연근무제 활용형태(복수응답)\n\n\nCode\nlink_type_list=[]\nlink=\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/101/DT_1DE7100S/2/1/20211204170759&prdSe=M&startPrdDe=\"\nfor i in range(201508,202109,100):\n  link_type_list.append(link+str(i))\n\n\n\n\nCode\npj_type=pd.read_json(link_type_list[0])\nfor i in link_type_list[1:]:\n  pj_type = pj_type >> bind_rows(pd.read_json(i), join='outer')\n\n\n\n\nCode\nratio_type=pj_type >> mask(X.ITM_NM_ENG==\"ratio\") >> select(X.PRD_DE,X.C1_NM,X.DT)\n\n\n\n\nCode\nratio_type['C1_NM']=pd.Categorical(ratio_type['C1_NM'], ordered=True,categories=['근로시간단축근무제','시차출퇴근제','선택적 근무시간제','재택 및 원격근무제','탄력적 근무제','기타유형(재량근무 등)'])\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(ratio_type,aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + geom_line() \n    + scale_y_continuous(limits=(-1,50),breaks= np.arange(0,101,10))\n    + geom_point(ratio_type, aes(x='factor(PRD_DE)', y=\"DT\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"유연근무 유형\")\n    + scale_color_manual(values=(\"#4a4e4d\",\"#0e9aa7\" ,\"#f9caa7\",\"red\",\"#f6cd61\",\"green\"))\n    + geom_text(aes(label=\"DT\"),nudge_x=0, nudge_y=+1,size=8,color=\"black\",format_string='{}%')\n    + geom_vline(xintercept=5+(4/12),linetype=\"dashed\")\n    + ggtitle('유연근무제 참여 유형별 비율\\n(자료 출처: 통계청 경제활동인구조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696647101)>\n\n\n\n\n유연근무제의 사용 현황 정리 (통계청 경제활동인구조사)\n\nCOVID-19과 상관 없이 인식 개선으로 유연근무제 사용 인원 수는 지속적으로 증가하여 왔음.\n예상했던 것 과는 달리 남성들이 유연근무제를 사용하는 비율이 여성이 사용하는 비율보다 높았음. (2021년 8월 기준 남성이 18.2%, 여성이 15.2%)\n유연근무제를 가장 많이 사용하는 계층은 30대 였으며 그 다음으로 40대 20대 50대 순이다. 60세 이상 집단의 경우 거의 사용하지 않았는데 50대 이상 집단과 비교하였을 때 큰 차이가 나타나고 있는 것을 확인할 수 있다.\nCOVID-19 이전까지 기혼집단이 유연근무제를 많이 사용하였으나, 이후로는 미혼집단이 오히려 더 많이 사용하고 있는 것으로 보인다.(2020년 8월 기준 미혼집단이 17.91%, 기혼집단이 16.35%)\nCOVID-19까지는 재택 및 원격 근무제(공간적 유연) 선택 비율이 가장 낮고 시차출퇴근 선택자들의 비율이 가장 높았으나, 2021년 기준 재택 및 원격 근무제 선택 비중이 가장 높은 것으로 확인된다. \n유연근무제가 원래 일 가정 갈등 정책의 하위 정책으로 분류되어진 상황에서 여성들에 비해 남성들이 더 많이 사용하고 있다는 점, 미혼 집단의 사용율이 오히려 높다는 점에서 초기의 정책 목표를 달성하였다고 보기 어려운 부분이 있다."
  },
  {
    "objectID": "posts/Public Data 3/FWA.html#공직생활실태조사-20172020",
    "href": "posts/Public Data 3/FWA.html#공직생활실태조사-20172020",
    "title": "Analysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]",
    "section": "2. 공직생활실태조사 (2017~2020)",
    "text": "2. 공직생활실태조사 (2017~2020)\n\n지금부터의 분석은 2017~2020년 한국행정연구원이 (주)리서치앤리서치에 의뢰하여 실시한 5개년도의 ‘공직생활 실태조사’ 자료를 활용하여 분석한 것이다.  - 정부의 인적자원관리 현황과 공무원의 인식을 파악하기 위한 목적으로 조사시점기준 46개 중앙행정기관 및 17개 광역자치단체 소속 일반직 공무원을 대상으로 진행  층화 집락 추출 방식을 활용하여 부차모집단별 추출된 각 표본 과/팀에서 10명내외를 계통추출  - 표본의 크기는 각년도 말일 기준으로 모집단을 모비율 추정의 목표 오차(95% 신뢰수준 오차의 한계)인 2%∼3%를 만족하도록 구성  - 확률표본 수집 후 E-mail 웹 조사를 사용하였음.\n\n\n\nCode\n# 반복작업의 어려움으로 파이썬용 함수를 만들었음.\nlink=\"https://kosis.kr/openapi/statisticsData.do?method=getList&apiKey=YjNjZjJmNDI2NWE1N2U3NGRiZWE3ZmI3MmU4YjliNGU=&format=json&jsonVD=Y&userStatsId=jsw0641/417/DT_417002N_011/2/1/20211204203850&prdSe=Y&startPrdDe=\"\n\ndef kosis(link,a,b):\n  link_list=[]\n  for i in range(int(a),int(b)+1,1):\n    new_link=link+str(i)\n    link_list.append(new_link)\n  \n  df= pd.read_json(link_list[0])\n  if len(link_list)>=2:\n    for i in link_list[1:]:\n      df = df >> bind_rows(pd.read_json(i), join='outer')\n  result=df.reset_index(drop=True)    \n  return result  \n\n\n\n\nCode\npj_gov=kosis(link,2017,2020)\n\n\n\na. 성별 정부조직 유연근무제 사용 비율\n\n\nCode\npj_gov=pj_gov>>mask(X.C2_NM==\"있다\")\n\n\n\n\nCode\npj_gov_gender=pj_gov >> mask((X.C1_NM==\"남성\")|(X.C1_NM==\"여성\")|(X.C1_NM==\"전체\"))\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(pj_gov_gender, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(50,80),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(pj_gov_gender, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"성별\")\n    + scale_color_manual(values=(\"#619CFF\",\"#F8766D\",\"gray\"))\n    + geom_text(aes(label=\"DT\"),nudge_x=.25, nudge_y=+.5,size=12,color=\"black\",format_string='{}%')\n    + ggtitle('성별 정부조직 공무원 유연근무제 참여 비율\\n(자료 출처: 한국행정연구원 공직생활실태조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762697292525)>\n\n\n\n\nb. 연령 별 정부조직 유연근무제 사용 비율\n\n\nCode\npj_gov_age=pj_gov >> mask((X.C1_NM==\"20대\")|(X.C1_NM==\"30대\")|(X.C1_NM==\"40대\")|(X.C1_NM==\"50대 이상\")|(X.C1_NM==\"전체\"))\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(pj_gov_age, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(42,85),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(pj_gov_age, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"연령대별\")\n    + geom_text(aes(label=\"DT\"),nudge_x=.25, nudge_y=+.5,size=12,color=\"black\",format_string='{}%')\n    + ggtitle('연령대별 정부조직 공무원 유연근무제 참여 비율\\n(자료 출처: 한국행정연구원 공직생활실태조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762698177993)>\n\n\n\n\nc. 직급 별 정부조직 유연근무제 사용 비율\n\n\nCode\npj_gov_rank=pj_gov >> mask((X.C1_NM==\"1~4급\")|(X.C1_NM==\"5급\")|(X.C1_NM==\"6~7급\")|(X.C1_NM==\"8~9급\"))\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(pj_gov_rank, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(42,85),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(pj_gov_rank, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"직급별\")\n    + geom_text(aes(label=\"DT\"),nudge_x=.25, nudge_y=+.5,size=12,color=\"black\",format_string='{}%')\n    + ggtitle('직급별 정부조직 공무원 유연근무제 참여 비율\\n(자료 출처: 한국행정연구원 공직생활실태조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762697124449)>\n\n\n\n\nd. 소속조직 수준 별 정부조직 유연근무제 사용 비율\n\n\nCode\npj_gov_level=pj_gov >> mask((X.C1_NM==\"광역자치단체\")|(X.C1_NM==\"중앙행정기관\"))\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(pj_gov_level, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(42,85),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(pj_gov_level, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"소속조직 수준별\")\n    + geom_text(aes(label=\"DT\"),nudge_x=.25, nudge_y=+.5,size=12,color=\"black\",format_string='{}%')\n    + ggtitle('소속조직 수준별 정부조직 공무원 유연근무제 참여 비율\\n(자료 출처: 한국행정연구원 공직생활실태조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696037269)>\n\n\n\n\ne. 재직기간별 유연근무제 사용 비율\n\n\nCode\npj_gov_length=pj_gov >> mask((X.C1_NM==\"5년 이하\")|(X.C1_NM==\"6~10년\")|(X.C1_NM==\"11~15년\")|(X.C1_NM==\"16~20년\")|(X.C1_NM==\"21~25년\")|(X.C1_NM==\"26년 이상\")  )\n\n\n\n\nCode\npj_gov_length[\"C1_NM\"]=pd.Categorical(pj_gov_length[\"C1_NM\"], ordered=True,categories=[\"5년 이하\",\"6~10년\",\"11~15년\",\"16~20년\",\"21~25년\",\"26년 이상\"])\n\n\n\n\nCode\nplt.figure()\npn.options.figure_size = (10,6)\n(ggplot(pj_gov_length, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\")) \n    + scale_y_continuous(limits=(42,80),breaks= np.arange(0,101,5))\n    + geom_line() \n    + geom_point(pj_gov_length, aes(x='factor(PRD_DE)', y=\"DT\",group=\"C1_NM\",color=\"C1_NM\"))\n    + theme_classic() \n    + ylab(\"비율 (%)\")\n    + xlab(\"시점\") \n    + theme(text=element_text(fontproperties=font))\n    + labs(colour=\"재직기간별\")\n    + geom_text(aes(label=\"DT\"),nudge_x=.25, nudge_y=+.5,size=12,color=\"black\",format_string='{}%')\n    + ggtitle('재직기간별 정부조직 공무원 유연근무제 참여 비율\\n(자료 출처: 한국행정연구원 공직생활실태조사)'))   \n\n\n/usr/local/lib/python3.7/dist-packages/plotnine/utils.py:1246: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  if pdtypes.is_categorical(arr):\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n<ggplot: (8762696138673)>\n\n\n\n\n정부조직 유연근무제의 특징 정리 (한국행정연구원 공직생활실태조사)\n\nCOVID-19이전까지 유연근무제의 사용이 꾸준히 감소하던 추세였다가, 발생 이후 급증함.\n전체기간에서 민간 및 공공기관 인구를 모두 포함해서 조사한 경제활동인구조사와 비교 시 정부조직 공무원들은 유연근무제를 더욱 약 2배 정도 많이 사용하고 있는 것으로 확인되었음.\n\n정부조직에서는 경제활동인구조사와는 달리 남성들이 유연근무제를 사용하는 비율이 여성이 사용하는 비율보다 낮았음. (2020년 기준 남성이 72.4%, 여성이 75.5%) 20대의 경우 정부조직에서 COVID-19 이전까지 유연근무제를 가장 적게 사용하던 연령대였으나, COVID-19이후 가장 많이 사용하는 연령대로 변화하였음. (2019년 47.5% 2020년 79.6%) 낮은 연령대의 공무원들이 유연근무제를 사용하지 못했던 원인에 대해서 깊이 있게 고찰해볼 필요가 존재함.(눈치가 보여서, 사용 못하였음… or 2015년 자체부서평가의 시행으로 잠시 올라갔다가 다시 떨어지는 추세)\n이러한 경향성은 직급별, 재직기간별 분석 자료를 통해 분석하였을 때 더욱 뚜렷하게 드러나는데, COVID-19이전까지는 하더라도 유연근무제가 재직기간이 길수록 직급이 높을 수록 훨씬 더 많이 사용하는 경향성이 있는 것으로 보였는데, 2020년 데이터에서는 직급 별 유연근무제 사용 비율이 큰 차이가 없었고, 재직기간이 5년 미만인 공무원들이 79%로 다른 세대에 비해서 월등하게 높은 유연근무제 사용율을 보이고 있는 것을 확인할 수 있었음.\n공공영역 유연근무제의 확산을 위해서는 유연근무제를 사용할 수 있는 조직적, 관리자적 지원이 필수적(Choi, 2017)이라는 이전 연구결과를 통해 미루어 보았을 때 유연근무제의 확산을 위해서 이후 소위 조직 내 입지가 적은 구성원들이 자유롭게 사용할 수 있는 분위기를 형성하기 위한 정책적 노력이 필요할 것으로 보인다."
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Research",
    "section": "",
    "text": "Publication\n\n\nJu, S., Lee, J. & Kim, B.J. (2022). Evidence-Based Policy in Asian Countries: A Comparative Analysis of Evidence-Based Policy Implementation in Myanmar, Indonesia, Hong Kong, Japan, and South Korea. Korean Society and Public Administration, 33(3), 97-131. DOI: https://doi.org/10.53865/KSPA.2022.11.33.3.97\n\nAbstract:  The present study explores the process of evidence-based policy (EBP) implementation in five Asian countries including Myanmar, Indonesia, Hong Kong, Japan, and South Korea. Building on the model of Evidence-based Practice Implementation (EPIS framework; Aarons et al., 2011; Moullin et al., 2019), this study conducted an exploratory comparative case study on five policy cases–a single policy case from each of the five Asian countries–and evaluated the extent to which each policy case has accomplished each of the four levels (i.e., exploration, preparation, implementation, sustainment) of EBP implementation suggested by the EPIS framework. Our findings suggest that the evaluation of the five EBP cases in Asian countries ranged from level 1 (exploration) to level 2 (preparation) out of a total of four levels. We discuss the theoretical and practical implications of our findings.\nKeywords: Evidence-Based Policy, The Model of Evidence-based Practice Implementation, Comparative Policy Research, Collective Case Study, Asian countries\n\n\n\n\n\nWorking Papers\n\n\nLee, J.*, Ju, S.* & Kim, B.J., Social Motivation and Policy Compliance in Crisis: Moderating Role of Ideological Self-Categorization in the United States and South Korea  (Submitted to Public Administration Review, * First and Second Author Contributed Equally)\n\nAbstract:  This article explores the role of social motivation for policy compliance in the context of uncertainty induced by the COVID-19 pandemic. The focus is on how citizens’ calculative (out of deterrent fear) and normative (out of a sense of duty) compliance motivation are affected by their ideological self-categorization in response to coercive prevention policies. Results from a three-way interaction analysis using survey data from the United States and South Korea suggests potential role of ideological identification on the association between motivations for compliance and dissonance in compliance. The degree of the categorization effect was moderated by the individualistic-collectivistic cultural dimension of the two countries. By integrating the social identity approach into research on policy compliance, this article concludes that policymakers should consider citizens’ propensity to conform to in-group norms when shaping the policy response to widespread crises as it plays a critical role in effectively managing their cooperation.\nKeywords: social motivation, self-categorization theory, policy compliance, ideological identification\n\nJu, S. & Kim, B.J., The Role of Information Technology on Improving Organizational Performance: Evidence from Korean Public Organizations  (Preparation in manuscript, Target journal: Government Information Quarterly)\n\nAbstract:  In the last three decates, e-government researchers focused on various aspects of information technology, including the technology itself, how organizations utilize it, and its application in diverse contexts. Nevertheless, there has been limited theoretical discussion and empirical evidence on how these factors influence attaining strategic aspirations in public organizations. Thus, this study aims to examine how investing in IT resources can enhance the capabilities of public organizations and subsequently improve the overall performance of the organization. Based on the resource-based view (RBV) and public value theory, this study empirically examines the influence of IT on public-sector organizations. To examine the impact of IT investment on organizational performance, this article analyzed four-year archival panel data from 482 South Korean public enterprises and quasi-governmental institutions by fixed-effects model and random-effects ordered logit model. The findings indicate that there is a positive relationship between the total investment in IT and operational capabilities. Furthermore, investing in IT resources has a positive overall impact on organizational performance, with operational capabilities serving as a mediator. The analysis revealed that investing in infrastructure for capacity-building, when compared to software and hardware maintenance, improves performance more. The article contributes to the understanding of e-government performance by discussing how investing in information technology can improve organizational outcomes. It also emphasizes the importance of strategic IT investment in gaining a competitive advantage in the political reputation market. In sum, this study sheds light on the intricate connection between IT resources and public organizational performance.\nKeywords: Public Sector IT Strategy, IT Resource, Organizational Performance, Public Value Theory, Resource Based View, E-Government\n\nJu, S., Psychological Micro-foundation of Evidence-Based Practice: The influence of Organizational Identities on Managerial Epistemic Motivation  (Gathering preliminary findings) \nJu, S., Public-Private Collaboration and Ambidexterity in Public Sector: Boundary Condition of Political Environment  (Gathering preliminary findings) \nLee, B., Ju, S., Lee, J., & Kim, B.J., Exploring the Challenges and Alternatives of SDGs Implementation: Text-Mining on UN SDG Voluntary National Review of 6 Countries [in Korean]  (Completed data analysis and Writing in progress)\n\n\n\n\nAcademic Presentations\n\n\nJu, S. & Lee, J. (2024), How does Self-Categorization shape Compliance?: Political Ideology and COVID-19, 2024 ASPA Annual Conference: Building Resilient Communities, Minneapolis, MN, April 2024.\nJu, S (2024), Psychological Micro-foundation of Evidence-Based Practice: The influence of Organizational Identities on Managerial Epistemic Motivation, 2024 University of Auckland - Seoul National University Joint International Conference: BK21 FOUR & 10-10 Project, Auckland, New Zealand, January 2024\nJu, S. & Lee, J. (2022), Evidence-Based Policy in Asian Countries: Applying Model of Evidence-Based Practice Implementation for Maturation Stage Analysis [in Korean, Poster Session], 2022 Korean Public Administration Summer Conference and Annual KAPA International Conference, Yeosu, Korea, June 2022."
  },
  {
    "objectID": "research_interests.html",
    "href": "research_interests.html",
    "title": "Interests and Prospects",
    "section": "",
    "text": "My research interests lie at the intersection of public management, macro-organization theories, and information systems. Toward better public-sector decision-making, my interests and prospected topics are\n\n\n\n\n\n(Strategic) Decision-Making Theories\nInteraction Between Macro and Micro Level Organizational Behavior\nHuman-Machine Cooperation in Organizational Decision-making: AI, Machine-Learning, and Automation\n\n\n\n\n\n\n\nNew Public Management vs. Public Value Theory\nStrategic Approach in Public Sector\n\n\n\n\n\n\n\nBeing Evidence-Based is Being Data-Driven?\nWhat are the Differences Between Evidence-Based Approach and Contemporary Policy Analysis? (such as discourse policy analysis)\nPositivism vs. Constructivism in Research Utilization\n\n\n\n\n\n\n\nGovernment IT Strategy and Management\nAcquiring (or Creating) Public Value with Information Systems\nQuantifying Societal Impact of Information Systems: Public Policy in the Information Age\n\n\n\n\n\n\n\nMachine-learning, Deep-Learning, and other Computational Methods for both Public and Business Data Analytics\nApplied Econometrics for Quasi-Experimental Approaches\n(Panel Data Analysis, Synthetic Control, Instrumental Variable)\nHierarchical Linear Modelling: Certificate\nPolicy Analysis Techniques\n(Cost-Benefit Analysis, Analytic Hierarchy Process etc.)\n\n\n```"
  },
  {
    "objectID": "data_analytics.html",
    "href": "data_analytics.html",
    "title": "Data Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAssignment 4-1\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:54:22 AM\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 4-2\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:54:35 AM\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 3-1\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:53:19 AM\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 3-2\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:53:33 AM\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 3-3\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:54:10 AM\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 2\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:29:23 AM\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 1\n\n\n\n\n\n\n\nMachine Learning & Deep Learning for Data Science (2022 Fall)\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/23/22, 10:29:39 AM\n\n\n\n\n\n\n  \n\n\n\n\nTextmining on UN SDG Voluntary National Review: 6 Countries\n\n\n\n\n\n\n\nEvidence-Based Research Lab-PPM Projects\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2022\n\n\nSangwon Ju, SNU GSPA\n\n\n12/22/22, 11:16:47 AM\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis 3: Analysis of the FWA in public sector organizations using data mining techniques [in Korean]\n\n\n\n\n\n\n\nData Analytics for Public Data (2021 Fall)\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\nSangwon Ju, SNU GSPA\n\n\n8/31/22, 1:59:50 PM\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis 2: Unemployment index\n\n\n\n\n\n\n\nData Analytics for Public Data (2021 Fall)\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2021\n\n\nSangwon Ju, SNU GSPA\n\n\n8/31/22, 2:04:42 PM\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis 1: Number of local government employees\n\n\n\n\n\n\n\nData Analytics for Public Data (2021 Fall)\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\nSangwon Ju, SNU GSPA\n\n\n8/31/22, 2:01:46 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 8: Final Exam\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:06:41 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 7: Chapter 9\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:06:43 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 6: Chapter 8\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:05:22 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 5: Chapter 7\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:05:29 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 4: Chapter 6\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:05:46 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 3: Chapter 5\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:06:05 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 2: Chapter 4\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:06:21 PM\n\n\n\n\n\n\n  \n\n\n\n\nEDA Assignment 1: Chapter 3\n\n\n\n\n\n\n\nExploratory Data Analysis (2021 Spring) [in Korean]\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\nSangwon Ju, Yonsei Public Administration\n\n\n8/31/22, 8:06:33 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Sangwon Ju",
    "section": "",
    "text": "Curriculum Vitae (As of 17 OCT 2023)"
  },
  {
    "objectID": "CV2.html",
    "href": "CV2.html",
    "title": "Sangwon Ju",
    "section": "",
    "text": "Curriculum Vitae (As of 17 OCT 2023)"
  }
]