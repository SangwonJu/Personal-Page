{
  "hash": "d05603fe319c0efdd898e53c670c4ecc",
  "result": {
    "markdown": "---\ntitle: \"Textmining on UN SDG Voluntary National Review: 6 Countries\"\nformat: \n    html:\n        code-fold: true\nauthor: \"Sangwon Ju, SNU GSPA\" \nknitr:\n  opts_knit:\n    verbose: false\njupyter: python3\nexecute:\n  freeze: auto \ndate: 2022/08/06\n---\n\n\n# 1. Analysis of Voluntary National Review\n\n## Goal\n\n-   According to the annual ranking of SDGs released by the UN, Republic of Korea ranked 28th as of 2021, lower than other developed countries.\n-   Accordingly, this section presents the strategic direction of environmental policy to achieve and promote SDGs.\n-   To achieve this goal, this section analyzes VNRs of five developed countries to verify the characteristics of 17 SDGs directed policies and to present the direction in which SDGs-related environmental policies should be developed in the future through comparative analysis with the Republic of Korea VNR.\n\n## Voluntary National Review (VNR)\n\n-   The United Nations (UN) member states are required to report their SDGs objectives and implementation plans to the United Nations High-Level Political Forum (HLPF) and the material submitted at the forum is a 'Voluntary National Review'.\n\n-   The purpose of VNRs is to present a snapshot of where the country stands in the implementation of the Sustainable Development Goals, with a view to help accelerate progress through experience sharing, peer-learning, identifying gaps and good practices, and mobilizing partnerships. (UN, 2021)\n\n-   Each member (54 countries) started submitting VNRs to the UN's 'Sustainable Development Knowledge Platform' since 2016 and was recommended to submit them at least once every four years\n\n-   The report contains voluntary assessment of the objectives and performance of each member countries SDGs and follows no uniform form\n\n## Countries of analysis\n\nRepublic of Korea, Germany, Finland, United Kingdom, Japan, Australia\n\n-   In this chapter, Korea, Germany, Finland, the United Kingdom, Japan, and Australia were selected as target countries\n-   Selected based on the [rankings of Sustainable Development Goals index announced by the UN](https://dashboards.sdgindex.org/rankings), and the rest of the countries tried to select evenly by continent, starting with Finland, which ranked first\n-   Identifying global trends on SDGs can be achieved through this analysis\n\n# 2. Analysis: Topic Modelling\n\n-   The analysis was c onducted in three steps. -- using\n\n::: column-page\n![](https://www.tidytextmining.com/images/tmwr_0601.png){fig-align=\"center\"} Figure: A flowchart of a text analysis that incorporates topic modeling (Silge & Robinson, 2017)\n:::\n\n## Loading Package & Setting Working Directory\n\n\n::: {.cell result='hide'}\n\n```{.r .cell-code}\npacman::p_load(\"reticulate\",\"tidyverse\",\"tidymodels\",\n               \"tidytext\",\"lda\",\"readr\",\"igraph\",\"forcats\",\n               \"sna\",\"ergm\",\"network\",\"stringr\",\"magrittr\",\"showtext\",\n               \"tokenizers\",\"janeaustenr\",\"forcats\",\"RColorBrewer\",\n               \"viridis\",\"stopwords\",\"purrr\",\"widyr\",\"textmineR\",\"stm\",\n               \"ggraph\",\"tidygraph\",\"ggfortify\",\"ggthemes\",\"cowplot\",\"fs\",\n               \"knitr\",\"kableExtra\",\"ggrepel\",\"grid\",\"gridExtra\",\"topicmodels\",\n               \"scales\",\"ggpattern\",\"magick\",\"corrplot\")\n\n# Font Setting\nfont_add_google(name=\"Nanum Gothic\") #fonts.google.com에서 고르시면 됨\nshowtext_auto()\n\ndata_folder = \"e:/OneDrive - SNU/r/vnr\"\nger_path = fs::path(data_folder, \"germany.txt\")\nfin_path = fs::path(data_folder, \"finland.txt\")\nkor_path = fs::path(data_folder, \"korea.txt\")\njp_path = fs::path(data_folder, \"japan.txt\")\nuk_path = fs::path(data_folder, \"uk.txt\")\nastl_path = fs::path(data_folder, \"austrailia.txt\")\n```\n:::\n\n\n## Tokening custom functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentence_word1=function(x){\n  x=as.character(x)\n  a=tibble(words=tokenize_sentences(x) %>%\n             unlist() %>% tibble(words=.),\n    sentence=1:nrow(words))\n  return(a)}\n\nsentence_word2=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_words(stopwords =  as.vector(stop_words$word),\n                                strip_numeric = T) %>%  \n               unlist())\n  return(a)}\n\nsentence_word3=function(x){\n  x=as.character(x)\n  a=tibble(words=textstem::lemmatize_strings(x) %>% \n               tokenize_ngrams(.,n = 2, n_min = 2,\n               stopwords = as.vector(stop_words$word)) %>%\n               unlist())\n  return(a)}\n```\n:::\n\n\n# Korea VNR\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea <-  readLines(kor_path) %>%\n    str_flatten(collapse = \" \") %>%\n    str_replace_all(pattern=\"//\", \" // \") %>%\n    str_split(pattern=\"//\") %>%\n    unlist() %>%\n    str_squish() %>%\n    str_remove_all(\"ABC*\\\\w*\") %>%\n    tibble(text=.) %>% \n    filter(trimws(text) != \"\") %>% \n    mutate(paragraph=1:n(),.before=text)\n```\n:::\n\n\n## Tokenizing words\n\nBreak the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_tokenized <- korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word2)) %>%\n   unnest(words) \n```\n:::\n\n\n## n-grams\n\nAn n-gram (sometimes written \"ngram\") is a term in linguistics for a contiguous sequence of\\\nn items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\\\nn words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_ngram  <-  korea %>% \n   mutate(words=map(text, sentence_word1)) %>%\n   dplyr::select(-text) %>%\n   unnest(words) %>% \n   transmute(paragraph,sentence,words) %>%\n   mutate(words=map(words, sentence_word3)) %>%\n   unnest(words) \n```\n:::\n\n\n## Remove inadequate words for analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_tokenized <- korea_tokenized  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\"))\n    \n\nkorea_ngram <- korea_ngram  %>% \n    filter(!str_detect(words,pattern=\"[0-9]\")) %>%  \n    filter(!str_detect(words,pattern=\"\\\\d+[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"[a-z]*\\\\d+\")) %>% \n    filter(!str_detect(words,pattern=\"kor[a-z]*\")) %>%\n    filter(!str_detect(words,pattern=\"rok[a-z]*\")) %>% \n    filter(!str_detect(words,pattern=\"key\")) %>%\n    filter(!str_detect(words,pattern=\"sdg*\")) %>% \n    filter(!str_detect(words,pattern=\"progra*\")) %>%\n    filter(!str_detect(words,pattern=\"agend*\")) %>% \n    filter(!str_detect(words,pattern=\"billi*\")) %>% \n    filter(!str_detect(words,pattern=\"milli*\")) %>% \n    filter(!str_detect(words,pattern=\"datum*\")) %>% \n    filter(!str_detect(words,pattern=\"support*\")) %>%\n    filter(!str_detect(words,pattern=\"progra*\")) \n```\n:::\n\n\n## Frequency table\n\nThe frequency of specific words and n-grams in each paragraphs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(korea_tokenized %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(30) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Republic of Korea\") +\n    theme_bw()+\n    theme(plot.title = element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> a)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\n(korea_ngram %>%\n    count(words) %>% \n    arrange(desc(n)) %>% \n    top_n(20) %>%\n    mutate(word = fct_reorder(words, n, .desc = F)) %>%\n    ggplot(aes(x=n, y=word)) +\n    geom_col_pattern(\n    colour = \"white\",\n    pattern = 'gradient',\n    pattern_fill2 = '#CD313A',\n    pattern_fill = '#0047A0',\n    pattern_orientation='horizontal',\n    pattern_alpha=1)+\n    scale_fill_hue() +\n    geom_text(aes(label=n),hjust=-0.1,size=7,family=\"Nanum Gothic\")+\n    labs(y = NULL, title=\"Republic of Korea\") +\n    theme_bw()+\n    theme(plot.title =  element_text(size=40),\n        text=element_text(family=\"Nanum Gothic\",size=25)) -> a1) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-7-2.png){fig-align='center' width=576}\n:::\n:::\n\n\n## TF-IDF per paragraph\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_tfidf <- korea_tokenized %>%\n    mutate(id=glue::glue(\"{paragraph}-{sentence}\")) %>% \n    group_by(paragraph) %>%\n    count(paragraph,words) %>% \n    bind_tf_idf(words, paragraph, n) %>% \n    arrange(desc(tf_idf)) %>% \n    relocate(words,.before=paragraph) \n      \nkorea_tfidf %>% \n    filter(n>=2)%>% \n    head(20) %>% \n    kbl() %>% \n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> words </th>\n   <th style=\"text-align:right;\"> paragraph </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> tf </th>\n   <th style=\"text-align:right;\"> idf </th>\n   <th style=\"text-align:right;\"> tf_idf </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> conflict </td>\n   <td style=\"text-align:right;\"> 52 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1379310 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.4480599 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> governmental </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1538462 </td>\n   <td style=\"text-align:right;\"> 2.842970 </td>\n   <td style=\"text-align:right;\"> 0.4373799 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> institutional </td>\n   <td style=\"text-align:right;\"> 78 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 2.437504 </td>\n   <td style=\"text-align:right;\"> 0.4062507 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> saemaul </td>\n   <td style=\"text-align:right;\"> 48 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1219512 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.3961506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> undong </td>\n   <td style=\"text-align:right;\"> 48 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.1219512 </td>\n   <td style=\"text-align:right;\"> 3.248435 </td>\n   <td style=\"text-align:right;\"> 0.3961506 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> disability </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 0.1304348 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.3946032 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> humanitarian </td>\n   <td style=\"text-align:right;\"> 49 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1555556 </td>\n   <td style=\"text-align:right;\"> 2.437504 </td>\n   <td style=\"text-align:right;\"> 0.3791674 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:right;\"> 89 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.0961538 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3789983 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mechanism </td>\n   <td style=\"text-align:right;\"> 78 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 2.236834 </td>\n   <td style=\"text-align:right;\"> 0.3728056 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tax </td>\n   <td style=\"text-align:right;\"> 87 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.0823529 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.3246009 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> indicator </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 0.1500000 </td>\n   <td style=\"text-align:right;\"> 2.069780 </td>\n   <td style=\"text-align:right;\"> 0.3104669 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> person </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0.1014493 </td>\n   <td style=\"text-align:right;\"> 3.025291 </td>\n   <td style=\"text-align:right;\"> 0.3069136 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> south </td>\n   <td style=\"text-align:right;\"> 91 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0645161 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2990148 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> indicator </td>\n   <td style=\"text-align:right;\"> 94 </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;\"> 0.1428571 </td>\n   <td style=\"text-align:right;\"> 2.069780 </td>\n   <td style=\"text-align:right;\"> 0.2956828 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> oda </td>\n   <td style=\"text-align:right;\"> 36 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.1666667 </td>\n   <td style=\"text-align:right;\"> 1.690290 </td>\n   <td style=\"text-align:right;\"> 0.2817150 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> principle </td>\n   <td style=\"text-align:right;\"> 64 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1304348 </td>\n   <td style=\"text-align:right;\"> 2.149822 </td>\n   <td style=\"text-align:right;\"> 0.2804116 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> correspond </td>\n   <td style=\"text-align:right;\"> 33 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.0689655 </td>\n   <td style=\"text-align:right;\"> 3.941582 </td>\n   <td style=\"text-align:right;\"> 0.2718332 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> discrimination </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.0579710 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2686799 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> administration </td>\n   <td style=\"text-align:right;\"> 89 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.0576923 </td>\n   <td style=\"text-align:right;\"> 4.634729 </td>\n   <td style=\"text-align:right;\"> 0.2673882 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:right;\"> 31 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.0937500 </td>\n   <td style=\"text-align:right;\"> 2.842970 </td>\n   <td style=\"text-align:right;\"> 0.2665284 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Topic modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\nkorea_tfidf  %>%\n  cast_dtm(paragraph, words, n) -> korea_dtm.old\n\nrowTotals <- apply(korea_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_dtm <- korea_dtm.old[rowTotals> 0, ] # Remove all docs without words\n```\n:::\n\n\n### Using perplexity for hold out set\n\nThe perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_split <- initial_split(korea_tfidf, prop = 0.8)\n\n# Train\nkorea_split_train <- training(korea_split)\nkorea_split_train  %>%\n    cast_dtm(paragraph, words, n) -> korea_train_dtm.old\nrowTotals <- apply(korea_train_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_train_dtm <- korea_train_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\n# Test\nkorea_split_test <- testing(korea_split)\nkorea_split_test  %>%\n    cast_dtm(paragraph, words, n) -> korea_valid_dtm.old\nrowTotals <- apply(korea_valid_dtm.old, 1, sum) # Find the sum of words in each Document\nkorea_valid_dtm <- korea_valid_dtm.old[rowTotals> 0, ] # Remove all docs without words\n\nloglik_v     <- vector(\"numeric\", 20)\nperplexity_v <- vector(\"numeric\", 20)\n\nfor (i in 2:20) {\n    cat(\"... \", i, \"\\n\")\n    tmp_mod  <- LDA(korea_train_dtm, k=i, method=\"Gibbs\", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))\n    loglik_v[i] <- logLik(tmp_mod)\n    perplexity_v[i] <- perplexity(tmp_mod, newdata = korea_valid_dtm)\n}\n\nkorea_topic_k_df <- tibble(\n    topic_k = 1:20,\n    loglik = loglik_v,\n    perplexity = perplexity_v)\n\nkorea_topic_k_df %>%\n    filter(topic_k != 1) %>%\n    gather(metric, value, -topic_k) %>%\n    ggplot(aes(x=topic_k, y=value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~metric, scales = \"free\")+\n    theme_bw() +\n    theme(plot.title = element_text(size=20),\n        text=element_text(family=\"Nanum Gothic\",size=40))\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Cast LDA (k=6)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_topic=\"\ngovernment, implementation, policy, international, establish, national, framework, ministry, stakeholder, oda, cooperation, humanitarian, assistance, project, strategy, saemual,  undong, partnership, global,\ndisability, sector, service, increase, public, act, person, welfare, strengthen, promote, ensure,\neconomic, social, plan, goal, target, integrate, environmental, society, relate,\nsustainable, development, create, environment, organization, civil, report,\ngirl, country, initiative, life, education, develop, health, quality, school, capacity\"\nkorea_topic_update <- korea_topic %>% \n    str_squish() %>% \n    str_split(pattern=\", \",simplify =T) \nkorea_topic_names=data.frame(\n  stringsAsFactors = FALSE,\n                topic = c(1L, 2L, 3L, 4L, 5L, 6L),\n                topic2= c(\"정부 정책\", \"국제협력\", \"복지\", \"통합적 목표설정\", \"지속가능개발\", \"인권\"))\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_lda <-  LDA(korea_dtm, k = 6,\n              method=\"Gibbs\",\n              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))\n\nkorea_tp_6 <- korea_lda %>%\n    tidy(matrix = \"beta\") %>%\n    group_by(term) %>%\n    slice_max(beta) %>%\n    dplyr::left_join(korea_topic_names,by=\"topic\") %>% \n    ungroup() %>% \n    select(-topic) %>% \n    rename(topic=\"topic2\")\n\nkorea_tp_6 %>% \n    group_by(topic) %>%\n    slice_max(beta,n=10) %>%\n    filter(term %in% as.vector(korea_topic_update)) %>% \n    arrange(topic)  %>% \n    mutate(term = reorder_within(term, beta, topic)) %>%\n    ggplot(aes(beta, term, fill = topic)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\",ncol=3) +\n    scale_y_reordered() +\n    theme_bw()+ \n    theme(plot.title = element_text(size=20),\n          text=element_text(family=\"Nanum Gothic\",size=40),\n          axis.text.x = element_text(size = 20),\n          axis.title.y=element_blank()) + \n    ghibli::scale_fill_ghibli_d(\"PonyoLight\", direction = 1)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Network Graph\n\n### Pairwise Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkorea_coocur <- korea_tfidf  %>%\n    ungroup() %>% \n    arrange(desc(n)) %>%\n    rename(term=words) %>%\n    pairwise_count(term, paragraph, sort = TRUE,upper = F) \n```\n:::\n\n\n### Building links and nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinks <- korea_coocur %>%\n    top_n(100) %>%\n    rename(source=\"item1\",target=\"item2\",weight=\"n\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n```{.r .cell-code}\nnodes <- tibble(term=union(links$source,links$target)) %>%\n    left_join(y=korea_tp_6,by=\"term\") %>%\n    dplyr::select(nodes=\"term\",topic)\n```\n:::\n\n\n### Building networks\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)\nkorea_g <- as_tbl_graph(korea_network)\n```\n:::\n\n\n### Network Graph\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1.5)) +\n    scale_edge_alpha(range = c(0.3,1)) +\n    scale_size(range = c(5,20)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30)) +\n    guides(size=\"none\",\n           color =\"none\") +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1)\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n### Network Graph per topic\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nkorea_g %>%\n    activate(\"nodes\") %>%\n    mutate(degree = centrality_degree(),\n             community = as.factor(topic)) %>%\n    ggraph(layout = 'kk') +\n    geom_edge_link(aes(alpha = weight,width=weight),\n                   colour = \"black\", show.legend = FALSE) +\n    scale_edge_width(range = c(0.05,1)) +\n    scale_edge_alpha(range = c(0.05,1)) + \n    geom_node_point(aes(colour = community, size = degree)) +\n    geom_node_text(aes(label = name),size=8, repel = TRUE,family=\"Nanum Gothic\") +\n    theme_graph(base_family=\"Nanum Gothic\") +\n    theme(text=element_text(family=\"Nanum Gothic\", size=30),\n          strip.text = element_text(size = 40),\n          panel.border = element_rect(color = \"black\", fill = NA, size = 1)) +\n    guides(size=\"none\",\n           color = guide_legend(title=\"Topic 명\",\n                                override.aes = list(size=5))) +\n    ghibli::scale_color_ghibli_d(\"PonyoLight\", direction = 1) +\n    facet_nodes(~ community, ncol=3, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](Vnr_analysis_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n",
    "supporting": [
      "Vnr_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}