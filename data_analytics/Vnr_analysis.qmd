---
title: "Textmining on SDG VNRs of ROK, Germany, Finland"
format: 
    html:
        code-fold: true
author: "Sangwon Ju, SNU GSPA" 
knitr:
  opts_knit:
    verbose: false
jupyter: python3
date: 2022/08/06
---

1.  기술통계 분석

-   TF-IDF (Term Frequency - Inverse Document Frequency): 단어 빈도 - 역 문서 빈도 총 문서 수 대비 적게 등장한 단어가 중요한 단어 특정 문서 안에서 많이 등장한다고 해도 중요도가 올라가지는 않음

-   TF: 특정 문서 D에서 특정 단어 T의 등장 횟수

-   DF: 특정 단어 T가 등장한 문서의 수

-   IDF: DF에 반비례 하는 수. ln(총문서수/(DF))

-   TF-IDF: TF \* IDF 희귀하면서도 특정 텍스트에서 자주 사용된 단어 (TF)는 그 텍스트에서 중요함

2.  단어간 혹은 기사간 상관관계

-   동시 출현(Co-Occurrence) 단어 분석 문장 혹은 기사에 함께 사용된 단어는 어떤 단어일지 분석하는 것. 단어의 "맥락"을 파악하기 위하여 어떤 단어들이 함께 쓰였는지를 알아야 함. 의미를 가진 단어(명사, 동사, 형용사)등을 추출하여 어떤 단어들이 함께 빈번하게 쓰였는지 분석해보는게 필요함.

3.  연관 규칙 분석 (Association Rules)

-   장바구니 분석 Apriori Algorithm(Agrawal et al., 1993): 어떤 단어가 다른 단어들과의 연관규칙을 가지는지를 추출 하는 방식

4.  **토픽 모델링**

-   토픽 모델링은 문서와 단어로 구성된 행렬(DTM)을 기반으로 문서에 잠재되어 (Latent)있다고 가정된 토픽의 등장확률을 추정하는 일련의 통계적 텍스트 처리기법을 일컫는다. (Blei, 2014; Blei and Lafferty, 2007;Blei, Ng and Jordan, 2003) DTM을 활용하여 주제-확률 분포, 단어-확률 분포를 구한뒤 잠재 주제를 찾는 LDA나 Singular Value Decomposition을 통해 차원 축소를 하는 방법이 있다.

-   **LDA(Latent Dirichlet Allocation):**

    **이 문서에서는 어떤 주제들이 오가고 있을까?**

    PLSA를 조건부 확률로 확장시킨 기법으로 잠재 주제의 확률적 분포에 대한 PLSA의 한계점을 보완한 모델이다. LDA모델은 무작위로 섞여있는 대량의 문서에서 단어들의 패턴을 추론하여 각 토픽의 특성을 도출하는데 용이하며, 텍스트 데이터의 의미구조를 파악하기에 적합한 방법 중 하나이다. 한 문서는 여러가지 토픽으로 이루어지고, 토픽은 여러 단어를 혼합하여 구성된다.

-   1개의 토픽은 여러 단어(서로 다른 확률을 가진)로 구성.

-   1개의 단어는 여러 토픽에서 서로 다른 확률을 가짐.

-   delta는 문장이 각 토픽에 등장할 확률, beta는 단어가 각 토픽에 등장할 확률

# Loading Package

```{r warning = FALSE, message = FALSE, result="hide"}
pacman::p_load("reticulate","tidyverse","tidymodels",
               "tidytext","lda","readr","igraph","forcats",
               "sna","ergm","network","stringr","magrittr","showtext",
               "tokenizers","janeaustenr","forcats","RColorBrewer",
               "viridis","stopwords","purrr","widyr","textmineR","stm",
               "ggraph","tidygraph","ggfortify","ggthemes","cowplot","fs",
               "knitr","kableExtra","ggrepel","grid","gridExtra","topicmodels",
               "scales","ggpattern","magick","corrplot")

# Font Setting
font_add_google(name="Gowun Dodum") #fonts.google.com에서 고르시면 됨
showtext_auto()
```

# Setting Working Directory

```{r}
data_folder = "e:/OneDrive - SNU/r/vnr"
ger_path = fs::path(data_folder, "germany.txt")
fin_path = fs::path(data_folder, "finland.txt")
kor_path = fs::path(data_folder, "korea.txt")
jp_path = fs::path(data_folder, "japan.txt")
uk_path = fs::path(data_folder, "uk.txt")
astl_path = fs::path(data_folder, "austrailia.txt")
```

표제어 추출을 통해 어근을 추출하고 토크나이징을 통해서 토픽 추출 용이하게 만들기.

```{r warning = FALSE, message = FALSE}
sentence_word1=function(x){
  x=as.character(x)
  a=tibble(words=tokenize_sentences(x) %>%
             unlist() %>% tibble(words=.),
    sentence=1:nrow(words))
  return(a)}

sentence_word2=function(x){
  x=as.character(x)
  a=tibble(words=textstem::lemmatize_strings(x) %>% 
               tokenize_words(stopwords =  as.vector(stop_words$word),
                                strip_numeric = T) %>%  
               unlist())
  return(a)}

sentence_word3=function(x){
  x=as.character(x)
  a=tibble(words=textstem::lemmatize_strings(x) %>% 
               tokenize_ngrams(.,n = 2, n_min = 2,
               stopwords = as.vector(stop_words$word)) %>%
               unlist())
  return(a)}
```

# Korea VNR

## Preprocess

```{r}
korea <-  readLines(kor_path) %>%
    str_flatten(collapse = " ") %>%
    str_replace_all(pattern="//", " // ") %>%
    str_split(pattern="//") %>%
    unlist() %>%
    str_squish() %>%
    str_remove_all("ABC*\\w*") %>%
    tibble(text=.) %>% 
    filter(trimws(text) != "") %>% 
    mutate(paragraph=1:n(),.before=text)
```

## Tokenizing words

Break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

```{r}
korea_tokenized <- korea %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word2)) %>%
   unnest(words) 
```

## n-grams

An n-gram (sometimes written "ngram") is a term in linguistics for a contiguous sequence of\
n items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\
n words.

```{r}
korea_ngram  <-  korea %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word3)) %>%
   unnest(words) 
```

## Remove inadequate words for analysis

```{r}
korea_tokenized <- korea_tokenized  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="\\d+[a-z]*")) %>% 
    filter(!str_detect(words,pattern="[a-z]*\\d+")) %>% 
    filter(!str_detect(words,pattern="kor[a-z]*")) %>%
    filter(!str_detect(words,pattern="rok[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>% 
    filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))
    

korea_ngram <- korea_ngram  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="\\d+[a-z]*")) %>% 
    filter(!str_detect(words,pattern="[a-z]*\\d+")) %>% 
    filter(!str_detect(words,pattern="kor[a-z]*")) %>%
    filter(!str_detect(words,pattern="rok[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>% 
     filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*")) 

```

## Frequency table

The frequency of specific words and n-grams in each paragraphs.

```{r, fig.align='center',fig.width=6,fig.height=4}
(korea_tokenized %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(30) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#CD313A',
    pattern_fill = '#0047A0',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="대한민국") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> a)

(korea_ngram %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(20) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#CD313A',
    pattern_fill = '#0047A0',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="대한민국") +
    theme_bw()+
    theme(plot.title =  element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> a1) 
```

## TF-IDF per paragraph

```{r, fig.align='center',fig.width=10,fig.height=10}
korea_tfidf <- korea_tokenized %>%
    mutate(id=glue::glue("{paragraph}-{sentence}")) %>% 
    group_by(paragraph) %>%
    count(paragraph,words) %>% 
    bind_tf_idf(words, paragraph, n) %>% 
    arrange(desc(tf_idf)) %>% 
    relocate(words,.before=paragraph) 
      
korea_tfidf %>% 
    filter(n>=2)%>% 
    head(20) %>% 
    kbl() %>% 
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Topic modeling

```{r}
set.seed(2022)
korea_tfidf  %>%
  cast_dtm(paragraph, words, n) -> korea_dtm.old

rowTotals <- apply(korea_dtm.old, 1, sum) # Find the sum of words in each Document
korea_dtm <- korea_dtm.old[rowTotals> 0, ] # Remove all docs without words
```

### Using perplexity for hold out set

The perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.

```{r warning = FALSE, message = FALSE, results='hide', fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_split <- initial_split(korea_tfidf, prop = 0.8)

# Train
korea_split_train <- training(korea_split)
korea_split_train  %>%
    cast_dtm(paragraph, words, n) -> korea_train_dtm.old
rowTotals <- apply(korea_train_dtm.old, 1, sum) # Find the sum of words in each Document
korea_train_dtm <- korea_train_dtm.old[rowTotals> 0, ] # Remove all docs without words

# Test
korea_split_test <- testing(korea_split)
korea_split_test  %>%
    cast_dtm(paragraph, words, n) -> korea_valid_dtm.old
rowTotals <- apply(korea_valid_dtm.old, 1, sum) # Find the sum of words in each Document
korea_valid_dtm <- korea_valid_dtm.old[rowTotals> 0, ] # Remove all docs without words

loglik_v     <- vector("numeric", 20)
perplexity_v <- vector("numeric", 20)

for (i in 2:20) {
    cat("... ", i, "\n")
    tmp_mod  <- LDA(korea_train_dtm, k=i, method="Gibbs", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))
    loglik_v[i] <- logLik(tmp_mod)
    perplexity_v[i] <- perplexity(tmp_mod, newdata = korea_valid_dtm)
}

korea_topic_k_df <- tibble(
    topic_k = 1:20,
    loglik = loglik_v,
    perplexity = perplexity_v)

korea_topic_k_df %>%
    filter(topic_k != 1) %>%
    gather(metric, value, -topic_k) %>%
    ggplot(aes(x=topic_k, y=value)) +
    geom_line() +
    geom_point() +
    facet_wrap(~metric, scales = "free")+
    theme_bw() +
    theme(plot.title = element_text(size=20),
        text=element_text(family="Gowun Dodum",size=40))
```

### Cast LDA (k=4)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_lda <-  LDA(korea_dtm, k = 4,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

korea_tp_4 <- korea_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

korea_tp_4 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20)) 
```

### Cast LDA (k=5)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_lda <-  LDA(korea_dtm, k = 5,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

korea_tp_5 <- korea_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

korea_tp_5 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20)) 
```

### Cast LDA (k=6)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_lda <-  LDA(korea_dtm, k = 6,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

korea_tp_6 <- korea_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

korea_tp_6 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20)) 
```

### Cast LDA (k=7)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_lda <-  LDA(korea_dtm, k = 7,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

korea_tp_7 <- korea_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

korea_tp_7 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20)) 
```

## Network Graph

### Pairwise Count

```{r warning = FALSE, message = FALSE}
korea_coocur <- korea_tfidf  %>%
    ungroup() %>% 
    arrange(desc(n)) %>%
    rename(term=words) %>%
    pairwise_count(term, paragraph, sort = TRUE,upper = F) 
```

### Building links and nodes

```{r}
links <- korea_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n")

nodes <- tibble(term=union(links$source,links$target)) %>%
    left_join(y=korea_tp_6,by="term") %>%
    dplyr::select(nodes="term",topic)
```

### Building networks

```{r, fig.align='center',fig.width=10,fig.height=7}
korea_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)
korea_g <- as_tbl_graph(korea_network)

```

### Network Graph

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum", size=40)) +
    guides(size="none")
```

### Network Graph per topic

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
korea_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1)) +
    scale_edge_alpha(range = c(0.05,1)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none")+
    facet_nodes(~ community,ncol=3)
```

# Germany VNR

## Preprocess

```{r}
germany<-readLines(ger_path) %>%
    str_flatten(collapse = " ") %>%
    str_replace_all(pattern="//", " // ") %>%
    str_split(pattern="//") %>%
    unlist() %>%
    str_squish() %>%
    str_remove_all("ABC*\\w*") %>%
    tibble(text=.) %>% 
    filter(trimws(text) != "") %>% 
    mutate(paragraph=1:n(),.before=text)
```

## Tokenizing words

break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

```{r}
germany_tokenized <- germany %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word2)) %>%
   unnest(words) 
```

## n-grams

An n-gram (sometimes written "ngram") is a term in linguistics for a contiguous sequence of\
n items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\
n words.

```{r}
germany_ngram  <-  germany %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word3)) %>%
   unnest(words) 
```

## Remove inadequate words for analysis

```{r}
germany_tokenized <- germany_tokenized  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="germa[a-z]*")) %>%
    filter(!str_detect(words,pattern="eur[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="eu*")) %>%
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
     filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))


germany_ngram <- germany_ngram  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="germa[a-z]*")) %>%
    filter(!str_detect(words,pattern="eur[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="eu*")) %>%
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
     filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))
```

## Frequency table

The frequency of specific words and n-grams in each paragraphs.

```{r, fig.align='center',fig.width=6,fig.height=4}
(germany_tokenized %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(30) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#FF0000',
    pattern_fill = '#FFCC00',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="독일") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> b)

(germany_ngram %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(20) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#FF0000',
    pattern_fill = '#FFCC00',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="독일") +
    theme_bw()+
    theme(plot.title =  element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> b1) 
```

## TF-IDF per paragraph

```{r, fig.align='center',fig.width=10,fig.height=10}
germany_tfidf <- germany_tokenized %>%
    mutate(id=glue::glue("{paragraph}-{sentence}")) %>% 
    group_by(paragraph) %>%
    count(paragraph,words) %>% 
    bind_tf_idf(words, paragraph, n) %>% 
    arrange(desc(tf_idf)) %>% 
    relocate(words,.before=paragraph) 
      
germany_tfidf %>% 
    filter(n>=2)%>% 
    head(20) %>% 
    kbl() %>% 
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Topic modeling

```{r}
set.seed(2022)

germany_tfidf  %>%
  cast_dtm(paragraph, words, n) -> germany_dtm.old

rowTotals <- apply(germany_dtm.old, 1, sum) # Find the sum of words in each Document
germany_dtm <- germany_dtm.old[rowTotals> 0, ] # Remove all docs without words

```

### Using perplexity for hold out set

The perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.

```{r warning = FALSE, message = FALSE, results='hide', fig.align='center',fig.width=10,fig.height=7}
germany_split <- initial_split(germany_tfidf, prop = 0.8)

# Train
germany_split_train <- training(germany_split)
germany_split_train  %>%
    cast_dtm(paragraph, words, n) -> germany_train_dtm.old

rowTotals <- apply(germany_train_dtm.old, 1, sum) # Find the sum of words in each Document
germany_train_dtm <- germany_train_dtm.old[rowTotals> 0, ] # Remove all docs without words

# Test
germany_split_test <- testing(germany_split)
germany_split_test  %>%
    cast_dtm(paragraph, words, n) -> germany_test_dtm.old

rowTotals <- apply(germany_test_dtm.old, 1, sum) # Find the sum of words in each Document
germany_test_dtm <- germany_test_dtm.old[rowTotals> 0, ] # Remove all docs without words

loglik_v     <- vector("numeric", 20)
perplexity_v <- vector("numeric", 20)


for (i in 2:20) {
    cat("... ", i, "\n")
    tmp_mod  <- LDA(germany_train_dtm, k=i, method="Gibbs", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))
    loglik_v[i] <- logLik(tmp_mod)
    perplexity_v[i] <- perplexity(tmp_mod, newdata = germany_test_dtm)
}

germany_topic_k_df <- tibble(
    topic_k = 1:20,
    loglik = loglik_v,
    perplexity = perplexity_v)

germany_topic_k_df %>%
    filter(topic_k != 1) %>%
    gather(metric, value, -topic_k) %>%
    ggplot(aes(x=topic_k, y=value)) +
    geom_line() +
    geom_point() +
    facet_wrap(~metric, scales = "free")+
    theme_bw() +
    theme(plot.title = element_text(size=20),
        text=element_text(family="Gowun Dodum",size=40))
```

### Cast LDA (k=4)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
germany_lda <-  LDA(germany_dtm, k = 4,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

germany_tp_4 <- germany_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

germany_tp_4 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=5)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
germany_lda <-  LDA(germany_dtm, k = 5,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

germany_tp_5 <- germany_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

germany_tp_5 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=6)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
germany_lda <-  LDA(germany_dtm, k = 6,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

germany_tp_6 <- germany_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

germany_tp_6 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=7)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
germany_lda <-  LDA(germany_dtm, k = 7,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

germany_tp_7 <- germany_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

germany_tp_7 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

## Network Graph

### Pairwise Count

```{r}
germany_coocur <- germany_tfidf  %>%
    ungroup() %>% 
    arrange(desc(n)) %>%
    rename(term=words) %>%
    pairwise_count(term, paragraph, sort = TRUE,upper = F) 
```

### Building links and nodes

```{r}
links <- germany_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n") 

nodes <- tibble(term=union(links$source,links$target)) %>%
    left_join(y=germany_tp_7,by="term") %>%
    dplyr::select(nodes="term",topic)
```

### Building networks

```{r, fig.align='center',fig.width=10,fig.height=7}
germany_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)
germany_g <- as_tbl_graph(germany_network)

```

### Network Graph

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
germany_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum", size=40)) +
    guides(size="none")
```

### Network Graph per topic

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
germany_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1)) +
    scale_edge_alpha(range = c(0.05,1)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none")+
    facet_nodes(~ community,ncol=3)
```

# Finland VNR

## Preprocess

```{r}
finland <-  readLines(fin_path) %>%
    str_flatten(collapse = " ") %>%
    str_replace_all(pattern="//", " // ") %>%
    str_split(pattern="//") %>%
    unlist() %>%
    str_squish() %>%
    str_remove_all("ABC*\\w*") %>%
    tibble(text=.) %>% 
    filter(trimws(text) != "") %>% 
    mutate(paragraph=1:n(),.before=text)
```

## Tokenizing words

break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

```{r}
finland_tokenized <- finland %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word2)) %>%
   unnest(words) 
```

## n-grams

An n-gram (sometimes written "ngram") is a term in linguistics for a contiguous sequence of\
n items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\
n words.

```{r}
finland_ngram  <-  finland %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word3)) %>%
   unnest(words) 
```

## Remove inadequate words for analysis

```{r}
finland_tokenized <-finland_tokenized  %>%
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="finl[a-z]*")) %>%
    filter(!str_detect(words,pattern="finn[a-z]*")) %>% 
    filter(!str_detect(words,pattern="eur*")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="million")) %>% 
    filter(!str_detect(words,pattern="billion")) %>% 
    filter(!str_detect(words,pattern="line")) %>% 
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>% 
     filter(!str_detect(words,pattern="datum*"))  %>%
    filter(!str_detect(words,pattern="progra*"))

finland_ngram <- finland_ngram  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="finl[a-z]*")) %>%
    filter(!str_detect(words,pattern="finn[a-z]*")) %>% 
    filter(!str_detect(words,pattern="eur*")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="million")) %>% 
    filter(!str_detect(words,pattern="billion")) %>% 
    filter(!str_detect(words,pattern="line")) %>% 
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
     filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))

```

## Frequency table

The frequency of specific words and n-grams in each paragraphs.

```{r, fig.align='center',fig.width=6,fig.height=4}
(finland_tokenized %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(30) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#003580',
    pattern_fill = '#FFFFFF',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="핀란드") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> c)

(germany_ngram %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(20) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#003580',
    pattern_fill = '#FFFFFF',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="핀란드") +
    theme_bw()+
    theme(plot.title =  element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> c1) 
```

## TF-IDF per paragraph

```{r, fig.align='center',fig.width=10,fig.height=10}
finland_tfidf <- finland_tokenized %>%
    mutate(id=glue::glue("{paragraph}-{sentence}")) %>% 
    group_by(paragraph) %>%
    count(paragraph,words) %>% 
    bind_tf_idf(words, paragraph, n) %>% 
    arrange(desc(tf_idf)) %>% 
    relocate(words,.before=paragraph) 
      
finland_tfidf %>% 
    filter(n>=2)%>% 
    head(20) %>% 
    kbl() %>% 
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Topic modeling

```{r}
set.seed(2022)

finland_tfidf  %>%
  cast_dtm(paragraph, words, n) -> finland_dtm.old

rowTotals <- apply(finland_dtm.old, 1, sum) # Find the sum of words in each Document
finland_dtm <- finland_dtm.old[rowTotals> 0, ] # Remove all docs without words

```

### Using perplexity for hold out set

The perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.

### Cast LDA (k=4)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 4,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_4 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_4 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=5)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 5,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_5 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_5 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=6)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 6,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_6 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_6 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=7)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 7,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_7 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_7 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=8)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 8,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_8 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_8 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=9)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 9,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_9 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_9 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=5) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=10)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_lda <-  LDA(finland_dtm, k = 10,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

finland_tp_10 <- finland_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

finland_tp_10 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=5) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```
## Network Graph

### Pairwise Count

```{r}
finland_coocur <- finland_tfidf  %>%
    ungroup() %>% 
    arrange(desc(n)) %>%
    rename(term=words) %>%
    pairwise_count(term, paragraph, sort = TRUE,upper = F) 
```

### Building links and nodes

```{r}
links <- finland_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n") 

nodes <- tibble(term=union(links$source,links$target)) %>%
    left_join(y=finland_tp_7,by="term") %>%
    dplyr::select(nodes="term",topic)
```

### Building networks

```{r, fig.align='center',fig.width=10,fig.height=7}

finland_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)
finland_g <- as_tbl_graph(finland_network)

```

### Network Graph

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum", size=40)) +
    guides(size="none")
```

### Network Graph per topic

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
finland_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1)) +
    scale_edge_alpha(range = c(0.05,1)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none")+
    facet_nodes(~ community,ncol=3)
```

# Japan VNR

## Preprocess

```{r}
japan<-readLines(jp_path) %>%
    str_flatten(collapse = " ") %>%
    str_replace_all(pattern="//", " // ") %>%
    str_split(pattern="//") %>%
    unlist() %>%
    str_squish() %>%
    str_remove_all("ABC*\\w*") %>%
    tibble(text=.) %>% 
    filter(trimws(text) != "") %>% 
    mutate(paragraph=1:n(),.before=text)
```

## Tokenizing words

break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

```{r}
japan_tokenized <- japan %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word2)) %>%
   unnest(words) 
```

## n-grams

An n-gram (sometimes written "ngram") is a term in linguistics for a contiguous sequence of\
n items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\
n words.

```{r}
japan_ngram  <-  japan %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word3)) %>%
   unnest(words) 
```

## Remove inadequate words for analysis

```{r}
japan_tokenized <- japan_tokenized  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="jap*[a-z]*")) %>%
    filter(!str_detect(words,pattern="asia[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
     filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))


japan_ngram <- japan_ngram  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="jap*[a-z]*")) %>%
    filter(!str_detect(words,pattern="asia[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="billi*")) %>% 
     filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))
```

## Frequency table

The frequency of specific words and n-grams in each paragraphs.

```{r, fig.align='center',fig.width=6,fig.height=4}
(japan_tokenized %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(30) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#BC002D',
    pattern_fill = '#FFFFFF',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="일본") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> d)

(japan_ngram %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(20) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#BC002D',
    pattern_fill = '#FFFFFF',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="일본") +
    theme_bw()+
    theme(plot.title =  element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> d1) 
```

## TF-IDF per paragraph

```{r, fig.align='center',fig.width=10,fig.height=10}
japan_tfidf <- japan_tokenized %>%
    mutate(id=glue::glue("{paragraph}-{sentence}")) %>% 
    group_by(paragraph) %>%
    count(paragraph,words) %>% 
    bind_tf_idf(words, paragraph, n) %>% 
    arrange(desc(tf_idf)) %>% 
    relocate(words,.before=paragraph) 
      
japan_tfidf %>% 
    filter(n>=2)%>% 
    head(20) %>% 
    kbl() %>% 
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Topic modeling

```{r}
set.seed(2022)

japan_tfidf  %>%
  cast_dtm(paragraph, words, n) -> japan_dtm.old

rowTotals <- apply(japan_dtm.old, 1, sum) # Find the sum of words in each Document
japan_dtm <- japan_dtm.old[rowTotals> 0, ] # Remove all docs without words

```

### Using perplexity for hold out set

The perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.

```{r warning = FALSE, message = FALSE, results='hide', fig.align='center',fig.width=10,fig.height=7}
japan_split <- initial_split(japan_tfidf, prop = 0.8)

# Train
japan_split_train <- training(japan_split)
japan_split_train  %>%
    cast_dtm(paragraph, words, n) -> japan_train_dtm.old

rowTotals <- apply(japan_train_dtm.old, 1, sum) # Find the sum of words in each Document
japan_train_dtm <- japan_train_dtm.old[rowTotals> 0, ] # Remove all docs without words

# Test
japan_split_test <- testing(japan_split)
japan_split_test  %>%
    cast_dtm(paragraph, words, n) -> japan_test_dtm.old

rowTotals <- apply(japan_test_dtm.old, 1, sum) # Find the sum of words in each Document
japan_test_dtm <- japan_test_dtm.old[rowTotals> 0, ] # Remove all docs without words

loglik_v     <- vector("numeric", 20)
perplexity_v <- vector("numeric", 20)


for (i in 2:20) {
    cat("... ", i, "\n")
    tmp_mod  <- LDA(japan_train_dtm, k=i, method="Gibbs", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))
    loglik_v[i] <- logLik(tmp_mod)
    perplexity_v[i] <- perplexity(tmp_mod, newdata = japan_test_dtm)
}

japan_topic_k_df <- tibble(
    topic_k = 1:20,
    loglik = loglik_v,
    perplexity = perplexity_v)

japan_topic_k_df %>%
    filter(topic_k != 1) %>%
    gather(metric, value, -topic_k) %>%
    ggplot(aes(x=topic_k, y=value)) +
    geom_line() +
    geom_point() +
    facet_wrap(~metric, scales = "free")+
    theme_bw() +
    theme(plot.title = element_text(size=20),
        text=element_text(family="Gowun Dodum",size=40))
```

### Cast LDA (k=4)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 4,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_4 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_4 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=5)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 5,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_5 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_5 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=6)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 6,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_6 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_6 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=7)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 7,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_7 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_7 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=8)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 8,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_8 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_8 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=9)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 9,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_9 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_9 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=5) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```


### Cast LDA (k=10)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_lda <-  LDA(japan_dtm, k = 10,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

japan_tp_10 <- japan_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

japan_tp_10 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=5) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```
## Network Graph

### Pairwise Count

```{r}
japan_coocur <- japan_tfidf  %>%
    ungroup() %>% 
    arrange(desc(n)) %>%
    rename(term=words) %>%
    pairwise_count(term, paragraph, sort = TRUE,upper = F) 
```

### Building links and nodes

```{r}
links <- japan_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n") 

nodes <- tibble(term=union(links$source,links$target)) %>%
    left_join(y=japan_tp_5,by="term") %>%
    dplyr::select(nodes="term",topic)
```

### Building networks

```{r, fig.align='center',fig.width=10,fig.height=7}
japan_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)
japan_g <- as_tbl_graph(japan_network)

```

### Network Graph

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum", size=40)) +
    guides(size="none")
```

### Network Graph per topic

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
japan_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1)) +
    scale_edge_alpha(range = c(0.05,1)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none")+
    facet_nodes(~ community,ncol=3)
```

# UK VNR

## Preprocess

```{r}
uk <-readLines(uk_path) %>%
    str_flatten(collapse = " ") %>%
    str_replace_all(pattern="//", " // ") %>%
    str_split(pattern="//") %>%
    unlist() %>%
    str_squish() %>%
    str_remove_all("ABC*\\w*") %>%
    tibble(text=.) %>% 
    filter(trimws(text) != "") %>% 
    mutate(paragraph=1:n(),.before=text)
```

## Tokenizing words

break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

```{r}
uk_tokenized <- uk %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word2)) %>%
   unnest(words) 
```

## n-grams

An n-gram (sometimes written "ngram") is a term in linguistics for a contiguous sequence of\
n items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\
n words.

```{r}
uk_ngram  <-  uk %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word3)) %>%
   unnest(words) 
```

## Remove inadequate words for analysis

```{r}
uk_tokenized <- uk_tokenized  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="uk[a-z]*")) %>%
    filter(!str_detect(words,pattern="eur[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="eu*")) %>%
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="wale*")) %>% 
    filter(!str_detect(words,pattern="scot*")) %>% 
    filter(!str_detect(words,pattern="eng*")) %>% 
    filter(!str_detect(words,pattern="irel*")) %>% 
    filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>% 
    filter(!str_detect(words,pattern="wt*")) %>% 
    filter(!str_detect(words,pattern="rbmps*")) %>% 
    filter(!str_detect(words,pattern="inn*")) %>%
    filter(!str_detect(words,pattern="whilst*")) %>% 
    filter(!str_detect(words,pattern="npt*")) %>%
    filter(!str_detect(words,pattern="britain*")) 

uk_ngram <- uk_ngram  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="uk[a-z]*")) %>%
    filter(!str_detect(words,pattern="eur[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="eu*")) %>%
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="progra*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="wale*")) %>% 
    filter(!str_detect(words,pattern="scot*")) %>% 
    filter(!str_detect(words,pattern="eng*")) %>% 
    filter(!str_detect(words,pattern="irel*")) %>% 
    filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>% 
    filter(!str_detect(words,pattern="wt*")) %>% 
    filter(!str_detect(words,pattern="rbmps*")) %>% 
    filter(!str_detect(words,pattern="inn*")) %>%
    filter(!str_detect(words,pattern="whilst*")) %>% 
    filter(!str_detect(words,pattern="npt*")) %>%
    filter(!str_detect(words,pattern="britain*")) 
```

## Frequency table

The frequency of specific words and n-grams in each paragraphs.

```{r, fig.align='center',fig.width=6,fig.height=4}
(uk_tokenized %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(30) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#00247D',
    pattern_fill = '#CF142B',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="영국") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> e)

(uk_ngram %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(20) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#00247D',
    pattern_fill = '#CF142B',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="독일") +
    theme_bw()+
    theme(plot.title =  element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> e1) 
```

## TF-IDF per paragraph

```{r, fig.align='center',fig.width=10,fig.height=10}
uk_tfidf <- uk_tokenized %>%
    mutate(id=glue::glue("{paragraph}-{sentence}")) %>% 
    group_by(paragraph) %>%
    count(paragraph,words) %>% 
    bind_tf_idf(words, paragraph, n) %>% 
    arrange(desc(tf_idf)) %>% 
    relocate(words,.before=paragraph) 
      
uk_tfidf %>% 
    filter(n>=2)%>% 
    head(20) %>% 
    kbl() %>% 
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Topic modeling

```{r}
set.seed(2022)

uk_tfidf  %>%
  cast_dtm(paragraph, words, n) -> uk_dtm.old

rowTotals <- apply(uk_dtm.old, 1, sum) # Find the sum of words in each Document
uk_dtm <- uk_dtm.old[rowTotals> 0, ] # Remove all docs without words

```

### Using perplexity for hold out set

The perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.

```{r warning = FALSE, message = FALSE, results='hide', fig.align='center',fig.width=10,fig.height=7}
uk_split <- initial_split(uk_tfidf, prop = 0.8)

# Train
uk_split_train <- training(uk_split)
uk_split_train  %>%
    cast_dtm(paragraph, words, n) -> uk_train_dtm.old

rowTotals <- apply(uk_train_dtm.old, 1, sum) # Find the sum of words in each Document
uk_train_dtm <- uk_train_dtm.old[rowTotals> 0, ] # Remove all docs without words

# Test
uk_split_test <- testing(uk_split)
uk_split_test  %>%
    cast_dtm(paragraph, words, n) -> uk_test_dtm.old

rowTotals <- apply(uk_test_dtm.old, 1, sum) # Find the sum of words in each Document
uk_test_dtm <- uk_test_dtm.old[rowTotals> 0, ] # Remove all docs without words

loglik_v     <- vector("numeric", 20)
perplexity_v <- vector("numeric", 20)


for (i in 2:20) {
    cat("... ", i, "\n")
    tmp_mod  <- LDA(uk_train_dtm, k=i, method="Gibbs", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))
    loglik_v[i] <- logLik(tmp_mod)
    perplexity_v[i] <- perplexity(tmp_mod, newdata = uk_test_dtm)
}

uk_topic_k_df <- tibble(
    topic_k = 1:20,
    loglik = loglik_v,
    perplexity = perplexity_v)

uk_topic_k_df %>%
    filter(topic_k != 1) %>%
    gather(metric, value, -topic_k) %>%
    ggplot(aes(x=topic_k, y=value)) +
    geom_line() +
    geom_point() +
    facet_wrap(~metric, scales = "free")+
    theme_bw() +
    theme(plot.title = element_text(size=20),
        text=element_text(family="Gowun Dodum",size=40))
```

### Cast LDA (k=4)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
uk_lda <-  LDA(uk_dtm, k = 4,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

uk_tp_4 <- uk_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

uk_tp_4 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=5)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
uk_lda <-  LDA(uk_dtm, k = 5,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

uk_tp_5 <- uk_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

uk_tp_5 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=6)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
uk_lda <-  LDA(uk_dtm, k = 6,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

uk_tp_6 <- uk_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

uk_tp_6 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=7)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
uk_lda <-  LDA(uk_dtm, k = 7,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

uk_tp_7 <- uk_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

uk_tp_7 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

## Network Graph

### Pairwise Count

```{r}
uk_coocur <- uk_tfidf  %>%
    ungroup() %>% 
    arrange(desc(n)) %>%
    rename(term=words) %>%
    pairwise_count(term, paragraph, sort = TRUE,upper = F) 
```

### Building links and nodes

```{r}
links <- uk_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n") 

nodes <- tibble(term=union(links$source,links$target)) %>%
    left_join(y=uk_tp_6,by="term") %>%
    dplyr::select(nodes="term",topic)
```

### Building networks

```{r, fig.align='center',fig.width=10,fig.height=7}
uk_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)
uk_g <- as_tbl_graph(uk_network)

```

### Network Graph

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
uk_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum", size=40)) +
    guides(size="none")
```

### Network Graph per topic

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
uk_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1)) +
    scale_edge_alpha(range = c(0.05,1)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none")+
    facet_nodes(~ community,ncol=3)
```

# Austrailia VNR

## Preprocess

```{r}
austrailia<-readLines(astl_path) %>%
    str_flatten(collapse = " ") %>%
    str_replace_all(pattern="//", " // ") %>%
    str_split(pattern="//") %>%
    unlist() %>%
    str_squish() %>%
    str_remove_all("ABC*\\w*") %>%
    tibble(text=.) %>% 
    filter(trimws(text) != "") %>% 
    mutate(paragraph=1:n(),.before=text)
```

## Tokenizing words

break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

```{r}
austrailia_tokenized <- austrailia %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word2)) %>%
   unnest(words) 
```

## n-grams

An n-gram (sometimes written "ngram") is a term in linguistics for a contiguous sequence of\
n items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of\
n words.

```{r}
austrailia_ngram  <-  austrailia %>% 
   mutate(words=map(text, sentence_word1)) %>%
   dplyr::select(-text) %>%
   unnest(words) %>% 
   transmute(paragraph,sentence,words) %>%
   mutate(words=map(words, sentence_word3)) %>%
   unnest(words) 
```

## Remove inadequate words for analysis

```{r}
austrailia_tokenized <- austrailia_tokenized  %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="austra*[a-z]*")) %>%
    filter(!str_detect(words,pattern="eur[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="eu*")) %>%
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="torres*"))%>% 
    filter(!str_detect(words,pattern="cent")) %>% 
    filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))

austrailia_ngram <- germany_ngram %>% 
    filter(!str_detect(words,pattern="[0-9]")) %>%  
    filter(!str_detect(words,pattern="austrail[a-z]*")) %>%
    filter(!str_detect(words,pattern="eur[a-z]*")) %>% 
    filter(!str_detect(words,pattern="key")) %>%
    filter(!str_detect(words,pattern="sdg*")) %>% 
    filter(!str_detect(words,pattern="eu*")) %>%
    filter(!str_detect(words,pattern="billi*")) %>% 
    filter(!str_detect(words,pattern="milli*")) %>%
    filter(!str_detect(words,pattern="inwh*")) %>%
    filter(!str_detect(words,pattern="agend*")) %>% 
    filter(!str_detect(words,pattern="torres*"))%>% 
    filter(!str_detect(words,pattern="cent")) %>% 
    filter(!str_detect(words,pattern="datum*")) %>% 
    filter(!str_detect(words,pattern="support*")) %>%
    filter(!str_detect(words,pattern="progra*"))
```

## Frequency table

The frequency of specific words and n-grams in each paragraphs.

```{r, fig.align='center',fig.width=6,fig.height=4}
(austrailia_tokenized %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(30) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#00008B',
    pattern_fill = '#FFFFFF',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="호주") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> f)

(austrailia_ngram %>%
    count(words) %>% 
    arrange(desc(n)) %>% 
    top_n(20) %>%
    mutate(word = fct_reorder(words, n, .desc = F)) %>%
    ggplot(aes(x=n, y=word)) +
    geom_col_pattern(
    colour = "white",
    pattern = 'gradient',
    pattern_fill2 = '#00008B',
    pattern_fill = '#FFFFFF',
    pattern_orientation='horizontal',
    pattern_alpha=1)+
    scale_fill_hue() +
    geom_text(aes(label=n),hjust=-0.1,size=7,family="Gowun Dodum")+
    labs(y = NULL, title="호주") +
    theme_bw()+
    theme(plot.title =  element_text(size=40),
        text=element_text(family="Gowun Dodum",size=25)) -> f1) 
```

## TF-IDF per paragraph

```{r, fig.align='center',fig.width=10,fig.height=10}
austrailia_tfidf <- austrailia_tokenized %>%
    mutate(id=glue::glue("{paragraph}-{sentence}")) %>% 
    group_by(paragraph) %>%
    count(paragraph,words) %>% 
    bind_tf_idf(words, paragraph, n) %>% 
    arrange(desc(tf_idf)) %>% 
    relocate(words,.before=paragraph) 
      
austrailia_tfidf %>% 
    filter(n>=2)%>% 
    head(20) %>% 
    kbl() %>% 
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Topic modeling

```{r}
set.seed(2022)

austrailia_tfidf  %>%
  cast_dtm(paragraph, words, n) -> austrailia_dtm.old

rowTotals <- apply(austrailia_dtm.old, 1, sum) # Find the sum of words in each Document
austrailia_dtm <- austrailia_dtm.old[rowTotals> 0, ] # Remove all docs without words

```

### Using perplexity for hold out set

The perplexity is the geometric mean of word likelihood. In 5-fold CV, we first estimate the model, usually called training model, for a given number of topics using 4 folds of the data and then use the left one fold of the data to calculate the perplexity. In calculating the perplexity, we set the model in LDA or CTM to be the training model and not to estimate the beta parameters.

```{r warning = FALSE, message = FALSE, results='hide', fig.align='center',fig.width=10,fig.height=7}
austrailia_split <- initial_split(austrailia_tfidf, prop = 0.8)

# Train
austrailia_split_train <- training(austrailia_split)
austrailia_split_train  %>%
    cast_dtm(paragraph, words, n) -> austrailia_train_dtm.old

rowTotals <- apply(austrailia_train_dtm.old, 1, sum) # Find the sum of words in each Document
austrailia_train_dtm <- austrailia_train_dtm.old[rowTotals> 0, ] # Remove all docs without words

# Test
austrailia_split_test <- testing(austrailia_split)
austrailia_split_test  %>%
    cast_dtm(paragraph, words, n) -> austrailia_test_dtm.old

rowTotals <- apply(austrailia_test_dtm.old, 1, sum) # Find the sum of words in each Document
austrailia_test_dtm <- austrailia_test_dtm.old[rowTotals> 0, ] # Remove all docs without words

loglik_v     <- vector("numeric", 20)
perplexity_v <- vector("numeric", 20)


for (i in 2:20) {
    cat("... ", i, "\n")
    tmp_mod  <- LDA(austrailia_train_dtm, k=i, method="Gibbs", control=list(alpha=0.5, iter=1000, seed=2020, thin=3))
    loglik_v[i] <- logLik(tmp_mod)
    perplexity_v[i] <- perplexity(tmp_mod, newdata = austrailia_test_dtm)}

austrailia_topic_k_df <- tibble(
    topic_k = 1:20,
    loglik = loglik_v,
    perplexity = perplexity_v)

austrailia_topic_k_df %>%
    filter(topic_k != 1) %>%
    gather(metric, value, -topic_k) %>%
    ggplot(aes(x=topic_k, y=value)) +
    geom_line() +
    geom_point() +
    facet_wrap(~metric, scales = "free")+
    theme_bw() +
    theme(plot.title = element_text(size=20),
        text=element_text(family="Gowun Dodum",size=40))
```

### Cast LDA (k=4)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
austrailia_lda <-  LDA(austrailia_dtm, k = 4,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

austrailia_tp_4 <- austrailia_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

austrailia_tp_4 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=5)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
austrailia_lda <-  LDA(austrailia_dtm, k = 5,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

austrailia_tp_5 <- austrailia_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

austrailia_tp_5 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=6)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
austrailia_lda <-  LDA(austrailia_dtm, k = 6,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

austrailia_tp_6 <- austrailia_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

austrailia_tp_6 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=3) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

### Cast LDA (k=7)

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
austrailia_lda <-  LDA(austrailia_dtm, k = 7,
              method="Gibbs",
              control = list(alpha=0.5, iter=1000, seed=2020, thin=3))

austrailia_tp_7 <- austrailia_lda %>%
    tidy(matrix = "beta") %>%
    group_by(term) %>%
    slice_max(beta) 

austrailia_tp_7 %>% 
    group_by(topic) %>%
    slice_max(beta,n=10) %>%
    arrange(topic) %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free",ncol=4) +
    scale_y_reordered() +
    theme_bw()+ 
    theme(plot.title = element_text(size=20),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  
```

## Network Graph

### Pairwise Count

```{r}
austrailia_coocur <- austrailia_tfidf  %>%
    ungroup() %>% 
    arrange(desc(n)) %>%
    rename(term=words) %>%
    pairwise_count(term, paragraph, sort = TRUE,upper = F) 
```

### Building links and nodes

```{r}
links <- austrailia_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n") 

nodes <- tibble(term=union(links$source,links$target)) %>%
    left_join(y=austrailia_tp_7,by="term") %>%
    dplyr::select(nodes="term",topic)
```

### Building networks

```{r, fig.align='center',fig.width=10,fig.height=7}
austrailia_network <- graph_from_data_frame(d = links, vertices = nodes, directed = F)
austrailia_g <- as_tbl_graph(austrailia_network)

```

### Network Graph

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
austrailia_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name, size=degree),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum", size=40)) +
    guides(size="none")
```

### Network Graph per topic

```{r, fig.align='center',fig.width=10,fig.height=7}
#| column: page
austrailia_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree(),
             community = as.factor(topic)) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight,width=weight),
                   colour = "black", show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1)) +
    scale_edge_alpha(range = c(0.05,1)) + 
    geom_node_point(aes(colour = community, size = degree)) +
    geom_node_text(aes(label = name),size=8, repel = TRUE,family="Gowun Dodum") +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none")+
    facet_nodes(~ community,ncol=3)
```

### simplify?

```{r}
links=finland_coocur %>%
    top_n(100) %>%
    rename(source="item1",target="item2",weight="n") %>%
    left_join(finland_tp_7 %>% dplyr::select(-beta,term) %>% 
                  transmute(source=term,topic1=topic),by="source") %>% 
    left_join(finland_tp_7 %>% dplyr::select(-beta,term) %>% 
                  transmute(target=term,topic2=topic),by="target") %>% 
    dplyr::select(topic1,topic2,weight) %>% 
    rename(source="topic1",target="topic2")

finland_network <- graph_from_data_frame(d = links, directed = F)
finland_g <- as_tbl_graph(simplify(finland_network))

finland_g %>%
    activate("nodes") %>%
    mutate(degree = centrality_degree()) %>%
    ggraph(layout = 'kk') +
    geom_edge_link(aes(alpha = weight), show.legend = FALSE) +
    scale_edge_width(range = c(0.05,1.5)) +
    scale_edge_alpha(range = c(0.3,1)) +
    scale_size(range = c(5,20)) + 
    geom_node_point(aes(size = degree, color="blue")) +
    geom_node_text(aes(label = name), repel = TRUE,family="Gowun Dodum",size=10, max.overlaps = Inf) +
    theme_graph(base_family="Gowun Dodum") +
    theme(text=element_text(family="Gowun Dodum",size=40)) +
    guides(size="none",color="none")

```

# Comparing Six countries

## Comparing Word Frequencies of Six Countries

```{r, fig.align='center',fig.width=10,fig.height=10, warning = FALSE, message = FALSE}
#| column: page

korea_tokenized %>% 
    mutate(key="대한민국") %>% 
    full_join(germany_tokenized %>%
                  mutate(key="독일")) %>% 
    full_join(finland_tokenized %>%
                  mutate(key="핀란드")) %>% 
    full_join(japan_tokenized %>% 
                mutate(key="일본")) %>%
    full_join(uk_tokenized %>% 
                mutate(key="영국")) %>%
    full_join(austrailia_tokenized %>% 
                mutate(key="호주")) %>%
    mutate(key=as_factor(key)) -> total_tokenized

total_tokenized %>% 
    group_by(key) %>%
    count(key,words) %>% 
    group_by(key) %>% 
    top_n(n, n=20) %>%
    arrange(desc(n)) %>% 
    mutate(words = reorder_within(words, n, key)) %>%
    ggplot(aes(x=n, y=words)) +
    geom_col(aes(fill=key),show.legend = FALSE) +
    scale_y_reordered() +
    facet_wrap(~key, nrow = 2, scales = "free") +
    labs(y = NULL, title="Comparing Word Frequencies of Six Countries") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))+
    ggeasy::easy_center_title()

```

## Comparing bigrams of Six Countries

```{r, fig.align='center',fig.width=10,fig.height=10, warning = FALSE, message = FALSE}
#| column: screen-inset-shaded
grid.arrange(a1,b1,c1,d1,e1,f1, nrow=2,
             top=textGrob("Comparing Bigrams of Six Countries",
                          gp = gpar(col = "black", 
                                    fontsize = 40,
                                    fontfamily="Gowun Dodum")))
```

## Comparing Tf-idf of Six Countries

```{r, fig.align='center',fig.width=10,fig.height=10, warning = FALSE, message = FALSE}
#| column: page

total_tokenized %>% 
    group_by(key) %>%
    count(key,words) %>% 
    bind_tf_idf(words, key, n) %>% 
    group_by(key) %>% 
    top_n(tf_idf, n=20) %>%
    arrange(desc(tf_idf)) %>% 
    mutate(words = reorder_within(words, tf_idf, key)) %>%
    ggplot(aes(x=tf_idf, y=words)) +
    geom_col(aes(fill=key),show.legend = FALSE) +
    scale_y_reordered() +
    facet_wrap(~key, nrow = 2, scales = "free") +
    labs(y = NULL, title="Comparing TF-IDF of Six Countries") +
    theme_bw()+
    theme(plot.title = element_text(size=40),
          text=element_text(family="Gowun Dodum",size=40),
          axis.text.x = element_text(size = 20))  +
    ggeasy::easy_center_title()

```

## Comparing the word frequencies

```{r, fig.align='center',fig.width=10,fig.height=7}
compare_six <- total_tokenized %>% 
    count(key, words) %>% 
    group_by(key) %>%
    mutate(proportion = n / sum(n)) %>%
    dplyr::select(-n) %>% 
    pivot_wider(names_from = key, values_from = proportion) %>%
    pivot_longer(3:7,
               names_to = "key", values_to = "proportion")
    
```

Words that are far from the line are words that are found more in one set of texts than another.

```{r, fig.align='center',fig.width=11,fig.height=9, warning = FALSE, message = FALSE}
#| column: page

# expect a warning about rows with missing values being removed
set.seed(2022)
compare_six %>%
    ggplot(aes(x = proportion, y = 대한민국, color = abs(대한민국 - proportion))) +
    geom_abline(color = "darkgreen", lty = 2, size=1.5) +
    geom_jitter(alpha = 0.1, size = 1.5, width = 0.3, height = 0.3) +
    geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5, family="Gowun Dodum",size=9) +
    scale_x_log10(labels = percent_format()) +
    scale_y_log10(labels = percent_format()) +
    scale_color_gradient(limits = c(0.001, 0.01), 
                         low = "darkgreen", high =  "black") +
    facet_wrap(~key, nrow = 2) +
    theme(legend.position="none") +
    labs(y = "대한민국", x = NULL, title="Comparing the word frequencies of Six Countries") +
    theme_bw() +
    theme(plot.title = element_text(size=40),
          text=element_text(family="Gowun Dodum",size=25))+
    guides(color="none") +
    ggeasy::easy_center_title()
```

## Correlation of words

```{r, fig.align='center',fig.width=6,fig.height=6, warning = FALSE, message = FALSE}
#| column: page
total_tokenized %>% 
    count(key, words) %>% 
    group_by(key) %>%
    mutate(proportion = n / sum(n)) %>%
    dplyr::select(- n) %>% 
    pivot_wider(names_from = key, values_from = proportion) %>% 
    dplyr::select(- words) -> correlation_total  

testRes=corrplot::cor.mtest(correlation_total, conf.level = 0.95)

correlation_total %>% 
    cor(use="pairwise.complete.obs") %>% 
    .[order(.[ , 1],decreasing = T), order(.[ 1, ],decreasing = T)] %>% 
    corrplot(tl.col = "black",
             diag=T, 
             type="lower",
             method="color",
             cl.pos="n",
             addCoef.col = 1,
             number.cex = 4,
             tl.cex = 3) 

```

```{r result="hide"}
cortrix = function (R, histogram = TRUE, method = c("pearson", "kendall", 
  "spearman"), ...) 
{
  x = as.matrix(R, method = "matrix")
  if (missing(method)) 
    method = method[1]
  cormeth <- method
  panel.cor <- function(x, y, digits = 2, prefix = "", use = "pairwise.complete.obs", 
    method = cormeth, cex.cor, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y, use = use, method = method)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste(prefix, txt, sep = "")
    if (missing(cex.cor)) 
      cex <- 2
    test <- cor.test(as.numeric(x), as.numeric(y), method = method)
    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", 
        "**", "*", ".", " "))
    text(0.5, 0.5, txt, cex = cex)
    text(0.8, 0.8, Signif, cex = 2, col = 2)
  }
  f <- function(t) {
    dnorm(t, mean = mean(x), sd = sd.xts(x))
  }
  dotargs <- list(...)
  dotargs$method <- NULL
  rm(method)
  hist.panel = function(x, ... = NULL) {
    par(new = TRUE)
    hist(x, col = "light blue", probability = TRUE, axes = FALSE, 
      main = "", breaks = "FD")
    lines(density(x, na.rm = TRUE), col = "red", lwd = 2)
    rug(x)
  }
  if (histogram) 
    pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, 
      diag.panel = hist.panel, ...)
  else pairs(x, gap = 0, lower.panel = panel.smooth, upper.panel = panel.cor, 
    ...)
}
```

```{r, fig.align='center',fig.width=4,fig.height=4, warning = FALSE, message = FALSE, out.width="150%"}

correlation_total %>% 
    cortrix(histogram=F)
```

\*\*\*: 0 - 0.001

## Document Similarity

```{r}
korea_string <- korea %>% 
    dplyr::select(2) %>%
    unlist() %>% 
    as.vector() %>% 
    str_c(.,collapse = " ") 
germany_string <- germany %>% 
    dplyr::select(2) %>%
    unlist() %>% 
    as.vector() %>% 
    str_c(.,collapse = " ") 
finland_string <- finland %>% 
    dplyr::select(2) %>%
    unlist() %>% 
    as.vector() %>% 
    str_c(.,collapse = " ") 
japan_string <- japan %>% 
    dplyr::select(2) %>%
    unlist() %>% 
    as.vector() %>% 
    str_c(.,collapse = " ") 
uk_string <- uk %>% 
    dplyr::select(2) %>%
    unlist() %>% 
    as.vector() %>% 
    str_c(.,collapse = " ") 
austrailia_string <- austrailia %>% 
    dplyr::select(2) %>%
    unlist() %>% 
    as.vector() %>% 
    str_c(.,collapse = " ") 
```

```{python}
import matplotlib.pylab as plt 
import matplotlib as mpl
import seaborn as sns
import numpy as np
import pandas as pd
from IPython.display import display, Markdown
from tabulate import tabulate
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import warnings
from nltk.stem import WordNetLemmatizer
import nltk
import string
```

```{python}
mpl.rc('font', family='NanumGothic') # 폰트 설정
mpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정

# 차트 스타일 설정
sns.set(font="NanumGothic", rc={"axes.unicode_minus":False}, style='white')
warnings.filterwarnings("ignore")
```

### Convert data from R objects

```{python}
document_df = pd.DataFrame.from_dict(
    {'filename': ["Korea", "Germany", "Finland", "Japan", "UK", "Austrailia"],
    'opinion_text':[r.korea_string, r.germany_string, r.finland_string, 
    r.japan_string, r.uk_string, r.austrailia_string]})
```

### Tokenizer

```{python}
# 단어 원형 추출 함수
lemmar = WordNetLemmatizer()

def LemTokens(tokens):
    return [lemmar.lemmatize(token) for token in tokens]

# 특수 문자 사전 생성: {33: None ...}
# ord(): 아스키 코드 생성
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)

# 특수 문자 제거 및 단어 원형 추출
def LemNormalize(text):
    text_new = text.lower().translate(remove_punct_dict)
    word_tokens = nltk.word_tokenize(text_new)
    return LemTokens(word_tokens)

# nltk.download('punkt')
# nltk.download('wordnet')
# nltk.download('omw-1.4')
tfidf_vect = TfidfVectorizer(stop_words='english' , ngram_range=(1,2), 
                             tokenizer = LemNormalize, min_df=0.05, max_df=0.85)

feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])    
```

### Cosine Similarity

```{python}
from sklearn.metrics.pairwise import cosine_similarity

similarity_pair = cosine_similarity(feature_vect[0] , feature_vect)
document_df["similarity"] = similarity_pair.reshape(-1,1)
```

### 시각화

```{python}
#| fig-align: center
plt.clf()
vis=document_df.iloc[1:].sort_values(by="similarity",ascending=False).reset_index(drop=True)
sns.set_style("white")
plt.figure(figsize = (10,7))
sns.barplot(x="similarity", y="filename", data=vis)
plt.title('Korea')
```
